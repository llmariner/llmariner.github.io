<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLMariner</title><link>https://llmariner.ai/</link><description>Recent content on LLMariner</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmariner.ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Inference with Open Models</title><link>https://llmariner.ai/docs/features/inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/inference/</guid><description>&lt;p>Here is an example chat completion command with the &lt;code>llma&lt;/code> CLI.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion &lt;span class="s2">&amp;#34;What is k8s?&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you want to use the Python library, you first need to create an API key:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma auth api-keys create &amp;lt;key name&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can then pass the API key to initialize the OpenAI client and run the completion:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">openai&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAI&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAI&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">base_url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;Base URL (e.g., http://localhost:8080/v1)&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">api_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;API key secret&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">completion&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">completions&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;google-gemma-2b-it-q4_0&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">messages&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;role&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;content&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;What is k8s?&amp;#34;&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stream&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">response&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">completion&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">choices&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">delta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can also just call ``client = OpenAI()&lt;code>if you set environment variables&lt;/code>OPENAI_BASE_URL&lt;code>and&lt;/code>OPENAI_API_KEY`.&lt;/p></description></item><item><title>Open WebUI</title><link>https://llmariner.ai/docs/intgration/openwebui/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/intgration/openwebui/</guid><description>&lt;p>&lt;a href="https://docs.openwebui.com/">Open WebUI&lt;/a> provides a web UI that works with OpenAI-compatible APIs. You can run Openn WebUI locally or run in a Kubernetes cluster.&lt;/p>
&lt;p>Here is an instruction for running Open WebUI in a Kubernetes cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">OPENAI_API_KEY&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;LLMariner API key&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">OPEN_API_BASE_URL&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;LLMariner API endpoint&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl create namespace open-webui
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl create secret generic -n open-webui llmariner-api-key --from-literal&lt;span class="o">=&lt;/span>&lt;span class="nv">key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">OPENAI_API_KEY&lt;/span>&lt;span class="si">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl apply -f - &lt;span class="s">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">kind: Deployment
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> namespace: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> selector:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> matchLabels:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> template:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> labels:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> containers:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> image: ghcr.io/open-webui/open-webui:main
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> ports:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: http
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> containerPort: 8080
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> protocol: TCP
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> env:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: OPENAI_API_BASE_URLS
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> value: ${OPEN_API_BASE_URL}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: WEBUI_AUTH
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> value: &amp;#34;false&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: OPENAI_API_KEYS
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> valueFrom:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> secretKeyRef:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: llmariner-api-key
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> key: key
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">---
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">apiVersion: v1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">kind: Service
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> namespace: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> type: ClusterIP
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> selector:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> ports:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - port: 8080
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: http
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> targetPort: http
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> protocol: TCP
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can then access Open WebUI with port forwarding:&lt;/p></description></item><item><title>Set up a Playground</title><link>https://llmariner.ai/docs/setup/playground/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/playground/</guid><description>&lt;p>Once all the setup completes, you can interact with the LLM service by directly hitting the API endpoints or using &lt;a href="https://github.com/openai/openai-python">the OpenAI Python library&lt;/a>.&lt;/p>
&lt;h2 id="step-1-install-terraform-and-ansible">Step 1: Install Terraform and Ansible&lt;a class="td-heading-self-link" href="#step-1-install-terraform-and-ansible" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>We use Terraform and Ansible. Follow the links to install if you haven't.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developer.hashicorp.com/terraform/install">Terraform&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html">Ansible&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html">kubernetes.core.k8s module for Ansible&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>To install &lt;code>kubernetes.core.k8s&lt;/code> module, run the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ansible-galaxy collection install kubernetes.core
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="step-2-clone-the-llmariner-repository">Step 2: Clone the LLMariner Repository&lt;a class="td-heading-self-link" href="#step-2-clone-the-llmariner-repository" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>We use the Terraform configuration and Ansible playbook in the &lt;a href="https://github.com/llmariner/llmariner">LLMariner repository&lt;/a>. Run the following commands to clone the repo and move to the directory where the Terraform configuration file is stored.&lt;/p></description></item><item><title>Technical Architecture</title><link>https://llmariner.ai/docs/advanced/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/advanced/architecture/</guid><description>&lt;h2 id="components">Components&lt;a class="td-heading-self-link" href="#components" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLMariner provisions the LLM stack consisting of the following micro services:&lt;/p>
&lt;ul>
&lt;li>Inference Manager&lt;/li>
&lt;li>Job Manager&lt;/li>
&lt;li>Model Manager&lt;/li>
&lt;li>File Manager&lt;/li>
&lt;li>Vector Store Server&lt;/li>
&lt;li>User Manager&lt;/li>
&lt;li>Cluster Manager&lt;/li>
&lt;li>Session Manager&lt;/li>
&lt;li>RBAC Manager&lt;/li>
&lt;/ul>
&lt;p>Each manager is responsible for the specific feature of LLM services as their names indicate. The following diagram shows the high-level architecture:&lt;/p>


&lt;p>&lt;img src="https://llmariner.ai/images/architecture_diagram.png" width="1949" height="815">&lt;/p>


&lt;p>LLMariner has dependency to the following components:&lt;/p>
&lt;ul>
&lt;li>Ingress controller&lt;/li>
&lt;li>SQL database&lt;/li>
&lt;li>S3-compatible object store&lt;/li>
&lt;li>&lt;a href="https://github.com/dexidp/dex">Dex&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://milvus.io/">Milvus&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Ingress controller is required to route traffic to each service. SQL database and S3-compatible object store are used to persist metadata (e.g., fine-tuning jobs), fine-tuned models, and training/validation files. Dex is used to provide authentication.&lt;/p></description></item><item><title>Continue</title><link>https://llmariner.ai/docs/intgration/continue/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/intgration/continue/</guid><description>&lt;p>&lt;a href="https://www.continue.dev/">Continue&lt;/a> provides an open source AI code assistant. You can use LLMariner as a backend endpoint for Continue.&lt;/p>
&lt;p>As LLMariner provides the OpenAI compatible API, you can set the &lt;code>provider&lt;/code> to &lt;code>&amp;quot;openai&amp;quot;&lt;/code>. &lt;code>apiKey&lt;/code> is set to an API key generated by LLMariner, and &lt;code>apiBase&lt;/code> is set to the endpoint URL of LLMariner (e.g., &lt;a href="http://localhost:8080/v1">http://localhost:8080/v1&lt;/a>).&lt;/p>
&lt;p>Here is an example configuration that you can put at &lt;code>~/.continue/config.json&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="line">&lt;span class="cl">&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;models&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;title&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;Meta-Llama-3.1-8B-Instruct-q4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;provider&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;openai&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;model&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;meta-llama-Meta-Llama-3.1-8B-Instruct-q4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;apiKey&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;apiBase&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;&amp;lt;LLMariner endpoint&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;tabAutocompleteModel&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;title&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;Auto complete&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;provider&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;openai&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;model&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;deepseek-ai-deepseek-coder-6.7b-base-q4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;apiKey&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;apiBase&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;&amp;lt;LLMariner endpoint&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;completionOptions&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;presencePenalty&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mf">1.1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;frequencyPenalty&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mf">1.1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;allowAnonymousTelemetry&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="kc">false&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Install LLMariner</title><link>https://llmariner.ai/docs/setup/install/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/</guid><description>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Logout of helm registry to perform an unauthenticated pull against the public ECR&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm registry &lt;span class="nb">logout&lt;/span> public.ecr.aws
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">helm upgrade --install &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --namespace &amp;lt;namespace&amp;gt; &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --create-namespace &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> llmariner oci://public.ecr.aws/cloudnatix/llmariner-charts/llmariner &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --values &amp;lt;values.yaml&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once installation completes, you can interact with the API endpoint using the &lt;a href="https://github.com/openai/openai-python">OpenAI Python library&lt;/a>, running our CLI, or directly hitting the endpoint. To download the CLI, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">ARCH&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;e.g., linux-amd64, darwin-arm64&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">curl --silent https://llmariner.ai/get-cli &lt;span class="p">|&lt;/span> bash
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">chmod u+x ./llma
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="eks-installation">EKS Installation&lt;a class="td-heading-self-link" href="#eks-installation" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Let's go through the details of the installation. Here we use EKS as a target K8s cluster, but it can be AKS, GKE, or any other K8s cluster.&lt;/p></description></item><item><title>Multi-Cluster and Multi-Cloud Deployment</title><link>https://llmariner.ai/docs/advanced/multi_cluster_deployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/advanced/multi_cluster_deployment/</guid><description>&lt;p>LLMariner deploys Kubernetes deployments to provision the LLM stack. In a typical configuration, all the services are deployed into a single Kubernetes cluster, but you can also deploy these services on multiple Kubernetes clusters. For example, you can deploy a control plane component in a CPU K8s cluster and deploy the rest of the components in GPU compute clusters.&lt;/p>
&lt;p>LLMariner can be deployed into multiple GPU clusters, and the clusters can span across multiple cloud providers (including GPU specific clouds like CoreWeave) and on-prem.&lt;/p></description></item><item><title>Supported Open Models</title><link>https://llmariner.ai/docs/features/models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/models/</guid><description>&lt;p>Please note that some models work only with specific inference runtimes.&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: left">Model&lt;/th>
 &lt;th style="text-align: left">Quantizations&lt;/th>
 &lt;th style="text-align: left">Supporting runtimes&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: left">TinyLlama/TinyLlama-1.1B-Chat-v1.0&lt;/td>
 &lt;td style="text-align: left">None&lt;/td>
 &lt;td style="text-align: left">vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">TinyLlama/TinyLlama-1.1B-Chat-v1.0&lt;/td>
 &lt;td style="text-align: left">AWQ&lt;/td>
 &lt;td style="text-align: left">vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">deepseek-ai/DeepSeek-Coder-V2-Lite-Base&lt;/td>
 &lt;td style="text-align: left">Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td style="text-align: left">Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&lt;/td>
 &lt;td style="text-align: left">Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td style="text-align: left">Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">deepseek-ai/deepseek-coder-6.7b-base&lt;/td>
 &lt;td style="text-align: left">None&lt;/td>
 &lt;td style="text-align: left">vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">deepseek-ai/deepseek-coder-6.7b-base&lt;/td>
 &lt;td style="text-align: left">AWQ&lt;/td>
 &lt;td style="text-align: left">vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">deepseek-ai/deepseek-coder-6.7b-base&lt;/td>
 &lt;td style="text-align: left">Q4_0&lt;/td>
 &lt;td style="text-align: left">vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">google/gemma-2b-it&lt;/td>
 &lt;td style="text-align: left">None&lt;/td>
 &lt;td style="text-align: left">Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">google/gemma-2b-it&lt;/td>
 &lt;td style="text-align: left">Q4_0&lt;/td>
 &lt;td style="text-align: left">Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">intfloat/e5-mistral-7b-instruct&lt;/td>
 &lt;td style="text-align: left">None&lt;/td>
 &lt;td style="text-align: left">vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">meta-llama/Meta-Llama-3.1-70B-Instruct&lt;/td>
 &lt;td style="text-align: left">AWQ&lt;/td>
 &lt;td style="text-align: left">vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">meta-llama/Meta-Llama-3.1-70B-Instruct&lt;/td>
 &lt;td style="text-align: left">Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td style="text-align: left">vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/td>
 &lt;td style="text-align: left">None&lt;/td>
 &lt;td style="text-align: left">vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/td>
 &lt;td style="text-align: left">AWQ&lt;/td>
 &lt;td style="text-align: left">vLLM, Triton&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/td>
 &lt;td style="text-align: left">Q4_0&lt;/td>
 &lt;td style="text-align: left">vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">mistralai/Mistral-7B-Instruct-v0.2&lt;/td>
 &lt;td style="text-align: left">Q4_0&lt;/td>
 &lt;td style="text-align: left">Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: left">sentence-transformers/all-MiniLM-L6-v2-f16&lt;/td>
 &lt;td style="text-align: left">None&lt;/td>
 &lt;td style="text-align: left">Ollama&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table></description></item><item><title>Hosted Control Plane</title><link>https://llmariner.ai/docs/advanced/hosted_control_plane/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/advanced/hosted_control_plane/</guid><description>&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Work-in-progress. This is subject to change, and we might limit the usage based on the number of API calls or the number of GPU nodes.

&lt;/div>

&lt;p>CloudNatix provides a hosted control plane of LLMariner. End users can use the full functionality of LLMariner just by registering their worker GPU clusters to this hosted control plane.&lt;/p>
&lt;h2 id="step-1-create-a-cloudnatix-account">Step 1. Create a CloudNatix account&lt;a class="td-heading-self-link" href="#step-1-create-a-cloudnatix-account" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Create a CloudNatix account if you haven't. Please visit &lt;a href="https://app.cloudnatix.com">https://app.cloudnatix.com&lt;/a>. You can click one of the &amp;quot;Sign in or sing up&amp;quot; buttons for SSO login or you can click &amp;quot;Sign up&amp;quot; at the bottom for the email &amp;amp; password login.&lt;/p></description></item><item><title>MLflow</title><link>https://llmariner.ai/docs/intgration/mlflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/intgration/mlflow/</guid><description>&lt;p>&lt;a href="https://mlflow.org/">MLflow&lt;/a> is an open-source tool for managing the machine learning lifecycle. It has various features for LLMs (&lt;a href="https://mlflow.org/docs/latest/llms/index.html">link&lt;/a>) and integration with OpenAI. We can apply these MLflow features to the LLM endpoints provided by LLMariner.&lt;/p>
&lt;p>For example, you can deploy a &lt;a href="https://mlflow.org/docs/latest/llms/index.html#id1">MLflow Deployments Server for LLMs&lt;/a> and use &lt;a href="https://mlflow.org/docs/latest/llms/index.html#id3">Prompt Engineering UI&lt;/a>.&lt;/p>
&lt;h2 id="deploying-mlflow-tracking-server">Deploying MLflow Tracking Server&lt;a class="td-heading-self-link" href="#deploying-mlflow-tracking-server" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Bitmani provides a &lt;a href="https://github.com/bitnami/charts/tree/main/bitnami/mlflow">Helm chart&lt;/a> for MLflow.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">helm upgrade &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --install &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --create-namespace &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> -n mlflow &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> mlflow oci://registry-1.docker.io/bitnamicharts/mlflow &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> -f values.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>An example &lt;code>values.yaml&lt;/code> is following:&lt;/p></description></item><item><title>Retrieval-Augmented Generation (RAG)</title><link>https://llmariner.ai/docs/features/rag/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/rag/</guid><description>&lt;h2 id="an-example-flow">An Example Flow&lt;a class="td-heading-self-link" href="#an-example-flow" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The first step is to create a vector store and create files in the vector store. Here is an example script with the OpenAI Python library:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">openai&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAI&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAI&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">base_url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner Endpoint URL&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">api_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">filename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;llmariner_overview.txt&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;w&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">fp&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">write&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;LLMariner builds a software stack that provides LLM as a service. It provides the OpenAI-compatible API.&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">files&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;rb&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">purpose&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;assistants&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Uploaded file. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">vs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">beta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vector_stores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Test vector store&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Created vector store. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">vs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">vfs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">beta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vector_stores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">files&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector_store_id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">vs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">file_id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Created vector store file. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">vfs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once the files are added into vector store, you can run the completion request with the RAG model.&lt;/p></description></item><item><title>Tutorials</title><link>https://llmariner.ai/docs/setup/tutorials/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/tutorials/</guid><description>&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Work-in-progress.

&lt;/div>

&lt;ul>
&lt;li>&lt;a href="https://github.com/llmariner/llmariner/blob/main/tutorials/getting_started.ipynb">https://github.com/llmariner/llmariner/blob/main/tutorials/getting_started.ipynb&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Model Fine-tuning</title><link>https://llmariner.ai/docs/features/fine_tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/fine_tuning/</guid><description>&lt;h2 id="submitting-a-fine-tuning-job">Submitting a Fine-Tuning Job&lt;a class="td-heading-self-link" href="#submitting-a-fine-tuning-job" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>You can use the OpenAI Python library to submit a fine-tuning job. Here is an example snippet that uploads a training file and uses that to run a fine-tuning job.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">openai&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAI&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAI&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">base_url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner Endpoint URL&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">api_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">files&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">training_filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;rb&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">purpose&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;fine-tune&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">job&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fine_tuning&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">jobs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;google-gemma-2b-it&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">suffix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;fine-tuning&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">training_file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Created job. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">job&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once a fine-tuning job is submitted, a k8s Job is created. A Job runs in a namespace where a user's project is associated.&lt;/p></description></item><item><title>Weights and Biases (W&amp;B)</title><link>https://llmariner.ai/docs/intgration/wandb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/intgration/wandb/</guid><description>&lt;p>&lt;a href="https://wandb.ai/">Weights and Biases (W&amp;amp;B)&lt;/a> is an AI developer platform. LLMariner provides the integration with W&amp;amp;B so that metrics for fine-tuning jobs are reported to W&amp;amp;B. With the integration, you can easily see the progress of your fine-tuning jobs, such as training epoch, loss, etc.&lt;/p>
&lt;p>Please take the following steps to enable the integration.&lt;/p>
&lt;p>First, obtain the API key of W&amp;amp;B and create a Kubernetes secret.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create secret generic wandb
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -n &amp;lt;fine-tuning job namespace&amp;gt; &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --from-literal&lt;span class="o">=&lt;/span>&lt;span class="nv">apiKey&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">WANDB_API_KEY&lt;/span>&lt;span class="si">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The secret needs to be created in a namespace where fine-tuning jobs run. Individual projects specify namespaces for fine-tuning jobs, and the default project runs fine-tuning jobs in the &amp;quot;default&amp;quot; namespace.&lt;/p></description></item><item><title>Jupyter Notebook</title><link>https://llmariner.ai/docs/features/jupyter_notebook/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/jupyter_notebook/</guid><description>&lt;h2 id="creating-a-jupyter-notebook">Creating a Jupyter Notebook&lt;a class="td-heading-self-link" href="#creating-a-jupyter-notebook" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>To create a Jupyter Notebook, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma workspace notebooks create my-notebook
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>By default, there is no GPU allocated to the Jupyter Notebook. If you want to allocate a GPU to the Jupyter Notebook, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma workspace notebooks create my-gpu-notebook --gpu &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There are other options that you can specify when creating a Jupyter Notebook, such as environment. You can see the list of options by using the &lt;code>--help&lt;/code> flag.&lt;/p></description></item><item><title>General-purpose Training</title><link>https://llmariner.ai/docs/features/training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/training/</guid><description>&lt;h2 id="creating-a-training-job">Creating a Training Job&lt;a class="td-heading-self-link" href="#creating-a-training-job" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>You can create a training job from the local pytorch code by running the following command.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma batch &lt;span class="nb">jobs&lt;/span> create &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --image&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;pytorch-2.1&amp;#34;&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --from-file&lt;span class="o">=&lt;/span>my-pytorch-script.py &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --from-file&lt;span class="o">=&lt;/span>requirements.txt &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --file-id&lt;span class="o">=&lt;/span>&amp;lt;file-id&amp;gt; &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --command &lt;span class="s2">&amp;#34;python -u /scripts/my-pytorch-script.py&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once a training job is created, a k8s Job is created. The job runs the command specified in the &lt;code>--command&lt;/code> flag, and files specified in the &lt;code>--from-file&lt;/code> flag are mounted to the /scripts directory in the container. If you specify the &lt;code>--file-id&lt;/code> flag (optional), the file will be download to the /data directory in the container.&lt;/p></description></item><item><title>API and GPU Usage Visibility</title><link>https://llmariner.ai/docs/features/visibility/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/visibility/</guid><description>&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Work-in-progress.

&lt;/div></description></item><item><title>User Management</title><link>https://llmariner.ai/docs/features/user_management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/user_management/</guid><description>&lt;p>LLMariner installs &lt;a href="https://github.com/dexidp/dex">Dex&lt;/a> by default. Dex is an identity service that uses &lt;a href="https://openid.net/developers/how-connect-works/">OpenID Connect&lt;/a> for authentication.&lt;/p>
&lt;p>The Helm chart for Dex is located at &lt;a href="https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server">https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server&lt;/a>. It uses a &lt;a href="https://dexidp.io/docs/connectors/local/">built-in local connector&lt;/a> and has the following configuration by default:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">staticPasswords&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>- &lt;span class="nt">userID&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">08a8684b-db88-4b73-90a9-3cd1661f5466&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">username&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">admin&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">email&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">admin@example.com&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># bcrypt hash of the string: $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">hash&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can switch a connector to an IdP in your environment (e.g., LDAP, GitHub). Here is an example connector configuration with Okta:&lt;/p></description></item><item><title>Access Control with Organizations and Projects</title><link>https://llmariner.ai/docs/features/access_control/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/access_control/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="basic-concepts">Basic Concepts&lt;a class="td-heading-self-link" href="#basic-concepts" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>LLMariner provides access control with two concepts: &lt;code>Organizations&lt;/code> and &lt;code>Projects&lt;/code>. The basic concept follows &lt;a href="https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects">OpenAI API&lt;/a>.&lt;/p>
&lt;p>You can define one or more than one organization. In each organization, you can define one or more than one project. For example, you can create an organization for each team in your company, and each team can create individual projects based on their needs.&lt;/p>
&lt;p>A project controls the visibility of resources such as models, fine-tuning jobs. For example, a model that is generated by a fine-tuned job in project &lt;code>P&lt;/code> is only visible from project members in &lt;code>P&lt;/code>.&lt;/p></description></item><item><title>Roadmap</title><link>https://llmariner.ai/docs/dev/roadmap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/dev/roadmap/</guid><description>&lt;h2 id="milestone-0-completed">Milestone 0 (Completed)&lt;a class="td-heading-self-link" href="#milestone-0-completed" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> OpenAI compatible API&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Models: &lt;code>google-gemma-2b-it&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-1-completed">Milestone 1 (Completed)&lt;a class="td-heading-self-link" href="#milestone-1-completed" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> API authorization with Dex&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> API key management&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Quota management for fine-tuning jobs&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Inference autoscaling with GPU utilization&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Models: &lt;code>Mistral-7B-Instruct&lt;/code>, &lt;code>Meta-Llama-3-8B-Instruct&lt;/code>, and &lt;code>google-gemma-7b-it&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-2-completed">Milestone 2 (Completed)&lt;a class="td-heading-self-link" href="#milestone-2-completed" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Jupyter Notebook workspace creation&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Dynamic model loading &amp;amp; offloading in inference (initial version)&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Organization &amp;amp; project management&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> MLflow integration&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Weights &amp;amp; Biases integration for fine-tuning jobs&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> VectorDB installation and RAG&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Multi k8s cluster deployment (initial version)&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-3-completed">Milestone 3 (Completed)&lt;a class="td-heading-self-link" href="#milestone-3-completed" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Object store other than MinIO&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Multi-GPU general-purpose training jobs&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Inference optimization (e.g., vLLM)&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Models: &lt;code>Meta-Llama-3-8B-Instruct&lt;/code>, &lt;code>Meta-Llama-3-70B-Instruct&lt;/code>, &lt;code>deepseek-coder-6.7b-base&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-4-in-progress">Milestone 4 (In-progress)&lt;a class="td-heading-self-link" href="#milestone-4-in-progress" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Embedding API&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> API usage visibility&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Fine-tuning support with vLLM&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> High availability&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Frontend&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> GPU showback&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Non-Nvidia GPU support&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Multi k8s cluster deployment (file and vector store management)&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Monitoring &amp;amp; alerting&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> More models&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-5">Milestone 5&lt;a class="td-heading-self-link" href="#milestone-5" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Multi-GPU LLM fine-tuning jobs&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Events and metrics for fine-tuning jobs&lt;/li>
&lt;/ul></description></item><item><title>Search Results</title><link>https://llmariner.ai/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/search/</guid><description/></item></channel></rss>