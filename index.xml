<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLMariner</title><link>https://llmariner.ai/</link><description>Recent content on LLMariner</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmariner.ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Inference with Open Models</title><link>https://llmariner.ai/docs/features/inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/inference/</guid><description>&lt;h2 id="chat-completion">Chat Completion&lt;a class="td-heading-self-link" href="#chat-completion" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Here is an example chat completion command with the &lt;code>llma&lt;/code> CLI.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion &lt;span class="s2">&amp;#34;What is k8s?&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you want to use the Python library, you first need to create an API key:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma auth api-keys create &amp;lt;key name&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can then pass the API key to initialize the OpenAI client and run the completion:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">openai&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAI&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAI&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">base_url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;Base URL (e.g., http://localhost:8080/v1)&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">api_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;API key secret&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">completion&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">completions&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;google-gemma-2b-it-q4_0&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">messages&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;role&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;content&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;What is k8s?&amp;#34;&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stream&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">response&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">completion&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">choices&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">delta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can also just call &lt;code>client = OpenAI()&lt;/code> if you set environment variables &lt;code>OPENAI_BASE_URL&lt;/code> and &lt;code>OPENAI_API_KEY&lt;/code>.&lt;/p></description></item><item><title>Open WebUI</title><link>https://llmariner.ai/docs/integration/openwebui/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/openwebui/</guid><description>&lt;p>&lt;a href="https://docs.openwebui.com/">Open WebUI&lt;/a> provides a web UI that works with OpenAI-compatible APIs. You can run Openn WebUI locally or run in a Kubernetes cluster.&lt;/p>
&lt;p>Here is an instruction for running Open WebUI in a Kubernetes cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">OPENAI_API_KEY&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;LLMariner API key&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">OPEN_API_BASE_URL&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;LLMariner API endpoint&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl create namespace open-webui
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl create secret generic -n open-webui llmariner-api-key --from-literal&lt;span class="o">=&lt;/span>&lt;span class="nv">key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">OPENAI_API_KEY&lt;/span>&lt;span class="si">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">kubectl apply -f - &lt;span class="s">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">kind: Deployment
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> namespace: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> selector:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> matchLabels:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> template:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> labels:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> containers:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> image: ghcr.io/open-webui/open-webui:main
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> ports:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: http
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> containerPort: 8080
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> protocol: TCP
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> env:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: OPENAI_API_BASE_URLS
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> value: ${OPEN_API_BASE_URL}
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: WEBUI_AUTH
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> value: &amp;#34;false&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: OPENAI_API_KEYS
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> valueFrom:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> secretKeyRef:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: llmariner-api-key
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> key: key
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">---
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">apiVersion: v1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">kind: Service
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> namespace: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> type: ClusterIP
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> selector:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: open-webui
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> ports:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - port: 8080
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: http
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> targetPort: http
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> protocol: TCP
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can then access Open WebUI with port forwarding:&lt;/p></description></item><item><title>Technical Details</title><link>https://llmariner.ai/docs/dev/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/dev/architecture/</guid><description>&lt;h2 id="components">Components&lt;a class="td-heading-self-link" href="#components" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLMariner provisions the LLM stack consisting of the following micro services:&lt;/p>
&lt;ul>
&lt;li>Inference Manager&lt;/li>
&lt;li>Job Manager&lt;/li>
&lt;li>Model Manager&lt;/li>
&lt;li>File Manager&lt;/li>
&lt;li>Vector Store Server&lt;/li>
&lt;li>User Manager&lt;/li>
&lt;li>Cluster Manager&lt;/li>
&lt;li>Session Manager&lt;/li>
&lt;li>RBAC Manager&lt;/li>
&lt;li>API Usage&lt;/li>
&lt;/ul>
&lt;p>Each manager is responsible for the specific feature of LLM services as their names indicate. The following diagram shows the high-level architecture:&lt;/p>
&lt;!-- original file is located at diagrams/architecture.excalidraw -->







&lt;p class="mt-4 mb-4 text-center">&lt;img src="https://llmariner.ai/images/architecture_diagram.png" width="3166" height="2188">&lt;/p>


&lt;p>LLMariner has dependency to the following components:&lt;/p></description></item><item><title>Continue</title><link>https://llmariner.ai/docs/integration/continue/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/continue/</guid><description>&lt;p>&lt;a href="https://www.continue.dev/">Continue&lt;/a> provides an open source AI code assistant. You can use LLMariner as a backend endpoint for Continue.&lt;/p>
&lt;p>As LLMariner provides the OpenAI compatible API, you can set the &lt;code>provider&lt;/code> to &lt;code>&amp;quot;openai&amp;quot;&lt;/code>. &lt;code>apiKey&lt;/code> is set to an API key generated by LLMariner, and &lt;code>apiBase&lt;/code> is set to the endpoint URL of LLMariner (e.g., &lt;a href="http://localhost:8080/v1">http://localhost:8080/v1&lt;/a>).&lt;/p>
&lt;p>Here is an example configuration that you can put at &lt;code>~/.continue/config.json&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-json" data-lang="json">&lt;span class="line">&lt;span class="cl">&lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;models&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;title&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;Meta-Llama-3.1-8B-Instruct-q4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;provider&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;openai&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;model&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;meta-llama-Meta-Llama-3.1-8B-Instruct-q4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;apiKey&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;apiBase&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;&amp;lt;LLMariner endpoint&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;tabAutocompleteModel&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;title&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;Auto complete&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;provider&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;openai&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;model&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;deepseek-ai-deepseek-coder-6.7b-base-q4&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;apiKey&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;apiBase&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;&amp;lt;LLMariner endpoint&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;completionOptions&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;presencePenalty&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mf">1.1&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;frequencyPenalty&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mf">1.1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nt">&amp;#34;allowAnonymousTelemetry&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="kc">false&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The following is a demo video that shows the Continue integration that enables the coding assistant with Llama-3.1-Nemotron-70B-Instruct.&lt;/p></description></item><item><title>Model Loading</title><link>https://llmariner.ai/docs/features/models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/models/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLMariner hosts LLMs in a Kubernetes cluster by downloading models from source repos and uploading to an S3-compatible object store. The supported source model repositories are following:&lt;/p>
&lt;ul>
&lt;li>LLMariner offical model repository&lt;/li>
&lt;li>HuggingFace repositories&lt;/li>
&lt;li>Ollama repositories&lt;/li>
&lt;li>S3 bucket&lt;/li>
&lt;/ul>
&lt;h2 id="official-model-repository">Official Model Repository&lt;a class="td-heading-self-link" href="#official-model-repository" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>This is the default configuration.
The following is a list of supported models where we have validated.&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>Model&lt;/th>
 &lt;th>Quantizations&lt;/th>
 &lt;th>Supporting runtimes&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>TinyLlama/TinyLlama-1.1B-Chat-v1.0&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>TinyLlama/TinyLlama-1.1B-Chat-v1.0&lt;/td>
 &lt;td>AWQ&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/DeepSeek-Coder-V2-Lite-Base&lt;/td>
 &lt;td>Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&lt;/td>
 &lt;td>Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/deepseek-coder-6.7b-base&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/deepseek-coder-6.7b-base&lt;/td>
 &lt;td>AWQ&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/deepseek-coder-6.7b-base&lt;/td>
 &lt;td>Q4_0&lt;/td>
 &lt;td>vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>fixie-ai/ultravox-v0_3&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>google/gemma-2b-it&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>google/gemma-2b-it&lt;/td>
 &lt;td>Q4_0&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>intfloat/e5-mistral-7b-instruct&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Llama-3.2-1B-Instruct&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.3-70B-Instruct&lt;/td>
 &lt;td>AWQ, FP8-Dynamic&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-70B-Instruct&lt;/td>
 &lt;td>AWQ&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-70B-Instruct&lt;/td>
 &lt;td>Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td>vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/td>
 &lt;td>AWQ&lt;/td>
 &lt;td>vLLM, Triton&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/td>
 &lt;td>Q4_0&lt;/td>
 &lt;td>vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>nvidia/Llama-3.1-Nemotron-70B-Instruct&lt;/td>
 &lt;td>Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>nvidia/Llama-3.1-Nemotron-70B-Instruct&lt;/td>
 &lt;td>FP8-Dynamic&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>mistralai/Mistral-7B-Instruct-v0.2&lt;/td>
 &lt;td>Q4_0&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>sentence-transformers/all-MiniLM-L6-v2-f16&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>Please note that some models work only with specific inference runtimes.&lt;/p></description></item><item><title>Aider</title><link>https://llmariner.ai/docs/integration/aider/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/aider/</guid><description>&lt;p>&lt;a href="https://aider.chat/">Aider&lt;/a> is AI pair programming in your terminal or browser.&lt;/p>
&lt;p>Aider supports the OpenAI compatible API, and you can configure the endpoint
and the API key with environment variables.&lt;/p>
&lt;p>Here is an example installation and configuration procedure.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">python -m pip install -U aider-chat
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">OPENAI_API_BASE&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;Base URL &lt;span class="o">(&lt;/span>e.g., http://localhost:8080/v1&lt;span class="o">)&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">OPENAI_API_KEY&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;API key&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can then run Aider in your terminal or browser. Here is an example command
that launches Aider in your browser with Llama 3.1 70B.&lt;/p></description></item><item><title>Retrieval-Augmented Generation (RAG)</title><link>https://llmariner.ai/docs/features/rag/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/rag/</guid><description>&lt;h2 id="an-example-flow">An Example Flow&lt;a class="td-heading-self-link" href="#an-example-flow" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The first step is to create a vector store and create files in the vector store. Here is an example script with the OpenAI Python library:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">openai&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAI&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAI&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">base_url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner Endpoint URL&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">api_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">filename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;llmariner_overview.txt&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;w&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">fp&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">write&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;LLMariner builds a software stack that provides LLM as a service. It provides the OpenAI-compatible API.&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">files&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;rb&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">purpose&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;assistants&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Uploaded file. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">vs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">beta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vector_stores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Test vector store&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Created vector store. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">vs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">vfs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">beta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vector_stores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">files&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector_store_id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">vs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">file_id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Created vector store file. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">vfs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once the files are added into vector store, you can run the completion request with the RAG model.&lt;/p></description></item><item><title>AI Shell</title><link>https://llmariner.ai/docs/integration/ai_shell/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/ai_shell/</guid><description>&lt;p>&lt;a href="https://github.com/BuilderIO/ai-shell">AI Shell&lt;/a> is an open source tool that converts natural language to shell commands.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">npm install -g @builder.io/ai-shell
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ai config &lt;span class="nb">set&lt;/span> &lt;span class="nv">OPENAI_API_ENDPOINT&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;Base URL &lt;span class="o">(&lt;/span>e.g., http://localhost:8080/v1&lt;span class="o">)&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ai config &lt;span class="nb">set&lt;/span> &lt;span class="nv">OPENAI_KEY&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;API key&amp;gt;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ai config &lt;span class="nb">set&lt;/span> &lt;span class="nv">MODEL&lt;/span>&lt;span class="o">=&lt;/span>&amp;lt;model name&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then you can run the &lt;code>ai&lt;/code> command and ask what you want in plain English and generate a shell command with a human readable explanation of it.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">ai what is my ip address
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Model Fine-tuning</title><link>https://llmariner.ai/docs/features/fine_tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/fine_tuning/</guid><description>&lt;h2 id="submitting-a-fine-tuning-job">Submitting a Fine-Tuning Job&lt;a class="td-heading-self-link" href="#submitting-a-fine-tuning-job" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>You can use the OpenAI Python library to submit a fine-tuning job. Here is an example snippet that uploads a training file and uses that to run a fine-tuning job.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">openai&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAI&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAI&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">base_url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner Endpoint URL&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">api_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">files&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">training_filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;rb&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">purpose&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;fine-tune&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">job&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fine_tuning&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">jobs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;google-gemma-2b-it&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">suffix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;fine-tuning&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">training_file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Created job. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">job&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once a fine-tuning job is submitted, a k8s Job is created. A Job runs in a namespace where a user's project is associated.&lt;/p></description></item><item><title>k8sgpt</title><link>https://llmariner.ai/docs/integration/k8sgpt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/k8sgpt/</guid><description>&lt;p>&lt;a href="https://github.com/k8sgpt-ai/k8sgpt">k8sgpt&lt;/a> is a tool for scanning your
Kubernetes clusters, diagnosing, and triaging issues in simple English.&lt;/p>
&lt;p>You can use LLMariner as a backend of k8sgpt by running the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">k8sgpt auth add &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --backend openai &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --baseurl &amp;lt;LLMariner base URL &lt;span class="o">(&lt;/span>e.g., http://localhost:8080/v1/&lt;span class="o">)&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --password &amp;lt;LLMariner API Key&amp;gt; &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --model &amp;lt;Model ID&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then you can a command like &lt;code>k8sgpt analyze&lt;/code> to inspect your Kubernetes cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">k8sgpt analyze --explain
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>General-purpose Training</title><link>https://llmariner.ai/docs/features/training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/training/</guid><description>&lt;h2 id="creating-a-training-job">Creating a Training Job&lt;a class="td-heading-self-link" href="#creating-a-training-job" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>You can create a training job from the local pytorch code by running the following command.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma batch &lt;span class="nb">jobs&lt;/span> create &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --image&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;pytorch-2.1&amp;#34;&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --from-file&lt;span class="o">=&lt;/span>my-pytorch-script.py &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --from-file&lt;span class="o">=&lt;/span>requirements.txt &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --file-id&lt;span class="o">=&lt;/span>&amp;lt;file-id&amp;gt; &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --command &lt;span class="s2">&amp;#34;python -u /scripts/my-pytorch-script.py&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once a training job is created, a k8s Job is created. The job runs the command specified in the &lt;code>--command&lt;/code> flag, and files specified in the &lt;code>--from-file&lt;/code> flag are mounted to the /scripts directory in the container. If you specify the &lt;code>--file-id&lt;/code> flag (optional), the file will be download to the /data directory in the container.&lt;/p></description></item><item><title>Dify</title><link>https://llmariner.ai/docs/integration/dify/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/dify/</guid><description>&lt;p>&lt;a href="https://dify.ai/">Dify&lt;/a> is is an open-source LLM app development platform.
It can orchestrate LLM apps from agents to complex AI workflows, with an RAG engine.&lt;/p>
&lt;p>You can add LLMariner as one of Dify&amp;rsquo;s model providers with the following steps:&lt;/p>
&lt;ol>
&lt;li>Click the user profile icon.&lt;/li>
&lt;li>Click &amp;ldquo;Settings&amp;rdquo;&lt;/li>
&lt;li>Click &amp;ldquo;Model Provider&amp;rdquo;&lt;/li>
&lt;li>Search &amp;ldquo;OpenAI-API-compatible&amp;rdquo; and click &amp;ldquo;Add model&amp;rdquo;&lt;/li>
&lt;li>Configure a model name, API key,a nd API endpoint URL.&lt;/li>
&lt;/ol>







&lt;p class="mt-4 mb-4 text-center">&lt;img src="https://llmariner.ai/images/dify.png" width="2242" height="1774">&lt;/p>


&lt;p>You can then use the registered model from your LLM applications. For example, you
can create a new application by &amp;ldquo;Create from Template&amp;rdquo; and replace the use of an OpenAI
model with the configured model.&lt;/p></description></item><item><title>Jupyter Notebook</title><link>https://llmariner.ai/docs/features/jupyter_notebook/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/jupyter_notebook/</guid><description>&lt;h2 id="creating-a-jupyter-notebook">Creating a Jupyter Notebook&lt;a class="td-heading-self-link" href="#creating-a-jupyter-notebook" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>To create a Jupyter Notebook, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma workspace notebooks create my-notebook
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>By default, there is no GPU allocated to the Jupyter Notebook. If you want to allocate a GPU to the Jupyter Notebook, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma workspace notebooks create my-gpu-notebook --gpu &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There are other options that you can specify when creating a Jupyter Notebook, such as environment. You can see the list of options by using the &lt;code>--help&lt;/code> flag.&lt;/p></description></item><item><title>API and GPU Usage Optimization</title><link>https://llmariner.ai/docs/features/visibility/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/visibility/</guid><description>&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Work-in-progress.

&lt;/div>

&lt;h2 id="api-usage-visibility">API Usage Visibility&lt;a class="td-heading-self-link" href="#api-usage-visibility" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h2 id="inference-request-rate-limiting">Inference Request Rate-limiting&lt;a class="td-heading-self-link" href="#inference-request-rate-limiting" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h2 id="optimize-gpu-utilization">Optimize GPU Utilization&lt;a class="td-heading-self-link" href="#optimize-gpu-utilization" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="auto-scaling-of-inference-runtimes">Auto-scaling of Inference Runtimes&lt;a class="td-heading-self-link" href="#auto-scaling-of-inference-runtimes" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;h3 id="scheduled-scale-up-and-down-of-inference-runtimes">Scheduled Scale Up and Down of Inference Runtimes&lt;a class="td-heading-self-link" href="#scheduled-scale-up-and-down-of-inference-runtimes" aria-label="Heading self-link">&lt;/a>&lt;/h3></description></item><item><title>n8n</title><link>https://llmariner.ai/docs/integration/n8n/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/n8n/</guid><description>&lt;p>&lt;a href="https://n8n.io/">n8n&lt;/a> is a no-code platform for workload automation. You can deploy
n8n to your Kubernetes clusters. Here is an example command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl apply -f - &lt;span class="s">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">kind: Deployment
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: n8n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> namespace: n8n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> selector:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> matchLabels:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: n8n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> template:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> labels:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: n8n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> containers:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: n8n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> image: docker.n8n.io/n8nio/n8n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> ports:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - name: http
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> containerPort: 5678
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> protocol: TCP
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">---
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">apiVersion: v1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">kind: Service
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: n8n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> namespace: n8n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">spec:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> type: ClusterIP
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> selector:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: n8n
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> ports:
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> - port: 5678
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> name: http
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> targetPort: http
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> protocol: TCP
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can then access n8n with port forwarding:&lt;/p></description></item><item><title>Slackbot</title><link>https://llmariner.ai/docs/integration/slackbot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/slackbot/</guid><description>&lt;p>You can build a Slackbot that is integrated with LLMariner. The bot can provide a chat UI with Slack
and answer questions from end users.&lt;/p>
&lt;p>An example implementation can be found in &lt;a href="https://github.com/llmariner/slackbot">https://github.com/llmariner/slackbot&lt;/a>. You can deploy it in your
Kubernetes clusters and build a Slack app with the following configuration:&lt;/p>
&lt;ul>
&lt;li>Create an app-level token whose scope is &lt;code>connections:write&lt;/code>.&lt;/li>
&lt;li>Enable the socket mode. Enable event subscription with the &lt;code>app_mentions:read&lt;/code> scope.&lt;/li>
&lt;li>Add the following scopes in &amp;ldquo;OAuth &amp;amp; Permissions&amp;rdquo;: &lt;code>app_mentions:read&lt;/code>, &lt;code>chat:write&lt;/code>, &lt;code>chat:write.customize&lt;/code>, and &lt;code>links:write&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>You can install the Slack application to your workspace and interact.&lt;/p></description></item><item><title>User Management</title><link>https://llmariner.ai/docs/features/user_management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/user_management/</guid><description>&lt;p>LLMariner installs &lt;a href="https://github.com/dexidp/dex">Dex&lt;/a> by default. Dex is an identity service that uses &lt;a href="https://openid.net/developers/how-connect-works/">OpenID Connect&lt;/a> for authentication.&lt;/p>
&lt;p>The Helm chart for Dex is located at &lt;a href="https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server">https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server&lt;/a>. It uses a &lt;a href="https://dexidp.io/docs/connectors/local/">built-in local connector&lt;/a> and has the following configuration by default:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">staticPasswords&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>- &lt;span class="nt">userID&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">08a8684b-db88-4b73-90a9-3cd1661f5466&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">username&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">admin&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">email&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">admin@example.com&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># bcrypt hash of the string: $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">hash&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can switch a connector to an IdP in your environment (e.g., LDAP, GitHub). Here is an example connector configuration with Okta:&lt;/p></description></item><item><title>Access Control with Organizations and Projects</title><link>https://llmariner.ai/docs/features/access_control/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/access_control/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="basic-concepts">Basic Concepts&lt;a class="td-heading-self-link" href="#basic-concepts" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>LLMariner provides access control with two concepts: &lt;code>Organizations&lt;/code> and &lt;code>Projects&lt;/code>. The basic concept follows &lt;a href="https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects">OpenAI API&lt;/a>.&lt;/p>
&lt;p>You can define one or more than one organization. In each organization, you can define one or more than one project. For example, you can create an organization for each team in your company, and each team can create individual projects based on their needs.&lt;/p>
&lt;p>A project controls the visibility of resources such as models, fine-tuning jobs. For example, a model that is generated by a fine-tuned job in project &lt;code>P&lt;/code> is only visible from project members in &lt;code>P&lt;/code>.&lt;/p></description></item><item><title>MLflow</title><link>https://llmariner.ai/docs/integration/mlflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/mlflow/</guid><description>&lt;p>&lt;a href="https://mlflow.org/">MLflow&lt;/a> is an open-source tool for managing the machine learning lifecycle. It has various features for LLMs (&lt;a href="https://mlflow.org/docs/latest/llms/index.html">link&lt;/a>) and integration with OpenAI. We can apply these MLflow features to the LLM endpoints provided by LLMariner.&lt;/p>
&lt;p>For example, you can deploy a &lt;a href="https://mlflow.org/docs/latest/llms/index.html#id1">MLflow Deployments Server for LLMs&lt;/a> and use &lt;a href="https://mlflow.org/docs/latest/llms/index.html#id3">Prompt Engineering UI&lt;/a>.&lt;/p>
&lt;h2 id="deploying-mlflow-tracking-server">Deploying MLflow Tracking Server&lt;a class="td-heading-self-link" href="#deploying-mlflow-tracking-server" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Bitmani provides a &lt;a href="https://github.com/bitnami/charts/tree/main/bitnami/mlflow">Helm chart&lt;/a> for MLflow.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">helm upgrade &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --install &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --create-namespace &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> -n mlflow &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> mlflow oci://registry-1.docker.io/bitnamicharts/mlflow &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> -f values.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>An example &lt;code>values.yaml&lt;/code> is following:&lt;/p></description></item><item><title>Langfuse</title><link>https://llmariner.ai/docs/integration/langfuse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/langfuse/</guid><description>&lt;p>&lt;a href="https://github.com/langfuse/langfuse">Langfuse&lt;/a> is an open source LLM engineering platform. You can integrate Langfuse
with LLMariner as Langfuse provides an SDK for the OpenAI API.&lt;/p>
&lt;p>Here is an example procedure for running Langfuse locally:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">git clone https://github.com/langfuse/langfuse.git
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">cd&lt;/span> langfuse
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">docker compose up -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can sign up and create your account. Then you can generate API keys
and put them in environmental variables.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">LANGFUSE_SECRET_KEY&lt;/span>&lt;span class="o">=&lt;/span>...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">LANGFUSE_PUBLIC_KEY&lt;/span>&lt;span class="o">=&lt;/span>...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">export&lt;/span> &lt;span class="nv">LANGFUSE_HOST&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;http://localhost:3000&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can then use &lt;code>langfuse.openai&lt;/code> instead of &lt;code>openai&lt;/code> in your Python scripts
to record traces in Langfuse.&lt;/p></description></item><item><title>Install with Helm</title><link>https://llmariner.ai/docs/setup/install/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/overview/</guid><description>&lt;h2 id="prerequisites">Prerequisites&lt;a class="td-heading-self-link" href="#prerequisites" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLMariner requires the following resources:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html">Nvidia GPU Operator&lt;/a>&lt;/li>
&lt;li>Ingress controller (to route API requests)&lt;/li>
&lt;li>SQL database (to store jobs/models/files metadata)&lt;/li>
&lt;li>S3-compatible object store (to store training files and models)&lt;/li>
&lt;li>&lt;a href="https://milvus.io/">Milvus&lt;/a> (for RAG, optional)&lt;/li>
&lt;/ul>
&lt;p>LLMariner can process inference requests on CPU nodes, but it can be best used with GPU nodes. Nvidia GPU Operator is required to install the device plugin and make GPUs visible in the K8s cluster.&lt;/p></description></item><item><title>Set up a Playground on a GPU EC2 Instance</title><link>https://llmariner.ai/docs/setup/install/playground/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/playground/</guid><description>&lt;p>You can easily set up a playground for LLMariner and learn it. In this page, we provision an EC2 instance, build a &lt;a href="https://kind.sigs.k8s.io/">Kind&lt;/a> cluster, and deploy LLMariner and other required components.&lt;/p>


&lt;div class="alert alert-secondary" role="alert">
&lt;h4 class="alert-heading">Warn&lt;/h4>

 Playground environments are for experimentation use only. For a production-ready installation, please refere to the other installation guide.

&lt;/div>

&lt;p>Once all the setup completes, you can interact with the LLM service by directly hitting the API endpoints or using &lt;a href="https://github.com/openai/openai-python">the OpenAI Python library&lt;/a>.&lt;/p></description></item><item><title>Why LLMariner?</title><link>https://llmariner.ai/docs/overview/why-llmariner/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/overview/why-llmariner/</guid><description>&lt;p>&lt;strong>LLMariner&lt;/strong> (= LLM + Mariner) is an extensible open source platform to simplify the management of generative AI workloads. Built on Kubernetes, it enables you to efficiently handle both training and inference data within your own clusters. With &lt;a href="https://platform.openai.com/docs/api-reference/introduction">OpenAI-compatible APIs&lt;/a>, LLMariner leverages ecosystem of tools, facilitating seamless integration for a wide range of AI-driven applications.&lt;/p>







&lt;p class="mt-4 mb-4 text-center">&lt;img src="https://llmariner.ai/images/concepts.png" width="85%" height="539">&lt;/p>


&lt;h2 id="why-you-need-llmariner-and-what-it-can-do-for-you">Why You Need LLMariner, and What It Can Do for You&lt;a class="td-heading-self-link" href="#why-you-need-llmariner-and-what-it-can-do-for-you" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>As generative AI becomes more integral to business operations, a platform that can manage their lifecycle from data management to deployment is essential. LLMariner offers a unified solution that enables users to:&lt;/p></description></item><item><title>Weights &amp; Biases (W&amp;B)</title><link>https://llmariner.ai/docs/integration/wandb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/integration/wandb/</guid><description>&lt;p>&lt;a href="https://wandb.ai/">Weights and Biases (W&amp;amp;B)&lt;/a> is an AI developer platform. LLMariner provides the integration with W&amp;amp;B so that metrics for fine-tuning jobs are reported to W&amp;amp;B. With the integration, you can easily see the progress of your fine-tuning jobs, such as training epoch, loss, etc.&lt;/p>
&lt;p>Please take the following steps to enable the integration.&lt;/p>
&lt;p>First, obtain the API key of W&amp;amp;B and create a Kubernetes secret.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">kubectl create secret generic wandb
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -n &amp;lt;fine-tuning job namespace&amp;gt; &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --from-literal&lt;span class="o">=&lt;/span>&lt;span class="nv">apiKey&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="si">${&lt;/span>&lt;span class="nv">WANDB_API_KEY&lt;/span>&lt;span class="si">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The secret needs to be created in a namespace where fine-tuning jobs run. Individual projects specify namespaces for fine-tuning jobs, and the default project runs fine-tuning jobs in the &amp;quot;default&amp;quot; namespace.&lt;/p></description></item><item><title>Set up a Playground on a CPU-only Kind Cluster</title><link>https://llmariner.ai/docs/setup/install/cpu-only/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/cpu-only/</guid><description>&lt;p>Following this guide provides you with a simplified, local LLMariner installation by using the Kind and Helm. You can use this simple LLMariner deployment to try out features without GPUs.&lt;/p>


&lt;div class="alert alert-secondary" role="alert">
&lt;h4 class="alert-heading">Warn&lt;/h4>

 Playground environments are for experimentation use only. For a production-ready installation, please refere to the other installation guide.

&lt;/div>

&lt;h2 id="before-you-begin">Before you begin&lt;a class="td-heading-self-link" href="#before-you-begin" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Before you can get started with the LLMariner deployment you must install:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start">kind (Kubernetes in Docker)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://helmfile.readthedocs.io/en/latest/#installation">Helmfile&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="step-1-clone-the-repository">Step 1: Clone the repository&lt;a class="td-heading-self-link" href="#step-1-clone-the-repository" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>To get started, clone the LLMariner repository.&lt;/p></description></item><item><title>High-Level Architecture</title><link>https://llmariner.ai/docs/overview/how-works/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/overview/how-works/</guid><description>&lt;p>This page provides a high-level overview of the essential components that make up a LLMariner:&lt;/p>
&lt;!-- original file is located at diagram/llmariner.excalidraw -->







&lt;p class="mt-4 mb-4 text-center">&lt;img src="https://llmariner.ai/images/highlevel_architecture.png" width="2376" height="1542">&lt;/p>


&lt;h2 id="overall-design">Overall Design&lt;a class="td-heading-self-link" href="#overall-design" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLMariner consists of a control-plane and one or more worker-planes:&lt;/p>
&lt;dl>
&lt;dt>Control-Plane components&lt;/dt>
&lt;dd>Expose the OpenAI-compatible APIs and manage the overall state of LLMariner and receive a request from the client.&lt;/dd>
&lt;dt>Worker-Plane components&lt;/dt>
&lt;dd>Run every worker cluster, process tasks using compute resources such as GPUs in response to requests from the control-plane.&lt;/dd>
&lt;/dl>
&lt;h2 id="core-components">Core Components&lt;a class="td-heading-self-link" href="#core-components" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Here&amp;rsquo;s a brief overview of the main components:&lt;/p></description></item><item><title>Install in a Single EKS Cluster</title><link>https://llmariner.ai/docs/setup/install/single_cluster_eks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/single_cluster_eks/</guid><description>&lt;p>This page goes through the concrete steps to create an EKS cluster, create necessary resources, and install LLMariner. You can skip some of the steps if you have already made necessary installation/setup.&lt;/p>
&lt;h2 id="step-1-provision-an-eks-cluster">Step 1. Provision an EKS cluster&lt;a class="td-heading-self-link" href="#step-1-provision-an-eks-cluster" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="step-11-create-a-new-cluster-with-karpenter">Step 1.1. Create a new cluster with Karpenter&lt;a class="td-heading-self-link" href="#step-11-create-a-new-cluster-with-karpenter" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>Either follow the &lt;a href="https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/">Karpenter getting started guide&lt;/a> and create an EKS cluster with Karpenter, or run the following simplified installation steps.&lt;/p></description></item><item><title>Tutorials</title><link>https://llmariner.ai/docs/setup/tutorials/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/tutorials/</guid><description>&lt;h2 id="use-openai-python-library">Use OpenAI Python Library&lt;a class="td-heading-self-link" href="#use-openai-python-library" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>You can download the following Jupyter Notebook and exercise the tutorial:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/llmariner/llmariner/blob/main/tutorials/getting_started.ipynb">https://github.com/llmariner/llmariner/blob/main/tutorials/getting_started.ipynb&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="fine-tune-models">Fine-tune Models&lt;a class="td-heading-self-link" href="#fine-tune-models" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>This section goes through model fine-tuning using Llama3.2-1B-Instruct. We follow the following
page and create a model that answers questions in a sarcastic fashion.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7">https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>You can also check &lt;a href="https://vimeo.com/1065288371">this video&lt;/a> to see the overall flow.&lt;/p>
&lt;h3 id="step-1-download-a-base-model">Step 1. Download a base model&lt;a class="td-heading-self-link" href="#step-1-download-a-base-model" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>Change the &lt;code>model-manager-loader&lt;/code> config so that Llama3.2-1B-Instruct is downloaded to your S3 bucket.&lt;/p></description></item><item><title>Install in a Single On-premise Cluster</title><link>https://llmariner.ai/docs/setup/install/single_cluster_onpremise/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/single_cluster_onpremise/</guid><description>&lt;p>This page goes through the concrete steps to install LLMariner on a on-premise K8s cluster (or a local K8s cluster).
You can skip some of the steps if you have already made necessary installation/setup.&lt;/p>


&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Installation of Postgres, MinIO, SeaweedFS, and Milvus are just example purposes, and
they are not intended for the production usage.
Please configure based on your requirements if you want to use LLMariner for your production environment.

&lt;/div>

&lt;h2 id="step-1-install-nvidia-gpu-operator">Step 1. Install Nvidia GPU Operator&lt;a class="td-heading-self-link" href="#step-1-install-nvidia-gpu-operator" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Nvidia GPU Operator is required to install the device plugin and make GPU resources visible in the K8s cluster. Run:&lt;/p></description></item><item><title>Install across Multiple Clusters</title><link>https://llmariner.ai/docs/setup/install/multi_cluster_production/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/multi_cluster_production/</guid><description>&lt;p>LLMariner deploys Kubernetes deployments to provision the LLM stack. In a typical configuration, all the services are deployed into a single Kubernetes cluster, but you can also deploy these services on multiple Kubernetes clusters. For example, you can deploy a control plane component in a CPU K8s cluster and deploy the rest of the components in GPU compute clusters.&lt;/p>
&lt;p>LLMariner can be deployed into multiple GPU clusters, and the clusters can span across multiple cloud providers (including GPU specific clouds like CoreWeave) and on-prem.&lt;/p></description></item><item><title>Hosted Control Plane</title><link>https://llmariner.ai/docs/setup/install/hosted_control_plane/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/hosted_control_plane/</guid><description>&lt;p>CloudNatix provides a hosted control plane of LLMariner.&lt;/p>


&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Work-in-progress. This is not fully ready yet, and the terms and conditions are subject to change as we might limit the usage based on the number of API calls or the number of GPU nodes.

&lt;/div>

&lt;p>CloudNatix provides a hosted control plane of LLMariner. End users can use the full functionality of LLMariner just by registering their worker GPU clusters to this hosted control plane.&lt;/p></description></item><item><title>Roadmap</title><link>https://llmariner.ai/docs/dev/roadmap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/dev/roadmap/</guid><description>&lt;h2 id="milestone-0-completed">Milestone 0 (Completed)&lt;a class="td-heading-self-link" href="#milestone-0-completed" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> OpenAI compatible API&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Models: &lt;code>google-gemma-2b-it&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-1-completed">Milestone 1 (Completed)&lt;a class="td-heading-self-link" href="#milestone-1-completed" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> API authorization with Dex&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> API key management&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Quota management for fine-tuning jobs&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Inference autoscaling with GPU utilization&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Models: &lt;code>Mistral-7B-Instruct&lt;/code>, &lt;code>Meta-Llama-3-8B-Instruct&lt;/code>, and &lt;code>google-gemma-7b-it&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-2-completed">Milestone 2 (Completed)&lt;a class="td-heading-self-link" href="#milestone-2-completed" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Jupyter Notebook workspace creation&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Dynamic model loading &amp;amp; offloading in inference (initial version)&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Organization &amp;amp; project management&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> MLflow integration&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Weights &amp;amp; Biases integration for fine-tuning jobs&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> VectorDB installation and RAG&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Multi k8s cluster deployment (initial version)&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-3-completed">Milestone 3 (Completed)&lt;a class="td-heading-self-link" href="#milestone-3-completed" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Object store other than MinIO&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Multi-GPU general-purpose training jobs&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Inference optimization (e.g., vLLM)&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Models: &lt;code>Meta-Llama-3-8B-Instruct&lt;/code>, &lt;code>Meta-Llama-3-70B-Instruct&lt;/code>, &lt;code>deepseek-coder-6.7b-base&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-4-completed">Milestone 4 (Completed)&lt;a class="td-heading-self-link" href="#milestone-4-completed" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Embedding API&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> API usage visibility&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Fine-tuning support with vLLM&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> API key encryption&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Nvidia Triton Inference Server (experimental)&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Release flow&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-5-in-progress">Milestone 5 (In-progress)&lt;a class="td-heading-self-link" href="#milestone-5-in-progress" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Frontend&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> GPU showback&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Non-Nvidia GPU support&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Multi k8s cluster deployment (file and vector store management)&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> High availability&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Monitoring &amp;amp; alerting&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> More models&lt;/li>
&lt;/ul>
&lt;h2 id="milestone-6">Milestone 6&lt;a class="td-heading-self-link" href="#milestone-6" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Multi-GPU LLM fine-tuning jobs&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Events and metrics for fine-tuning jobs&lt;/li>
&lt;/ul></description></item><item><title>Search Results</title><link>https://llmariner.ai/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/search/</guid><description/></item></channel></rss>