<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://llmariner.ai/docs/><link rel=alternate type=application/rss+xml href=https://llmariner.ai/docs/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Welcome to LLMariner Documentation! | LLMariner</title>
<meta name=description content="Transform your GPU clusters into a powerhouse for generative AI workloads
"><meta property="og:url" content="https://llmariner.ai/docs/"><meta property="og:site_name" content="LLMariner"><meta property="og:title" content="Welcome to LLMariner Documentation!"><meta property="og:description" content="Transform your GPU clusters into a powerhouse for generative AI workloads"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Welcome to LLMariner Documentation!"><meta itemprop=description content="Transform your GPU clusters into a powerhouse for generative AI workloads"><meta itemprop=dateModified content="2024-10-28T14:28:54+09:00"><meta itemprop=wordCount content="46"><meta name=twitter:card content="summary"><meta name=twitter:title content="Welcome to LLMariner Documentation!"><meta name=twitter:description content="Transform your GPU clusters into a powerhouse for generative AI workloads"><link rel=preload href=/scss/main.min.f5f1bfbe5d5d7a6f0bb1b9d8ae3a3aee20f3a7a4d3e278a18d40b0d46e2b0ddb.css as=style><link href=/scss/main.min.f5f1bfbe5d5d7a6f0bb1b9d8ae3a3aee20f3a7a4d3e278a18d40b0d46e2b0ddb.css rel=stylesheet><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-JBN3K7Y529"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JBN3K7Y529")}</script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg id="Layer_2" viewBox="0 0 426 426"><defs><style>.cls-1{fill:none}.cls-2{fill:#2491eb}.cls-3{fill:#fff}.cls-4{fill:#102c3f}</style></defs><g id="Layer_4"><rect class="cls-1" width="426" height="426"/></g><g id="Layer_1-2" data-name="Layer_1"><g><path class="cls-3" d="M297.78 142.6c-.58-.78-.73-1.54-.75-2.49.1-6.62-.3-28.22.19-34.77.36-1.5 3.4-3.12 5.17-4.29 23.46-14.42 32.1-37.79 13.17-57.48-18.33-21.77-74.32-35.96-118.14-32.11-43.09 1.55-97.3 22.81-99.25 52.59-1.55 18.53 13.46 31.7 28.95 40.28 2.39 1.31 5.2 2.63 4.84 5.73.0 5.84.02 20.42-.02 25.02-.07 1.61-.28 2.3-1.29 3.29-15.69 14.17-34.21 39.77-40.92 64.65-26.6 91.07 58.97 179.78 151.47 157.86 101.16-23.37 134.6-150.45 56.63-218.18l-.07-.09z"/><path class="cls-2" d="M295.05 136.36c-6.98 7.43-15.99 13.84-26.47 18.86 25.74 17.44 42.65 46.92 42.65 80.36.0 53.57-43.43 97-97 97s-97-43.43-97-97c0-33.35 16.83-62.76 42.46-80.22-11.37-5.39-21.01-12.43-28.24-20.63-29.98 23.81-49.21 60.58-49.21 101.85.0 71.8 58.2 130 130 130s130-58.2 130-130c0-40.33-18.36-76.37-47.19-100.21z"/><path class="cls-4" d="M328.22 322.29c2.26 9.28 22.08 15.7 27.56 26.74 11.43 17.87-8.84 37.22-26.58 25.78-9.21-5.77-12.56-16.75-18.27-25.5-3.07-5.09-5.42-6.97-9.88-2.77-19.67 16.58-42.54 24.87-68.13 27.94-1.54.19-3.63.48-4.53 1.54-.85.88-.95 2.29-.64 3.96.56 2.75 1.39 5.54 2.13 8.24 1.08 3.89 2.14 7.73 2.63 11.69 1.59 10.12-3.55 22.67-14.13 25.25-11.45 2.91-22.27-4.78-24.06-16.58-3.76-15.85 17.86-32.59.2-34.5-25.37-3.81-49.39-12.98-69.68-28.64-1.18-.89-2.75-2.08-4.07-2.05-1.47-.08-2.74 1.36-3.58 2.59-7.88 11-15.62 26.79-28.27 32.28-13.22 4.53-26.6-10.3-22.79-23.2 1.49-4.91 4.92-9.41 8.61-12.86 7.37-6.76 15.69-8.23 23.69-15.72 2.93-2.48 2.89-5.82.44-8.61-12.26-16.48-20.13-36.23-23.49-56.51-.51-2.23-.77-5.47-2.6-6.59-3.89-2.56-13.17 3.97-23.23 5.6-10.93 2.71-26.13-7.21-26.02-17.93-.5-13.11 16.04-25.99 31.83-20.82 3.52.69 15.32 7.66 17.66 4.91.88-.89 1-2.9 1.13-4.31 1.68-22.16 9.69-40.84 22.64-57.67 2.95-3.16 2.15-6.87-.94-9.62-14.13-13.06-31.37-10.71-32.06-31.83.23-11.9 13.31-21.57 24.72-18.3 9.61 2.95 19.88 19.48 26.82 27.24 4.28 5.52 6.39 1.56 6.47-3.47.11-2.18.0-4.38-.03-6.56-.18-5.29.97-8.91-2.32-12.32-4.35-4.56-14.08-6.82-21.3-16.79-11.66-16.14-9.9-34.68 3.64-49.08 59.69-57.99 166.35-58.78 223.98 2.58 13.71 17.62 9.88 44.2-10.26 56.44-4.38 2.85-10.32 5.22-11.59 10.72-.57 2.66-2.2 15.99-.5 18.44 1.58 2.49 5.29 2.97 7.15.49 5.61-7.86 9.47-16.84 16.24-23.76 9.24-9.24 24.45-5.21 30.22 5.39 4.16 6.74 1.47 16.87-4.27 22.63-6.46 6.77-16.19 6.77-20.56 13.87-2.22 2.98-2.68 6.78-.19 10.38 6.9 11.29 12.76 21.3 16.83 33.96 6.86 19.21 1.69 38.69 16.61 30.76 3.53-1.54 7.43-3.03 11.22-3.8 12.14-3.09 24.91 2.93 28.49 15.03 1.76 6.25-1.28 12.83-6.08 16.92-8.93 8.27-22.48 7.22-33.02 1.67-9.96-4.57-11.47-1.12-12.86 8.35-2.66 19.7-13.47 36.69-23.43 53.55-.84 1.42-1.67 3.13-1.57 4.69v.16zM255.37 172.65c-4.73-1.2-9.2 1.26-13.84 1.99-5.54 1.28-11.12 2.49-16.77 3.25-15.09 2.06-30.02.48-44.74-3.24-3.57-1-7.36-1.42-10.69.49-27.47 16.28-42.82 49.15-34.71 80.26 2.06 8.06 5.88 15.95 10.9 22.79 74.63 104.21 220.91-27.66 110.04-105.47l-.2-.06zm-65.68-72.44c17.39.68 66.75-.25 86.69-1.01 3.66-.14 6.9-.4 10.37-1.58 8.23-2.85 17.06-8.72 22.57-15.18 8.14-9.47 3.88-19.85-4.65-27.71-31.19-26.75-72.1-37.53-112.62-32.33-27.92 3.81-91.19 31.28-80.66 52.87 4.98 8.33 15.55 19.06 24.6 23.03 3.06 1.17 6.58.96 9.91 1.01 15.65.15 28.01.8 43.66.89h.12zm-55.96 50.81c-1.49.03-3.11 1.28-4.29 2.38-16.62 15.03-27.34 34.45-32.39 55.24-11.92 47.94 7.66 103.54 51.97 128.04 123.31 73.93 249.68-75.84 149.57-185.06-2.96-3.34-5.63-3.99-8.6-.76-2.09 2.01-4.17 4.02-6.25 6.03-1.89 1.89-4.4 3.82-4.29 5.74.08 1.7 1.47 2.83 2.66 3.96 16.81 14.72 27.59 33.1 31.89 52.56 6.32 27.91-1.54 57.55-20.26 80.02-92.83 106.7-253.39-33.94-146.88-136.66.07-2.41-3.46-4.51-5.37-6.36-2.41-1.76-4.48-4.86-7.6-5.12h-.15zm10.66-29.55c12.91 49.6 121.56 47.77 138.36.78-.08-1.08-1.12-1.6-2.78-1.73-44.34-.17-87.73.0-131.3-.06-1.37-.02-3.56.03-4.21.92l-.07.09z"/><path class="cls-1" d="M255.06 172.51c3.11 1 5.1 3.89 7.68 5.87 12.2 9.18 22.72 21.19 27.47 34.59 9.65 25.89 4.17 59.63-16.42 79.2-49.47 48.7-138.36 17.5-141.36-53.5-.51-21.56 10.48-45.01 29-57.42 3.79-2.39 6.89-6.14 11.27-7.25 2.39-.53 5.1.12 7.53.72 14.71 3.7 29.65 5.25 44.74 3.16 5.77-.79 11.48-2.04 17.15-3.36 4.2-.7 8.58-2.83 12.78-2.08l.16.04zM253.92 217.15c-5.98 5.08-11.51 13.48-9.39 21.64 2.63 11.24 15.82 17.72 26.77 13.38 11.89-4.27 15.5-19.61 7.45-29.2-5.5-6.95-17.27-11.5-24.68-5.94l-.15.11zM157.76 215.76c.73 1.63 3.03 2.37 4.11 3.83 3.84 4.02.04 10.35-4.72 11.56-2.8.8-6.03-1.11-7.71-3.69-.91-1.06-1.58-4.08-3.32-3.24-2.49 2.4-2.98 6.35-3.1 9.73-.16 22.97 30.48 26.53 38.81 6.32 4.3-11.15-4.19-22.18-14.85-25.35-2.13-.78-8.67-1.52-9.24.77v.08zm55.66 46.66c11.38.44 23.08-10.59 22.79-22.87-.5-5.19-7.28-9.61-11.86-6.63-2.51 1.94-3.32 6.23-5.23 8.83-2.44 3.85-8.81 3.79-11.32.07-1.87-2.44-2.4-6.42-4.56-8.34-2.61-2.41-7.59-1.5-9.91.91-2.13 2.07-2.78 5.35-2.58 8.26.68 11.08 11.22 19.81 22.46 19.76h.2z"/><path class="cls-1" d="M130.11 155.2c3.26-4.15 5.35-3.62 8.84-.31 2.31 2.1 5.05 4.13 7.21 6.44 1.89 1.98.89 4.33-.89 6.15-3.73 4.16-7.59 8.22-11.4 12.5-11.73 13.2-19.15 30.13-21.28 47.31-5.87 47.02 22.26 88.14 67.08 103.74 35.43 13.95 80.64 5.14 108.39-24.7 11.27-12.45 20.44-26.88 24.75-43.06 7.53-29.79 2.72-62.48-20.45-84.92-4.08-4.31-8.43-8.22-12.26-12.74-2.34-3.17.92-5.59 3.5-8.11 1.85-1.79 3.7-3.57 5.55-5.35 1.86-1.82 4.31-4.52 6.77-2.11 9.58 12.05 19.26 24.15 26.69 37.6 30.94 56.42 1.86 132.89-56.2 157.48-31.35 15.05-66.94 15.14-99.04.85-35.74-14.59-62.55-43.95-70.7-81.16"/><path class="cls-4" d="M213.22 262.42c-11.24.05-21.78-8.68-22.46-19.76-.2-2.91.45-6.19 2.58-8.26 2.32-2.41 7.3-3.32 9.91-.91 2.17 1.92 2.69 5.9 4.56 8.34 2.51 3.72 8.88 3.78 11.32-.07 1.92-2.6 2.72-6.89 5.23-8.83 4.58-2.99 11.35 1.44 11.86 6.62.28 12.29-11.41 23.31-22.79 22.87h-.2z"/><path class="cls-3" d="M254.27 217.19c4.9-.94 10.11 4.79 8.25 9.59-.66 1.93-1.85 4.11-3.76 4.96-2.74.93-7.01-.47-8.16-3.28-.92-3.73-.33-9.42 3.49-11.22l.17-.06z"/><path class="cls-2" d="M195.48 62.7c-.11-2.8.2-5.67.48-8.47.37-3.4.72-7.12 2.6-10.02 2.07-3.27 6.23-5.17 10.03-4.41 8.61 2.16 7.02 13.36 7.15 20.23-1.12 15.67 7.22 6.68 16 10.39 5.24 2.35 4.12 9.37.69 12.85-6.99 7.97-23.36 10.29-31.51 1.67-5.8-5.63-5.22-14.71-5.43-22.05v-.18z"/></g><circle class="cls-4" cx="162.5" cy="234.5" r="19.5"/><circle class="cls-4" cx="262.5" cy="234.5" r="19.5"/><circle class="cls-3" cx="254.5" cy="224.5" r="7.5"/><circle class="cls-3" cx="154.5" cy="223.5" r="7.5"/></g></svg></span><span class=navbar-brand__name>LLMariner</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class="nav-link active" href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/llmariner/llmariner target=_blank rel=noopener><span>GitHub</span><sup><i class="ps-1 fa-solid fa-up-right-from-square fa-xs" aria-hidden=true></i></sup></a></li><li class="td-light-dark-menu nav-item dropdown"><svg class="d-none"><symbol id="check2" viewBox="0 0 16 16"><path d="M13.854 3.646a.5.5.0 010 .708l-7 7a.5.5.0 01-.708.0l-3.5-3.5a.5.5.0 11.708-.708L6.5 10.293l6.646-6.647a.5.5.0 01.708.0z"/></symbol><symbol id="circle-half" viewBox="0 0 16 16"><path d="M8 15A7 7 0 108 1v14zm0 1A8 8 0 118 0a8 8 0 010 16z"/></symbol><symbol id="moon-stars-fill" viewBox="0 0 16 16"><path d="M6 .278a.768.768.0 01.08.858 7.208 7.208.0 00-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527.0 1.04-.055 1.533-.16a.787.787.0 01.81.316.733.733.0 01-.031.893A8.349 8.349.0 018.344 16C3.734 16 0 12.286.0 7.71.0 4.266 2.114 1.312 5.124.06A.752.752.0 016 .278z"/><path d="M10.794 3.148a.217.217.0 01.412.0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217.0 010 .412l-1.162.387A1.734 1.734.0 0011.593 7.69l-.387 1.162a.217.217.0 01-.412.0l-.387-1.162A1.734 1.734.0 009.31 6.593l-1.162-.387a.217.217.0 010-.412l1.162-.387a1.734 1.734.0 001.097-1.097l.387-1.162zM13.863.099a.145.145.0 01.274.0l.258.774c.115.346.386.617.732.732l.774.258a.145.145.0 010 .274l-.774.258a1.156 1.156.0 00-.732.732l-.258.774a.145.145.0 01-.274.0l-.258-.774a1.156 1.156.0 00-.732-.732l-.774-.258a.145.145.0 010-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"/></symbol><symbol id="sun-fill" viewBox="0 0 16 16"><path d="M8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 0zm0 13a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 13zm8-5a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2a.5.5.0 01.5.5zM3 8a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2A.5.5.0 013 8zm10.657-5.657a.5.5.0 010 .707l-1.414 1.415a.5.5.0 11-.707-.708l1.414-1.414a.5.5.0 01.707.0zm-9.193 9.193a.5.5.0 010 .707L3.05 13.657a.5.5.0 01-.707-.707l1.414-1.414a.5.5.0 01.707.0zm9.193 2.121a.5.5.0 01-.707.0l-1.414-1.414a.5.5.0 01.707-.707l1.414 1.414a.5.5.0 010 .707zM4.464 4.465a.5.5.0 01-.707.0L2.343 3.05a.5.5.0 11.707-.707l1.414 1.414a.5.5.0 010 .708z"/></symbol></svg>
<button class="btn btn-link nav-link dropdown-toggle d-flex align-items-center" id=bd-theme type=button aria-expanded=false data-bs-toggle=dropdown data-bs-display=static aria-label="Toggle theme (auto)"><svg class="bi my-1 theme-icon-active"><use href="#circle-half"/></svg></button><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=bd-theme-text><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=light aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#sun-fill"/></svg>
Light<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=dark aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#moon-stars-fill"/></svg>
Dark<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center active" data-bs-theme-value=auto aria-pressed=true>
<svg class="bi me-2 opacity-50"><use href="#circle-half"/></svg>
Auto<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li></ul></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.61a7d3034cee2ee160af0d167b8e99cf.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/>Return to the regular view of this page</a>.</p></div><h1 class=title>Welcome to LLMariner Documentation!</h1><div class=lead>Transform your GPU clusters into a powerhouse for generative AI workloads</div><ul><li>1: <a href=#pg-6557438925ac398717df86f494c6abb8>Overview</a></li><ul><li>1.1: <a href=#pg-a458178c7781d257c919d1f6c28f6650>Why LLMariner?</a></li><li>1.2: <a href=#pg-3a950f9f5941e5fdc692f7addf798fc4>High-Level Architecture</a></li></ul><li>2: <a href=#pg-66b565805ca1061be35ff2c0165f13c1>Getting Started</a></li><ul><li>2.1: <a href=#pg-855651eded50e67ad419215904cd5a8c>Installation</a></li><ul><li>2.1.1: <a href=#pg-80cf3fa410b01619b38e4057af7ec5c8>Install with Helm</a></li><li>2.1.2: <a href=#pg-fad99d871e69bfc31c6b63374b6793ea>Set up a Playground on a GPU EC2 Instance</a></li><li>2.1.3: <a href=#pg-2822b0bc47bba2d10518e57315f06e35>Set up a Playground on a CPU-only Kind Cluster</a></li><li>2.1.4: <a href=#pg-8de5d0eb22b83ad8b3efcf34d11c8f4b>Install in a Single EKS Cluster</a></li><li>2.1.5: <a href=#pg-b97c20cd3e745f78416fc8eddf907efc>Install in a Single On-premise Cluster</a></li><li>2.1.6: <a href=#pg-830457b369082df60146f171d2fecbac>Install across Multiple Clusters</a></li><li>2.1.7: <a href=#pg-aa1273494c214f035a8e9386ce6707b0>Hosted Control Plane</a></li></ul><li>2.2: <a href=#pg-270067265d5e2225da8588f16967a603>Tutorials</a></li></ul><li>3: <a href=#pg-56dc2b1c45b2c6b0983526530ba75d01>Features</a></li><ul><li>3.1: <a href=#pg-e254f2bb7611309715fbd117e4ceb3b7>Inference with Open Models</a></li><li>3.2: <a href=#pg-a90036a92e39d96ca98dbb924258b27b>Supported Open Models</a></li><li>3.3: <a href=#pg-559ba074f66298c7e9908d15750ab530>Retrieval-Augmented Generation (RAG)</a></li><li>3.4: <a href=#pg-e543d8263acb53250256de185d42cd21>Model Fine-tuning</a></li><li>3.5: <a href=#pg-0eae5360aca6a618ec933d565b825e7e>General-purpose Training</a></li><li>3.6: <a href=#pg-28b6f41d7083500232434001016b060d>Jupyter Notebook</a></li><li>3.7: <a href=#pg-801a2e5cd9c3c1c32f39935005c3f24e>API and GPU Usage Optimization</a></li><li>3.8: <a href=#pg-728e0b08215c55af1b72e7bae479d8a2>User Management</a></li><li>3.9: <a href=#pg-dac07794e5c1cd0aa1170dedef9d101d>Access Control with Organizations and Projects</a></li></ul><li>4: <a href=#pg-52f3e6a6d26d58b3ca63890586812ed7>Integration</a></li><ul><li>4.1: <a href=#pg-3262aec6d92a36c90b5ff177e164086a>Open WebUI</a></li><li>4.2: <a href=#pg-acd15456d1ec0db7c8fc89d3e94c2d27>Continue</a></li><li>4.3: <a href=#pg-9da4b3679d05a1e4de898c134966c56e>Aider</a></li><li>4.4: <a href=#pg-923663fb245c62d0612c79e0f6b52ba5>AI Shell</a></li><li>4.5: <a href=#pg-aa09c8195c9d19a9308971cb82ac7e53>k8sgpt</a></li><li>4.6: <a href=#pg-55c6586ef1140acfb8e48058350d103a>Dify</a></li><li>4.7: <a href=#pg-66e3bf8dad85a9252eb8b996c935ef64>n8n</a></li><li>4.8: <a href=#pg-382415db5d1ad482de2bd54181991589>Slackbot</a></li><li>4.9: <a href=#pg-e6225eb8b68621a923dbfdca6dd573ca>MLflow</a></li><li>4.10: <a href=#pg-484e4e9dc5d71bd6e9a10c85edd5bc6e>Langfuse</a></li><li>4.11: <a href=#pg-5a61ba5d4376fbae7610a65a0d32db3b>Weights & Biases (W&amp;B)</a></li></ul><li>5: <a href=#pg-cde904b13b4c787d29f40d57853861f5>Development</a></li><ul><li>5.1: <a href=#pg-c174fff464205008bddc116b984cc877>Technical Details</a></li><li>5.2: <a href=#pg-998d0585ca29638d7ea6bebdd5056382>Roadmap</a></li></ul></ul><div class=content><p>Do you want an API compatible with OpenAI to leverage the extensive GenAI ecosystem? If so, LLMariner is what you need. It instantly builds a software stack that provides an OpenAI-compatible API for inference, fine-tuning, and model management. Please see the presentation below to learn more:</p><p class="mt-4 mb-4 text-center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/dCNwoMKEiuYfnd?hostedIn=slideshare&page=upload" width=571 height=384 frameborder=0 marginwidth=0 marginheight=0 scrolling=no></iframe></p></div></div><div class=td-content><h1 id=pg-6557438925ac398717df86f494c6abb8>1 - Overview</h1><div class=lead>A high-level introduction to LLMariner.</div></div><div class=td-content><h1 id=pg-a458178c7781d257c919d1f6c28f6650>1.1 - Why LLMariner?</h1><div class=lead>Why you need LLMariner and what it can do for you?</div><p><strong>LLMariner</strong> (= LLM + Mariner) is an extensible open source platform to simplify the management of generative AI workloads. Built on Kubernetes, it enables you to efficiently handle both training and inference data within your own clusters. With <a href=https://platform.openai.com/docs/api-reference/introduction>OpenAI-compatible APIs</a>, LLMariner leverages ecosystem of tools, facilitating seamless integration for a wide range of AI-driven applications.</p><p class="mt-4 mb-4 text-center"><img src=/images/concepts.png width=85% height=539></p><h2 id=why-you-need-llmariner-and-what-it-can-do-for-you>Why You Need LLMariner, and What It Can Do for You<a class=td-heading-self-link href=#why-you-need-llmariner-and-what-it-can-do-for-you aria-label="Heading self-link"></a></h2><p>As generative AI becomes more integral to business operations, a platform that can manage their lifecycle from data management to deployment is essential. LLMariner offers a unified solution that enables users to:</p><ul><li><strong>Centralize Model Management</strong>: Manage data, resources, and AI model lifecycles all in one place, reducing the overhead of fragmented systems.</li><li><strong>Utilize an Existing Ecosystem</strong>: LLMariner&rsquo;s OpenAI-compatible APIs make it easy to integrate with popular AI tools, such as assistant web UIs, code generation tools, and more.</li><li><strong>Optimize Resource Utilization</strong>: Its Kubernetes-based architecture enables efficient scaling and resource management in response to user demands.</li></ul><h2 id=why-choose-llmariner>Why Choose LLMariner<a class=td-heading-self-link href=#why-choose-llmariner aria-label="Heading self-link"></a></h2><p>LLMariner stands out with its focus on extensibility, compatibility, and scalability:</p><ul><li><strong>Open Ecosystem</strong>: By aligning with OpenAI&rsquo;s API standards, LLMariner allows you to use a vast array of tools, enabling diverse use cases from conversational AI to intelligent code assistance.</li><li><strong>Kubernetes-Powered Scalability</strong>: Leveraging Kubernetes ensures that LLMariner remains efficient, scalable, and adaptable to changing resource demands, making it suitable for teams of any size.</li><li><strong>Customizable and Extensible</strong>: Built with openness in mind, LLMariner can be customized to fit specific workflows, empowering you to build upon its core for unique applications.</li></ul><h2 id=whats-next>What&rsquo;s Next<a class=td-heading-self-link href=#whats-next aria-label="Heading self-link"></a></h2><ul><li>Take a look at <a href=https://llmariner.ai/docs/overview/how-works/>High-Level Architecture</a></li><li>Take a look at LLMariner core <a href=https://llmariner.ai/docs/features/>Features</a></li><li>Ready to <a href=https://llmariner.ai/docs/setup/>Get Started</a>?</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3a950f9f5941e5fdc692f7addf798fc4>1.2 - High-Level Architecture</h1><div class=lead>An overview of the key components that make up a LLMarinr.</div><p>This page provides a high-level overview of the essential components that make up a LLMariner:</p><p class="mt-4 mb-4 text-center"><img src=/images/highlevel_architecture.png width=2376 height=1542></p><h2 id=overall-design>Overall Design<a class=td-heading-self-link href=#overall-design aria-label="Heading self-link"></a></h2><p>LLMariner consists of a control-plane and one or more worker-planes:</p><dl><dt>Control-Plane components</dt><dd>Expose the OpenAI-compatible APIs and manage the overall state of LLMariner and receive a request from the client.</dd><dt>Worker-Plane components</dt><dd>Run every worker cluster, process tasks using compute resources such as GPUs in response to requests from the control-plane.</dd></dl><h2 id=core-components>Core Components<a class=td-heading-self-link href=#core-components aria-label="Heading self-link"></a></h2><p>Here&rsquo;s a brief overview of the main components:</p><dl><dt>Inference Manager</dt><dd>Manage inference runtimes (e.g., vLLM and Ollama) in containers, load models, and process requests. Also, auto-scale runtimes based on the number of in-flight requests.</dd><dt>Job Manager</dt><dd>Run fine-tuning or training jobs based on requests, and launch Jupyter Notebooks.</dd><dt>Session Manager</dt><dd>Forwards requests from the client to the worker cluster that need the Kubernetes API, like displaying Job logs.</dd><dt>Data Managers</dt><dd>Manage models, files, and vector data for RAG.</dd><dt>Auth Managers</dt><dd>Manage information such as users, organizations, and clusters, and perform authentication and role-based access control for API requests.</dd></dl><h2 id=whats-next>What&rsquo;s Next<a class=td-heading-self-link href=#whats-next aria-label="Heading self-link"></a></h2><ul><li>Ready to <a href=https://llmariner.ai/docs/setup/>Get Started</a>?</li><li>Take a look at the <a href=https://llmariner.ai/docs/dev/architecture/>Technical Details</a></li><li>Take a look at LLMariner core <a href=https://llmariner.ai/docs/features/>Features</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-66b565805ca1061be35ff2c0165f13c1>2 - Getting Started</h1><div class=lead>Get LLMariner running based on your resources and needs.</div></div><div class=td-content><h1 id=pg-855651eded50e67ad419215904cd5a8c>2.1 - Installation</h1><div class=lead>Choose the guide that best suits your needs and platform.</div><p>LLMariner takes ControlPlane-Worker model. The control plane gets a request and gives instructions to the worker while the worker processes a task such as inference.</p><p>Both components can operate within a single cluster, but if you want to utilize GPU resources across multiple clusters, they can also be installed into separate clusters.</p><p class="mt-4 mb-4 text-center"><img src=/images/install_modes.png width=2504 height=773></p></div><div class=td-content style=page-break-before:always><h1 id=pg-80cf3fa410b01619b38e4057af7ec5c8>2.1.1 - Install with Helm</h1><div class=lead>Install LLMariner with Helm.</div><h2 id=prerequisites>Prerequisites<a class=td-heading-self-link href=#prerequisites aria-label="Heading self-link"></a></h2><p>LLMariner requires the following resources:</p><ul><li><a href=https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html>Nvidia GPU Operator</a></li><li>Ingress controller (to route API requests)</li><li>SQL database (to store jobs/models/files metadata)</li><li>S3-compatible object store (to store training files and models)</li><li><a href=https://milvus.io/>Milvus</a> (for RAG, optional)</li></ul><p>LLMariner can process inference requests on CPU nodes, but it can be best used with GPU nodes. Nvidia GPU Operator is required to install the device plugin and make GPUs visible in the K8s cluster.</p><p>Preferably the ingress controller should have a DNS name or an IP that is reachable from the outside of the EKS cluster. If not, you can rely on port-forwarding to reach the API endpoints.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>When port-forwarding is used, the same port needs to be used consistently as the port number will be included the OIDC issuer URL. We will explain details later.</div><p>You can provision RDS and S3 in AWS, or you can deploy Postgres and <a href=https://min.io/>MinIO</a> inside your EKS cluster.</p><h2 id=install-with-helm>Install with Helm<a class=td-heading-self-link href=#install-with-helm aria-label="Heading self-link"></a></h2><p>We provide a Helm chart for installing LLMariner. You can obtain the Helm chart from our repository and install.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Logout of helm registry to perform an unauthenticated pull against the public ECR</span>
</span></span><span class=line><span class=cl>helm registry <span class=nb>logout</span> public.ecr.aws
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace &lt;namespace&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  llmariner oci://public.ecr.aws/cloudnatix/llmariner-charts/llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --values &lt;values.yaml&gt;
</span></span></code></pre></div><p>Once installation completes, you can interact with the API endpoint using the <a href=https://github.com/openai/openai-python>OpenAI Python library</a>, running our CLI, or directly hitting the endpoint. To download the CLI, run:</p><ul class="nav nav-tabs" id=tabs-0 role=tablist><li class=nav-item><button class="nav-link disabled" id=tabs-00-00-tab data-bs-toggle=tab data-bs-target=#tabs-00-00 role=tab aria-controls=tabs-00-00 aria-selected=false>
<strong>Install From</strong>:</button></li><li class=nav-item><button class="nav-link active" id=tabs-00-01-tab data-bs-toggle=tab data-bs-target=#tabs-00-01 role=tab data-td-tp-persist=script aria-controls=tabs-00-01 aria-selected=true>
Script</button></li><li class=nav-item><button class=nav-link id=tabs-00-02-tab data-bs-toggle=tab data-bs-target=#tabs-00-02 role=tab data-td-tp-persist=homebrew aria-controls=tabs-00-02 aria-selected=false>
Homebrew</button></li><li class=nav-item><button class=nav-link id=tabs-00-03-tab data-bs-toggle=tab data-bs-target=#tabs-00-03 role=tab data-td-tp-persist=go aria-controls=tabs-00-03 aria-selected=false>
Go</button></li><li class=nav-item><button class=nav-link id=tabs-00-04-tab data-bs-toggle=tab data-bs-target=#tabs-00-04 role=tab data-td-tp-persist=binary aria-controls=tabs-00-04 aria-selected=false>
Binary</button></li></ul><div class=tab-content id=tabs-0-content><div class="tab-body tab-pane fade" id=tabs-00-00 role=tabpanel aria-labelled-by=tabs-00-00-tab tabindex=0></div><div class="tab-body tab-pane fade show active" id=tabs-00-01 role=tabpanel aria-labelled-by=tabs-00-01-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl --silent https://llmariner.ai/get-cli <span class=p>|</span> bash
</span></span><span class=line><span class=cl>mv llma &lt;your/PATH&gt;
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-02 role=tabpanel aria-labelled-by=tabs-00-02-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>brew install llmariner/tap/llma
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-03 role=tabpanel aria-labelled-by=tabs-00-03-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>go install github.com/llmariner/llmariner/cli/cmd@latest
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-04 role=tabpanel aria-labelled-by=tabs-00-04-tab tabindex=0><p>Download the binary from <a href=https://github.com/llmariner/llmariner/releases/latest>GitHub Release Page</a>.</p></div></div></div><div class=td-content style=page-break-before:always><h1 id=pg-fad99d871e69bfc31c6b63374b6793ea>2.1.2 - Set up a Playground on a GPU EC2 Instance</h1><div class=lead>Set up the playground environment on an Amazon EC2 instance with GPUs.</div><p>You can easily set up a playground for LLMariner and learn it. In this page, we provision an EC2 instance, build a <a href=https://kind.sigs.k8s.io/>Kind</a> cluster, and deploy LLMariner and other required components.</p><div class="alert alert-secondary" role=alert><h4 class=alert-heading>Warn</h4>Playground environments are for experimentation use only. For a production-ready installation, please refere to the other installation guide.</div><p>Once all the setup completes, you can interact with the LLM service by directly hitting the API endpoints or using <a href=https://github.com/openai/openai-python>the OpenAI Python library</a>.</p><h2 id=step-1-install-terraform-and-ansible>Step 1: Install Terraform and Ansible<a class=td-heading-self-link href=#step-1-install-terraform-and-ansible aria-label="Heading self-link"></a></h2><p>We use Terraform and Ansible. Follow the links to install if you haven't.</p><ul><li><a href=https://developer.hashicorp.com/terraform/install>Terraform</a></li><li><a href=https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html>Ansible</a></li><li><a href=https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html>kubernetes.core.k8s module for Ansible</a></li></ul><p>To install <code>kubernetes.core.k8s</code> module, run the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ansible-galaxy collection install kubernetes.core
</span></span></code></pre></div><h2 id=step-2-clone-the-llmariner-repository>Step 2: Clone the LLMariner Repository<a class=td-heading-self-link href=#step-2-clone-the-llmariner-repository aria-label="Heading self-link"></a></h2><p>We use the Terraform configuration and Ansible playbook in the <a href=https://github.com/llmariner/llmariner>LLMariner repository</a>. Run the following commands to clone the repo and move to the directory where the Terraform configuration file is stored.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/llmariner/llmariner.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> llmariner/provision/aws
</span></span></code></pre></div><h2 id=step-3-run-terraform>Step 3: Run Terraform<a class=td-heading-self-link href=#step-3-run-terraform aria-label="Heading self-link"></a></h2><p>First create a <code>local.tfvars</code> file for your deployment. Here is an example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-terraform data-lang=terraform><span class=line><span class=cl><span class=na>project_name</span> = <span class=s2>&#34;&lt;instance-name&gt; (default: &#34;</span><span class=nx>llmariner</span><span class=o>-</span><span class=nx>demo</span><span class=s2>&#34;)&#34;</span>
</span></span><span class=line><span class=cl><span class=na>profile</span>      = <span class=s2>&#34;&lt;aws-profile&gt;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=na>public_key_path</span>  = <span class=s2>&#34;&lt;/path/to/public_key_path&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=na>private_key_path</span> = <span class=s2>&#34;&lt;/path/to/private_key_path&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=na>ssh_ip_range</span>     = <span class=s2>&#34;&lt;ingress CIDR block for SSH (default: &#34;</span><span class=m>0</span><span class=p>.</span><span class=m>0</span><span class=p>.</span><span class=m>0</span><span class=p>.</span><span class=m>0</span><span class=o>/</span><span class=m>0</span><span class=s2>&#34;)&gt;&#34;</span>
</span></span></code></pre></div><p><code>profile</code> is an AWS profile that is used to create an EC2 instance. <code>public_key_path</code> and <code>private_key_path</code> specify an SSH key used to access the EC2 instance.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>See <code>variables.tf</code> for other customizable and default values.</div><p>Then, run the following Terraform commands to initialize and create an EC2 instance. This will approximately take 10 minutes.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>terraform init
</span></span><span class=line><span class=cl>terraform apply -var-file<span class=o>=</span>local.tfvars
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>If you want to run only the Ansible playbook, you can just run <code>ansible-playbook -i inventory.ini playbook.yml</code>.</div><p>Once the deployment completes, a Kind cluster is built in the EC2 instance and LLMariner is running in the cluster. It will take another about five minutes for LLMariner to load base models, but you can move to the next step meanwhile.</p><h2 id=step-4-set-up-ssh-connection>Step 4: Set up SSH Connection<a class=td-heading-self-link href=#step-4-set-up-ssh-connection aria-label="Heading self-link"></a></h2><p>You can access the API endpoint and Grafana by establishing SSH port-forwarding.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ansible all <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -i inventory.ini <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --ssh-extra-args<span class=o>=</span><span class=s2>&#34;-L8080:localhost:80 -L8081:localhost:8081&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -a <span class=s2>&#34;kubectl port-forward -n monitoring service/grafana 8081:80&#34;</span>
</span></span></code></pre></div><p>With the above command, you can hit the API via <code>http://localhost:8080</code>. You can directly hit the endpoint via <code>curl</code> or other commands, or you can use the <a href=https://github.com/openai/openai-python>OpenAI Python library</a>.</p><p>You can also reach Grafana at <code>http://localhost:8081</code>. The login username is <code>admin</code>, and the password can be obtained with the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ansible all <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -i inventory.ini <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -a <span class=s2>&#34;kubectl get secrets -n monitoring grafana -o jsonpath=&#39;{.data.admin-password}&#39;&#34;</span> <span class=p>|</span> tail -1 <span class=p>|</span> base64 --decode<span class=p>;</span> <span class=nb>echo</span>
</span></span></code></pre></div><h2 id=step-5-obtain-an-api-key>Step 5: Obtain an API Key<a class=td-heading-self-link href=#step-5-obtain-an-api-key aria-label="Heading self-link"></a></h2><p>To access LLM service, you need an API key. You can download the LLMariner CLI and use that to login the system, and obtain the API key.</p><ul class="nav nav-tabs" id=tabs-0 role=tablist><li class=nav-item><button class="nav-link disabled" id=tabs-00-00-tab data-bs-toggle=tab data-bs-target=#tabs-00-00 role=tab aria-controls=tabs-00-00 aria-selected=false>
<strong>Install From</strong>:</button></li><li class=nav-item><button class="nav-link active" id=tabs-00-01-tab data-bs-toggle=tab data-bs-target=#tabs-00-01 role=tab data-td-tp-persist=script aria-controls=tabs-00-01 aria-selected=true>
Script</button></li><li class=nav-item><button class=nav-link id=tabs-00-02-tab data-bs-toggle=tab data-bs-target=#tabs-00-02 role=tab data-td-tp-persist=homebrew aria-controls=tabs-00-02 aria-selected=false>
Homebrew</button></li><li class=nav-item><button class=nav-link id=tabs-00-03-tab data-bs-toggle=tab data-bs-target=#tabs-00-03 role=tab data-td-tp-persist=go aria-controls=tabs-00-03 aria-selected=false>
Go</button></li><li class=nav-item><button class=nav-link id=tabs-00-04-tab data-bs-toggle=tab data-bs-target=#tabs-00-04 role=tab data-td-tp-persist=binary aria-controls=tabs-00-04 aria-selected=false>
Binary</button></li></ul><div class=tab-content id=tabs-0-content><div class="tab-body tab-pane fade" id=tabs-00-00 role=tabpanel aria-labelled-by=tabs-00-00-tab tabindex=0></div><div class="tab-body tab-pane fade show active" id=tabs-00-01 role=tabpanel aria-labelled-by=tabs-00-01-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl --silent https://llmariner.ai/get-cli <span class=p>|</span> bash
</span></span><span class=line><span class=cl>mv llma &lt;your/PATH&gt;
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-02 role=tabpanel aria-labelled-by=tabs-00-02-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>brew install llmariner/tap/llma
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-03 role=tabpanel aria-labelled-by=tabs-00-03-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>go install github.com/llmariner/llmariner/cli/cmd@latest
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-04 role=tabpanel aria-labelled-by=tabs-00-04-tab tabindex=0><p>Download the binary from <a href=https://github.com/llmariner/llmariner/releases/latest>GitHub Release Page</a>.</p></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Login. Please see below for the details.</span>
</span></span><span class=line><span class=cl>llma auth login
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create an API key.</span>
</span></span><span class=line><span class=cl>llma auth api-keys create my-key
</span></span></code></pre></div><p><code>llma auth login</code> will ask for the endpoint URL and the issuer URL. Please use the default values for them (<code>http://localhost:8080/v1</code> and <code>http://kong-proxy.kong/v1/dex</code>).</p><p>Then the command will open a web browser to login. Please use the following username and the password.</p><ul><li>Username: <code>admin@example.com</code></li><li>Password: <code>password</code></li></ul><p>The output of <code>llma auth api-keys create</code> contains the secret of the created API key. Please save the value in the environment variable to use that in the following step:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_TOKEN</span><span class=o>=</span>&lt;Secret obtained from llma auth api-keys create&gt;
</span></span></code></pre></div><h2 id=step-6-interact-with-the-llm-service>Step 6: Interact with the LLM Service<a class=td-heading-self-link href=#step-6-interact-with-the-llm-service aria-label="Heading self-link"></a></h2><p>There are mainly three ways to interact with the LLM service.</p><p>The first option is to use the CLI. Here are example commands:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma models list
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion <span class=s2>&#34;What is k8s?&#34;</span>
</span></span></code></pre></div><p>The second option is to run the <code>curl</code> command and hit the API endpoint. Here is an example command for listing all available models and hitting the chat endpoint.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:8080/v1/models <span class=p>|</span> jq
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data <span class=s1>&#39;{&#34;model&#34;: &#34;google-gemma-2b-it-q4_0&#34;, &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is k8s?&#34;}]}&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:8080/v1/chat/completions
</span></span></code></pre></div><p>The third option is to use Python. Here is an example Python code for hitting the chat endpoint.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>os</span> <span class=kn>import</span> <span class=n>environ</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;http://localhost:8080/v1&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;LLMARINER_TOKEN&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it-q4_0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;What is k8s?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>response</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Please visit <code>tutorials</code>{.interpreted-text role=&ldquo;doc&rdquo;} to further exercise LLMariner.</p><h2 id=step-7-clean-up>Step 7: Clean up<a class=td-heading-self-link href=#step-7-clean-up aria-label="Heading self-link"></a></h2><p>Run the following command to destroy the EC2 instance.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>terraform destroy -var-file<span class=o>=</span>local.tfvars
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2822b0bc47bba2d10518e57315f06e35>2.1.3 - Set up a Playground on a CPU-only Kind Cluster</h1><div class=lead>Set up the playground environment on a local kind cluster (CPU-only).</div><p>Following this guide provides you with a simplified, local LLMariner installation by using the Kind and Helm. You can use this simple LLMariner deployment to try out features without GPUs.</p><div class="alert alert-secondary" role=alert><h4 class=alert-heading>Warn</h4>Playground environments are for experimentation use only. For a production-ready installation, please refere to the other installation guide.</div><h2 id=before-you-begin>Before you begin<a class=td-heading-self-link href=#before-you-begin aria-label="Heading self-link"></a></h2><p>Before you can get started with the LLMariner deployment you must install:</p><ul><li><a href=https://kind.sigs.k8s.io/docs/user/quick-start>kind (Kubernetes in Docker)</a></li><li><a href=https://helmfile.readthedocs.io/en/latest/#installation>Helmfile</a></li></ul><h2 id=step-1-clone-the-repository>Step 1: Clone the repository<a class=td-heading-self-link href=#step-1-clone-the-repository aria-label="Heading self-link"></a></h2><p>To get started, clone the LLMariner repository.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/llmariner/llmariner.git
</span></span></code></pre></div><h2 id=step-2-create-a-kind-cluster>Step 2: Create a kind cluster<a class=td-heading-self-link href=#step-2-create-a-kind-cluster aria-label="Heading self-link"></a></h2><p>The installation files are in <code>provision/dev/</code>. Create a new Kubernetes cluster using kind by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>cd</span> provision/dev/
</span></span><span class=line><span class=cl>./create_cluster.sh single
</span></span></code></pre></div><h2 id=step-3-install-llmariner>Step 3: Install LLMariner<a class=td-heading-self-link href=#step-3-install-llmariner aria-label="Heading self-link"></a></h2><p>To install LLMariner using helmfile, run the following commands:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helmfile apply --skip-diff-on-install
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Tips</h4>You can filter the components to deploy using the <code>--selector(-l)</code> flag. For example, to filter out the monitoring components, set the <code>-l tier!=monitoring</code> flag. For deploying just the llmariner, use <code>-l app=llmariner</code>.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-8de5d0eb22b83ad8b3efcf34d11c8f4b>2.1.4 - Install in a Single EKS Cluster</h1><div class=lead>Install LLMariner in an EKS cluster with the standalone mode.</div><p>This page goes through the concrete steps to create an EKS cluster, create necessary resources, and install LLMariner. You can skip some of the steps if you have already made necessary installation/setup.</p><h2 id=step-1-provision-an-eks-cluster>Step 1. Provision an EKS cluster<a class=td-heading-self-link href=#step-1-provision-an-eks-cluster aria-label="Heading self-link"></a></h2><h3 id=step-11-create-a-new-cluster-with-karpenter>Step 1.1. Create a new cluster with Karpenter<a class=td-heading-self-link href=#step-11-create-a-new-cluster-with-karpenter aria-label="Heading self-link"></a></h3><p>Either follow the <a href=https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/>Karpenter getting started guide</a> and create an EKS cluster with Karpenter, or run the following simplified installation steps.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CLUSTER_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner-demo&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>AWS_DEFAULT_REGION</span><span class=o>=</span><span class=s2>&#34;us-east-1&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>AWS_ACCOUNT_ID</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span>aws sts get-caller-identity --query Account --output text<span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>KARPENTER_NAMESPACE</span><span class=o>=</span><span class=s2>&#34;kube-system&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>KARPENTER_VERSION</span><span class=o>=</span><span class=s2>&#34;1.0.1&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>K8S_VERSION</span><span class=o>=</span><span class=s2>&#34;1.30&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>TEMPOUT</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span>mktemp<span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>curl -fsSL https://raw.githubusercontent.com/aws/karpenter-provider-aws/v<span class=s2>&#34;</span><span class=si>${</span><span class=nv>KARPENTER_VERSION</span><span class=si>}</span><span class=s2>&#34;</span>/website/content/en/preview/getting-started/getting-started-with-karpenter/cloudformation.yaml  &gt; <span class=s2>&#34;</span><span class=si>${</span><span class=nv>TEMPOUT</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=o>&amp;&amp;</span> aws cloudformation deploy <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --stack-name <span class=s2>&#34;Karpenter-</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --template-file <span class=s2>&#34;</span><span class=si>${</span><span class=nv>TEMPOUT</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --capabilities CAPABILITY_NAMED_IAM <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --parameter-overrides <span class=s2>&#34;ClusterName=</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>eksctl create cluster -f - <span class=s>&lt;&lt;EOF
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: eksctl.io/v1alpha5
</span></span></span><span class=line><span class=cl><span class=s>kind: ClusterConfig
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: ${CLUSTER_NAME}
</span></span></span><span class=line><span class=cl><span class=s>  region: ${AWS_DEFAULT_REGION}
</span></span></span><span class=line><span class=cl><span class=s>  version: &#34;${K8S_VERSION}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  tags:
</span></span></span><span class=line><span class=cl><span class=s>    karpenter.sh/discovery: ${CLUSTER_NAME}
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>iam:
</span></span></span><span class=line><span class=cl><span class=s>  withOIDC: true
</span></span></span><span class=line><span class=cl><span class=s>  podIdentityAssociations:
</span></span></span><span class=line><span class=cl><span class=s>  - namespace: &#34;${KARPENTER_NAMESPACE}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    serviceAccountName: karpenter
</span></span></span><span class=line><span class=cl><span class=s>    roleName: ${CLUSTER_NAME}-karpenter
</span></span></span><span class=line><span class=cl><span class=s>    permissionPolicyARNs:
</span></span></span><span class=line><span class=cl><span class=s>    - arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME}
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>iamIdentityMappings:
</span></span></span><span class=line><span class=cl><span class=s>- arn: &#34;arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  username: system:node:{{EC2PrivateDNSName}}
</span></span></span><span class=line><span class=cl><span class=s>  groups:
</span></span></span><span class=line><span class=cl><span class=s>  - system:bootstrappers
</span></span></span><span class=line><span class=cl><span class=s>  - system:nodes
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>managedNodeGroups:
</span></span></span><span class=line><span class=cl><span class=s>- instanceType: m5.large
</span></span></span><span class=line><span class=cl><span class=s>  amiFamily: AmazonLinux2
</span></span></span><span class=line><span class=cl><span class=s>  name: ${CLUSTER_NAME}-ng
</span></span></span><span class=line><span class=cl><span class=s>  desiredCapacity: 2
</span></span></span><span class=line><span class=cl><span class=s>  minSize: 1
</span></span></span><span class=line><span class=cl><span class=s>  maxSize: 10
</span></span></span><span class=line><span class=cl><span class=s>addons:
</span></span></span><span class=line><span class=cl><span class=s>- name: eks-pod-identity-agent
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create the service linked role if it does not exist. Ignore an already-exists error.</span>
</span></span><span class=line><span class=cl>aws iam create-service-linked-role --aws-service-name spot.amazonaws.com <span class=o>||</span> <span class=nb>true</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Logout of helm registry to perform an unauthenticated pull against the public ECR.</span>
</span></span><span class=line><span class=cl>helm registry <span class=nb>logout</span> public.ecr.aws
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Deploy Karpenter.</span>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace <span class=s2>&#34;</span><span class=si>${</span><span class=nv>KARPENTER_NAMESPACE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  karpenter oci://public.ecr.aws/karpenter/karpenter <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version <span class=s2>&#34;</span><span class=si>${</span><span class=nv>KARPENTER_VERSION</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=s2>&#34;settings.clusterName=</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=s2>&#34;settings.interruptionQueue=</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set controller.resources.requests.cpu<span class=o>=</span><span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set controller.resources.requests.memory<span class=o>=</span>1Gi <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set controller.resources.limits.cpu<span class=o>=</span><span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set controller.resources.limits.memory<span class=o>=</span>1Gi
</span></span></code></pre></div><h3 id=step-12-provision-gpu-nodes>Step 1.2. Provision GPU nodes<a class=td-heading-self-link href=#step-12-provision-gpu-nodes aria-label="Heading self-link"></a></h3><p>Once Karpenter is installed, we need to create an <code>EC2NodeClass</code> and a <code>NodePool</code> so that GPU nodes are provisioned. We configure <code>blockDeviceMappings</code> in the <code>EC2NodeClass</code> definition so that nodes have sufficient local storage to store model files.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>GPU_AMI_ID</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span>aws ssm get-parameter --name /aws/service/eks/optimized-ami/<span class=si>${</span><span class=nv>K8S_VERSION</span><span class=si>}</span>/amazon-linux-2-gpu/recommended/image_id --query Parameter.Value --output text<span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst | kubectl apply -f -
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: karpenter.sh/v1
</span></span></span><span class=line><span class=cl><span class=s>kind: NodePool
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: default
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  template:
</span></span></span><span class=line><span class=cl><span class=s>    spec:
</span></span></span><span class=line><span class=cl><span class=s>      requirements:
</span></span></span><span class=line><span class=cl><span class=s>      - key: kubernetes.io/arch
</span></span></span><span class=line><span class=cl><span class=s>        operator: In
</span></span></span><span class=line><span class=cl><span class=s>        values: [&#34;amd64&#34;]
</span></span></span><span class=line><span class=cl><span class=s>      - key: kubernetes.io/os
</span></span></span><span class=line><span class=cl><span class=s>        operator: In
</span></span></span><span class=line><span class=cl><span class=s>        values: [&#34;linux&#34;]
</span></span></span><span class=line><span class=cl><span class=s>      - key: karpenter.sh/capacity-type
</span></span></span><span class=line><span class=cl><span class=s>        operator: In
</span></span></span><span class=line><span class=cl><span class=s>        values: [&#34;on-demand&#34;]
</span></span></span><span class=line><span class=cl><span class=s>      - key: karpenter.k8s.aws/instance-family
</span></span></span><span class=line><span class=cl><span class=s>        operator: In
</span></span></span><span class=line><span class=cl><span class=s>        values: [&#34;g5&#34;]
</span></span></span><span class=line><span class=cl><span class=s>      nodeClassRef:
</span></span></span><span class=line><span class=cl><span class=s>        group: karpenter.k8s.aws
</span></span></span><span class=line><span class=cl><span class=s>        kind: EC2NodeClass
</span></span></span><span class=line><span class=cl><span class=s>        name: default
</span></span></span><span class=line><span class=cl><span class=s>      expireAfter: 720h
</span></span></span><span class=line><span class=cl><span class=s>  disruption:
</span></span></span><span class=line><span class=cl><span class=s>    consolidationPolicy: WhenEmptyOrUnderutilized
</span></span></span><span class=line><span class=cl><span class=s>    consolidateAfter: 1m
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: karpenter.k8s.aws/v1
</span></span></span><span class=line><span class=cl><span class=s>kind: EC2NodeClass
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: default
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  amiFamily: AL2
</span></span></span><span class=line><span class=cl><span class=s>  role: &#34;KarpenterNodeRole-${CLUSTER_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  subnetSelectorTerms:
</span></span></span><span class=line><span class=cl><span class=s>  - tags:
</span></span></span><span class=line><span class=cl><span class=s>      karpenter.sh/discovery: &#34;${CLUSTER_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  securityGroupSelectorTerms:
</span></span></span><span class=line><span class=cl><span class=s>  - tags:
</span></span></span><span class=line><span class=cl><span class=s>      karpenter.sh/discovery: &#34;${CLUSTER_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  amiSelectorTerms:
</span></span></span><span class=line><span class=cl><span class=s>  - id: &#34;${GPU_AMI_ID}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  blockDeviceMappings:
</span></span></span><span class=line><span class=cl><span class=s>  - deviceName: /dev/xvda
</span></span></span><span class=line><span class=cl><span class=s>    ebs:
</span></span></span><span class=line><span class=cl><span class=s>      deleteOnTermination: true
</span></span></span><span class=line><span class=cl><span class=s>      encrypted: true
</span></span></span><span class=line><span class=cl><span class=s>      volumeSize: 256Gi
</span></span></span><span class=line><span class=cl><span class=s>      volumeType: gp3
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span></code></pre></div><h3 id=step-13-install-nvidia-gpu-operator>Step 1.3. Install Nvidia GPU Operator<a class=td-heading-self-link href=#step-13-install-nvidia-gpu-operator aria-label="Heading self-link"></a></h3><p>Nvidia GPU Operator is required to install the device plugin and make GPU resources visible in the K8s cluster. Run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace nvidia <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  gpu-operator nvidia/gpu-operator <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set cdi.enabled<span class=o>=</span><span class=nb>true</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set driver.enabled<span class=o>=</span><span class=nb>false</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set toolkit.enabled<span class=o>=</span><span class=nb>false</span>
</span></span></code></pre></div><h3 id=step-14-install-an-ingress-controller>Step 1.4. Install an ingress controller<a class=td-heading-self-link href=#step-14-install-an-ingress-controller aria-label="Heading self-link"></a></h3><p>An ingress controller is required to route HTTP/HTTPS requests to the LLMariner components. Any ingress controller works, and you can skip this step if your EKS cluster already has an ingress controller.</p><p>Here is an example that installs <a href=https://konghq.com/>Kong</a> and make the ingress controller reachable via AWS loadbalancer:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add kong https://charts.konghq.com
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace kong <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  kong-proxy kong/kong <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set proxy.annotations.service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout<span class=o>=</span><span class=m>300</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set ingressController.installCRDs<span class=o>=</span><span class=nb>false</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=nv>fullnameOverride</span><span class=o>=</span><span class=nb>false</span>
</span></span></code></pre></div><h2 id=step-2-create-an-rds-instance>Step 2. Create an RDS instance<a class=td-heading-self-link href=#step-2-create-an-rds-instance aria-label="Heading self-link"></a></h2><p>We will create an RDS in the same VPC as the EKS cluster so that it can be reachable from the LLMariner components. Here are example commands for creating a DB subnet group:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>DB_SUBNET_GROUP_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner-demo-db-subnet&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EKS_SUBNET_IDS</span><span class=o>=</span><span class=k>$(</span>aws eks describe-cluster --name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.cluster.resourcesVpcConfig.subnetIds | join(&#34; &#34;)&#39;</span> --raw-output<span class=k>)</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EKS_SUBNET_ID0</span><span class=o>=</span><span class=k>$(</span><span class=nb>echo</span> <span class=si>${</span><span class=nv>EKS_SUBNET_IDS</span><span class=si>}</span> <span class=p>|</span> cut -d<span class=s1>&#39; &#39;</span> -f1<span class=k>)</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EKS_SUBNET_ID1</span><span class=o>=</span><span class=k>$(</span><span class=nb>echo</span> <span class=si>${</span><span class=nv>EKS_SUBNET_IDS</span><span class=si>}</span> <span class=p>|</span> cut -d<span class=s1>&#39; &#39;</span> -f2<span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws rds create-db-subnet-group <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-subnet-group-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_SUBNET_GROUP_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-subnet-group-description <span class=s2>&#34;LLMariner Demo&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --subnet-ids <span class=s2>&#34;</span><span class=si>${</span><span class=nv>EKS_SUBNET_ID0</span><span class=si>}</span><span class=s2>&#34;</span> <span class=s2>&#34;</span><span class=si>${</span><span class=nv>EKS_SUBNET_ID1</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>and an RDS instance:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>DB_INSTANCE_ID</span><span class=o>=</span><span class=s2>&#34;llmariner-demo&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_USER</span><span class=o>=</span><span class=s2>&#34;admin_user&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_PASSWORD</span><span class=o>=</span><span class=s2>&#34;secret_password&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EKS_SECURITY_GROUP_ID</span><span class=o>=</span><span class=k>$(</span>aws eks describe-cluster --name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.cluster.resourcesVpcConfig.clusterSecurityGroupId&#39;</span> --raw-output<span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws rds create-db-instance <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-instance-identifier <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_INSTANCE_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-instance-class db.t3.small <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --engine postgres <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --allocated-storage <span class=m>10</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --storage-encrypted <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --master-username <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_USER</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --master-user-password <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_PASSWORD</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --vpc-security-group-ids <span class=s2>&#34;</span><span class=si>${</span><span class=nv>EKS_SECURITY_GROUP_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-subnet-group-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_SUBNET_GROUP_NAME</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>You can run the following command to check the provisioning status.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>aws rds describe-db-instances --db-instance-identifier <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_INSTANCE_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.DBInstances[].DBInstanceStatus&#39;</span>
</span></span></code></pre></div><p>Once the RDS instance is fully provisioned and its status becomes <code>available</code>, obtain the endpoint information for later use.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_ADDR</span><span class=o>=</span><span class=k>$(</span>aws rds describe-db-instances --db-instance-identifier <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_INSTANCE_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.DBInstances[].Endpoint.Address&#39;</span> --raw-output<span class=k>)</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_PORT</span><span class=o>=</span><span class=k>$(</span>aws rds describe-db-instances --db-instance-identifier <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_INSTANCE_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.DBInstances[].Endpoint.Port&#39;</span> --raw-output<span class=k>)</span>
</span></span></code></pre></div><p>You can verify if the DB instance is reachable from the EKS cluster by running the <code>psql</code> command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl run psql --image jbergknoff/postgresql-client --env<span class=o>=</span><span class=s2>&#34;PGPASSWORD=</span><span class=si>${</span><span class=nv>POSTGRES_PASSWORD</span><span class=si>}</span><span class=s2>&#34;</span> -- -h <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_ADDR</span><span class=si>}</span><span class=s2>&#34;</span> -U <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_USER</span><span class=si>}</span><span class=s2>&#34;</span> -p <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_PORT</span><span class=si>}</span><span class=s2>&#34;</span> -d template1 -c <span class=s2>&#34;select now();&#34;</span>
</span></span><span class=line><span class=cl>kubectl logs psql
</span></span><span class=line><span class=cl>kubectl delete pods psql
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>LLMariner will create additional databases on the fly for each API service (e.g., <code>job_manager</code>, <code>model_manager</code>). You can see all created databases by running <code>SELECT count(datname) FROM pg_database;</code>.</div><h2 id=step-3-create-an-s3-bucket>Step 3. Create an S3 bucket<a class=td-heading-self-link href=#step-3-create-an-s3-bucket aria-label="Heading self-link"></a></h2><p>We will create an S3 bucket where model files are stored. Here is an example</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Please change the bucket name to something else.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_BUCKET_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner-demo&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_REGION</span><span class=o>=</span><span class=s2>&#34;us-east-1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws s3api create-bucket --bucket <span class=s2>&#34;</span><span class=si>${</span><span class=nv>S3_BUCKET_NAME</span><span class=si>}</span><span class=s2>&#34;</span> --region <span class=s2>&#34;</span><span class=si>${</span><span class=nv>S3_REGION</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>If you want to set up Milvus for RAG, please create another S3 bucket for Milvus:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Please change the bucket name to something else.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>MILVUS_S3_BUCKET_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner-demo-milvus&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws s3api create-bucket --bucket <span class=s2>&#34;</span><span class=si>${</span><span class=nv>MILVUS_S3_BUCKET_NAME</span><span class=si>}</span><span class=s2>&#34;</span> --region <span class=s2>&#34;</span><span class=si>${</span><span class=nv>S3_REGION</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>Pods running in the EKS cluster need to be able to access the S3 bucket. We will create an <a href=https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html>IAM role for service account</a> for that.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_NAMESPACE</span><span class=o>=</span>llmariner
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_POLICY</span><span class=o>=</span><span class=s2>&#34;LLMarinerPolicy&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_SERVICE_ACCOUNT_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_ROLE</span><span class=o>=</span><span class=s2>&#34;LLMarinerRole&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst &gt; policy.json
</span></span></span><span class=line><span class=cl><span class=s>{
</span></span></span><span class=line><span class=cl><span class=s>  &#34;Version&#34;: &#34;2012-10-17&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;Statement&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>    {
</span></span></span><span class=line><span class=cl><span class=s>      &#34;Effect&#34;: &#34;Allow&#34;,
</span></span></span><span class=line><span class=cl><span class=s>      &#34;Action&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>        &#34;s3:PutObject&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;s3:GetObject&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;s3:DeleteObject&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;s3:ListBucket&#34;
</span></span></span><span class=line><span class=cl><span class=s>      ],
</span></span></span><span class=line><span class=cl><span class=s>      &#34;Resource&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>        &#34;arn:aws:s3:::${S3_BUCKET_NAME}/*&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;arn:aws:s3:::${S3_BUCKET_NAME}&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;arn:aws:s3:::${MILVUS_S3_BUCKET_NAME}/*&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;arn:aws:s3:::${MILVUS_S3_BUCKET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      ]
</span></span></span><span class=line><span class=cl><span class=s>    }
</span></span></span><span class=line><span class=cl><span class=s>  ]
</span></span></span><span class=line><span class=cl><span class=s>}
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws iam create-policy --policy-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>LLMARINER_POLICY</span><span class=si>}</span><span class=s2>&#34;</span> --policy-document file://policy.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>eksctl create iamserviceaccount <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>LLMARINER_SERVICE_ACCOUNT_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace <span class=s2>&#34;</span><span class=si>${</span><span class=nv>LLMARINER_NAMESPACE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cluster <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --role-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>LLMARINER_ROLE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --attach-policy-arn <span class=s2>&#34;arn:aws:iam::</span><span class=si>${</span><span class=nv>AWS_ACCOUNT_ID</span><span class=si>}</span><span class=s2>:policy/</span><span class=si>${</span><span class=nv>LLMARINER_POLICY</span><span class=si>}</span><span class=s2>&#34;</span> --approve
</span></span></code></pre></div><h2 id=step-4-install-milvus>Step 4. Install Milvus<a class=td-heading-self-link href=#step-4-install-milvus aria-label="Heading self-link"></a></h2><p>Install <a href=https://milvus.io/>Milvus</a> as it is used a backend vector database for RAG.</p><p>Milvus creates Persistent Volumes. Follow <a href=https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html>https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html</a> and install EBS CSI driver.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EBS_CSI_DRIVER_ROLE</span><span class=o>=</span><span class=s2>&#34;AmazonEKS_EBS_CSI_DriverRole&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>eksctl create iamserviceaccount <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name ebs-csi-controller-sa <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace kube-system <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cluster <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --role-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>EBS_CSI_DRIVER_ROLE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --role-only <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --approve
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>eksctl create addon <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cluster <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name aws-ebs-csi-driver <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version latest <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --service-account-role-arn <span class=s2>&#34;arn:aws:iam::</span><span class=si>${</span><span class=nv>AWS_ACCOUNT_ID</span><span class=si>}</span><span class=s2>:role/</span><span class=si>${</span><span class=nv>EBS_CSI_DRIVER_ROLE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --force
</span></span></code></pre></div><p>Then install the Helm chart. Milvus requires access to the S3 bucket. To use the same service account created above, we deploy Milvus in the same namespace as LLMariner.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst &gt; milvus-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>cluster:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>etcd:
</span></span></span><span class=line><span class=cl><span class=s>  replicaCount: 1
</span></span></span><span class=line><span class=cl><span class=s>  persistence:
</span></span></span><span class=line><span class=cl><span class=s>    storageClass: gp2 # Use gp3 if available
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>pulsar:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>minio:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>standalone:
</span></span></span><span class=line><span class=cl><span class=s>  persistence:
</span></span></span><span class=line><span class=cl><span class=s>    persistentVolumeClaim:
</span></span></span><span class=line><span class=cl><span class=s>      storageClass: gp2 # Use gp3 if available
</span></span></span><span class=line><span class=cl><span class=s>      size: 10Gi
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>  create: false
</span></span></span><span class=line><span class=cl><span class=s>  name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>externalS3:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: true
</span></span></span><span class=line><span class=cl><span class=s>  host: s3.us-east-1.amazonaws.com
</span></span></span><span class=line><span class=cl><span class=s>  port: 443
</span></span></span><span class=line><span class=cl><span class=s>  useSSL: true
</span></span></span><span class=line><span class=cl><span class=s>  bucketName: &#34;${MILVUS_S3_BUCKET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  useIAM: true
</span></span></span><span class=line><span class=cl><span class=s>  cloudProvider: aws
</span></span></span><span class=line><span class=cl><span class=s>  iamEndpoint: &#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s>  logLevel: info
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm repo add zilliztech https://zilliztech.github.io/milvus-helm/
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace milvus <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  milvus zilliztech/milvus <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f milvus-values.yaml
</span></span></code></pre></div><p>Please see the <a href=https://milvus.io/docs/install-overview.md>Milvus installation document</a> and the <a href=https://artifacthub.io/packages/helm/milvus/milvus>Helm chart</a> for other installation options.</p><p>Set the environmental variables so that LLMariner can later access the Postgres database.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>MILVUS_ADDR</span><span class=o>=</span>milvus.milvus
</span></span></code></pre></div><h2 id=step-5-install-llmariner>Step 5. Install LLMariner<a class=td-heading-self-link href=#step-5-install-llmariner aria-label="Heading self-link"></a></h2><p>Run the following command to set up a <code>values.yaml</code> and install LLMariner with Helm.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Set the endpoint URL of LLMariner. Please change if you are using a different ingress controller.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>INGRESS_CONTROLLER_URL</span><span class=o>=</span>http://<span class=k>$(</span>kubectl get services -n kong kong-proxy-kong-proxy  -o <span class=nv>jsonpath</span><span class=o>=</span><span class=s1>&#39;{.status.loadBalancer.ingress[0].hostname}&#39;</span><span class=k>)</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_SECRET_NAME</span><span class=o>=</span><span class=s2>&#34;db-secret&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst &gt; llmariner-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>global:
</span></span></span><span class=line><span class=cl><span class=s>  # This is an ingress configuration with Kong. Please change if you are using a different ingress controller.
</span></span></span><span class=line><span class=cl><span class=s>  ingress:
</span></span></span><span class=line><span class=cl><span class=s>    ingressClassName: kong
</span></span></span><span class=line><span class=cl><span class=s>    # The URL of the ingress controller. this can be a port-forwarding URL (e.g., http://localhost:8080) if there is
</span></span></span><span class=line><span class=cl><span class=s>    # no URL that is reachable from the outside of the EKS cluster.
</span></span></span><span class=line><span class=cl><span class=s>    controllerUrl: &#34;${INGRESS_CONTROLLER_URL}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    annotations:
</span></span></span><span class=line><span class=cl><span class=s>      # To remove the buffering from the streaming output of chat completion.
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/response-buffering: &#34;false&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  database:
</span></span></span><span class=line><span class=cl><span class=s>    host: &#34;${POSTGRES_ADDR}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    port: ${POSTGRES_PORT}
</span></span></span><span class=line><span class=cl><span class=s>    username: &#34;${POSTGRES_USER}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    ssl:
</span></span></span><span class=line><span class=cl><span class=s>      mode: require
</span></span></span><span class=line><span class=cl><span class=s>    createDatabase: true
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  databaseSecret:
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${POSTGRES_SECRET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    key: password
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  objectStore:
</span></span></span><span class=line><span class=cl><span class=s>    s3:
</span></span></span><span class=line><span class=cl><span class=s>      bucket: &#34;${S3_BUCKET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      region: &#34;${S3_REGION}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      endpointUrl: &#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>prepare:
</span></span></span><span class=line><span class=cl><span class=s>  database:
</span></span></span><span class=line><span class=cl><span class=s>    createSecret: true
</span></span></span><span class=line><span class=cl><span class=s>    secret:
</span></span></span><span class=line><span class=cl><span class=s>      password: &#34;${POSTGRES_PASSWORD}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>dex-server:
</span></span></span><span class=line><span class=cl><span class=s>  staticPasswords:
</span></span></span><span class=line><span class=cl><span class=s>  - email: admin@example.com
</span></span></span><span class=line><span class=cl><span class=s>    # bcrypt hash of the string: $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)
</span></span></span><span class=line><span class=cl><span class=s>    hash: &#34;\$2a\$10\$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W&#34;
</span></span></span><span class=line><span class=cl><span class=s>    username: admin-user
</span></span></span><span class=line><span class=cl><span class=s>    userID: admin-id
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>file-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>inference-manager-engine:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  model:
</span></span></span><span class=line><span class=cl><span class=s>    default:
</span></span></span><span class=line><span class=cl><span class=s>      runtimeName: vllm
</span></span></span><span class=line><span class=cl><span class=s>      preloaded: true
</span></span></span><span class=line><span class=cl><span class=s>      resources:
</span></span></span><span class=line><span class=cl><span class=s>        limits:
</span></span></span><span class=line><span class=cl><span class=s>          nvidia.com/gpu: 1
</span></span></span><span class=line><span class=cl><span class=s>    overrides:
</span></span></span><span class=line><span class=cl><span class=s>      meta-llama/Meta-Llama-3.1-8B-Instruct-q4_0:
</span></span></span><span class=line><span class=cl><span class=s>        contextLength: 16384
</span></span></span><span class=line><span class=cl><span class=s>      google/gemma-2b-it-q4_0:
</span></span></span><span class=line><span class=cl><span class=s>        runtimeName: ollama
</span></span></span><span class=line><span class=cl><span class=s>        resources:
</span></span></span><span class=line><span class=cl><span class=s>         limits:
</span></span></span><span class=line><span class=cl><span class=s>           nvidia.com/gpu: 0
</span></span></span><span class=line><span class=cl><span class=s>      sentence-transformers/all-MiniLM-L6-v2-f16:
</span></span></span><span class=line><span class=cl><span class=s>        runtimeName: ollama
</span></span></span><span class=line><span class=cl><span class=s>        resources:
</span></span></span><span class=line><span class=cl><span class=s>         limits:
</span></span></span><span class=line><span class=cl><span class=s>           nvidia.com/gpu: 0
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>inference-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  service:
</span></span></span><span class=line><span class=cl><span class=s>    annotations:
</span></span></span><span class=line><span class=cl><span class=s>      # These annotations are only meaningful for Kong ingress controller to extend the timeout.
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/connect-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/read-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/write-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>job-manager-dispatcher:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  notebook:
</span></span></span><span class=line><span class=cl><span class=s>    # Used to set the base URL of the API endpoint. This can be same as global.ingress.controllerUrl
</span></span></span><span class=line><span class=cl><span class=s>    # if the URL is reachable from the inside cluster. Otherwise you can change this to the
</span></span></span><span class=line><span class=cl><span class=s>    # to the URL of the ingress controller that is reachable inside the K8s cluster.
</span></span></span><span class=line><span class=cl><span class=s>    llmarinerBaseUrl: &#34;${INGRESS_CONTROLLER_URL}/v1&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>model-manager-loader:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  baseModels:
</span></span></span><span class=line><span class=cl><span class=s>  - meta-llama/Meta-Llama-3.1-8B-Instruct-q4_0
</span></span></span><span class=line><span class=cl><span class=s>  - google/gemma-2b-it-q4_0
</span></span></span><span class=line><span class=cl><span class=s>  - sentence-transformers/all-MiniLM-L6-v2-f16
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s># Required when RAG is used.
</span></span></span><span class=line><span class=cl><span class=s>vector-store-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  vectorDatabase:
</span></span></span><span class=line><span class=cl><span class=s>    host: &#34;${MILVUS_ADDR}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  llmEngineAddr: ollama-sentence-transformers-all-minilm-l6-v2-f16:11434
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  llmariner oci://public.ecr.aws/cloudnatix/llmariner-charts/llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f llmariner-values.yaml
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Starting from Helm v3.8.0, the OCI registry is supported by default. If you are using an older version, please upgrade to v3.8.0 or later. For more details, please refer to <a href=https://helm.sh/docs/topics/registries/>Helm OCI-based registries</a>.</div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>If you are getting a 403 forbidden error, please try <code>docker logout public.ecr.aws</code>. Please see <a href=https://docs.aws.amazon.com/AmazonECR/latest/public/public-troubleshooting.html>AWS document</a> for more details.</div><p>If you would like to install only the control-plane components or the worker-plane components, please see <code>multi_cluster_deployment</code>{.interpreted-text role=&ldquo;doc&rdquo;}.</p><h2 id=step-6-verify-the-installation>Step 6. Verify the installation<a class=td-heading-self-link href=#step-6-verify-the-installation aria-label="Heading self-link"></a></h2><p>You can verify the installation by sending sample chat completion requests.</p><p>Note, if you have used LLMariner in other cases before you may need to delete the previous config by running <code>rm -rf ~/.config/llmariner</code></p><p>The default login user name is <code>admin@example.com</code> and the password is
<code>password</code>. You can change this by updating the Dex configuration
(<a href=/docs/features/user_management/>link</a>).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;This is your endpoint URL: </span><span class=si>${</span><span class=nv>INGRESS_CONTROLLER_URL</span><span class=si>}</span><span class=s2>/v1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma auth login
</span></span><span class=line><span class=cl><span class=c1># Type the above endpoint URL.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma models list
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion <span class=s2>&#34;what is k8s?&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model meta-llama-Meta-Llama-3.1-8B-Instruct-q4_0 --role user --completion <span class=s2>&#34;hello&#34;</span>
</span></span></code></pre></div><h2 id=optional-monitor-gpu-utilization>Optional: Monitor GPU utilization<a class=td-heading-self-link href=#optional-monitor-gpu-utilization aria-label="Heading self-link"></a></h2><p>If you would like to install Prometheus and Grafana to see GPU utilization, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Add Prometheus</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; prom-scrape-configs.yaml
</span></span></span><span class=line><span class=cl><span class=s>- job_name: nvidia-dcgm
</span></span></span><span class=line><span class=cl><span class=s>  scrape_interval: 5s
</span></span></span><span class=line><span class=cl><span class=s>  static_configs:
</span></span></span><span class=line><span class=cl><span class=s>  - targets: [&#39;nvidia-dcgm-exporter.nvidia.svc:9400&#39;]
</span></span></span><span class=line><span class=cl><span class=s>- job_name: inference-manager-engine-metrics
</span></span></span><span class=line><span class=cl><span class=s>  scrape_interval: 5s
</span></span></span><span class=line><span class=cl><span class=s>  static_configs:
</span></span></span><span class=line><span class=cl><span class=s>  - targets: [&#39;inference-manager-server-http.llmariner.svc:8083&#39;]
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace monitoring <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set-file <span class=nv>extraScrapeConfigs</span><span class=o>=</span>prom-scrape-configs.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  prometheus prometheus-community/prometheus
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add Grafana with DCGM dashboard</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; grafana-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>datasources:
</span></span></span><span class=line><span class=cl><span class=s> datasources.yaml:
</span></span></span><span class=line><span class=cl><span class=s>   apiVersion: 1
</span></span></span><span class=line><span class=cl><span class=s>   datasources:
</span></span></span><span class=line><span class=cl><span class=s>   - name: Prometheus
</span></span></span><span class=line><span class=cl><span class=s>     type: prometheus
</span></span></span><span class=line><span class=cl><span class=s>     url: http://prometheus-server
</span></span></span><span class=line><span class=cl><span class=s>     isDefault: true
</span></span></span><span class=line><span class=cl><span class=s>dashboardProviders:
</span></span></span><span class=line><span class=cl><span class=s>  dashboardproviders.yaml:
</span></span></span><span class=line><span class=cl><span class=s>    apiVersion: 1
</span></span></span><span class=line><span class=cl><span class=s>    providers:
</span></span></span><span class=line><span class=cl><span class=s>    - name: &#39;default&#39;
</span></span></span><span class=line><span class=cl><span class=s>      orgId: 1
</span></span></span><span class=line><span class=cl><span class=s>      folder: &#39;default&#39;
</span></span></span><span class=line><span class=cl><span class=s>      type: file
</span></span></span><span class=line><span class=cl><span class=s>      disableDeletion: true
</span></span></span><span class=line><span class=cl><span class=s>      editable: true
</span></span></span><span class=line><span class=cl><span class=s>      options:
</span></span></span><span class=line><span class=cl><span class=s>        path: /var/lib/grafana/dashboards/standard
</span></span></span><span class=line><span class=cl><span class=s>dashboards:
</span></span></span><span class=line><span class=cl><span class=s>  default:
</span></span></span><span class=line><span class=cl><span class=s>    nvidia-dcgm-exporter:
</span></span></span><span class=line><span class=cl><span class=s>      gnetId: 12239
</span></span></span><span class=line><span class=cl><span class=s>      datasource: Prometheus
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>helm repo add grafana https://grafana.github.io/helm-charts
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace monitoring <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f grafana-values.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  grafana grafana/grafana
</span></span></code></pre></div><h2 id=optional-enable-tls>Optional: Enable TLS<a class=td-heading-self-link href=#optional-enable-tls aria-label="Heading self-link"></a></h2><p>First follow the <a href=https://cert-manager.io/docs/installation/>cert-manager installation document</a> and install cert-manager to your K8s cluster if you don&rsquo;t have one. Then create a <code>ClusterIssuer</code> for your domain. Here is an example manifest that uses Let's Encrypt.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert-manager.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIssuer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>acme</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>server</span><span class=p>:</span><span class=w> </span><span class=l>https://acme-v02.api.letsencrypt.org/directory</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>user@mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>privateKeySecretRef</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>solvers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>http01</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>       </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>ingressClassName</span><span class=p>:</span><span class=w> </span><span class=l>kong</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>dnsZones</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dns01</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div><p>Then you can add the following to <code>values.yaml</code> of LLMariner to enable TLS.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span></code></pre></div><p>The ingresses created from the Helm chart will have the following annotation and spec:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nn>...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-b97c20cd3e745f78416fc8eddf907efc>2.1.5 - Install in a Single On-premise Cluster</h1><div class=lead>Install LLMariner in an on-premise Kubernetes cluster with the standalone mode.</div><p>This page goes through the concrete steps to install LLMariner on a on-premise K8s cluster (or a local K8s cluster).
You can skip some of the steps if you have already made necessary installation/setup.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Installation of Postgres, MinIO, SeaweedFS, and Milvus are just example purposes, and
they are not intended for the production usage.
Please configure based on your requirements if you want to use LLMariner for your production environment.</div><h2 id=step-1-install-nvidia-gpu-operator>Step 1. Install Nvidia GPU Operator<a class=td-heading-self-link href=#step-1-install-nvidia-gpu-operator aria-label="Heading self-link"></a></h2><p>Nvidia GPU Operator is required to install the device plugin and make GPU resources visible in the K8s cluster. Run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace nvidia <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  gpu-operator nvidia/gpu-operator <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set cdi.enabled<span class=o>=</span><span class=nb>true</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set driver.enabled<span class=o>=</span><span class=nb>false</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set toolkit.enabled<span class=o>=</span><span class=nb>false</span>
</span></span></code></pre></div><h2 id=step-2-install-an-ingress-controller>Step 2. Install an ingress controller<a class=td-heading-self-link href=#step-2-install-an-ingress-controller aria-label="Heading self-link"></a></h2><p>An ingress controller is required to route HTTP/HTTPS requests to the LLMariner components. Any ingress controller works, and you can skip this step if your EKS cluster already has an ingress controller.</p><p>Here is an example that installs <a href=https://konghq.com/>Kong</a> and make the ingress controller:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add kong https://charts.konghq.com
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; kong-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>proxy:
</span></span></span><span class=line><span class=cl><span class=s> type: NodePort
</span></span></span><span class=line><span class=cl><span class=s> http:
</span></span></span><span class=line><span class=cl><span class=s>   hostPort: 80
</span></span></span><span class=line><span class=cl><span class=s> tls:
</span></span></span><span class=line><span class=cl><span class=s>   hostPort: 443
</span></span></span><span class=line><span class=cl><span class=s> annotations:
</span></span></span><span class=line><span class=cl><span class=s>   service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: &#34;300&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>nodeSelector:
</span></span></span><span class=line><span class=cl><span class=s>  ingress-ready: &#34;true&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>tolerations:
</span></span></span><span class=line><span class=cl><span class=s>- key: node-role.kubernetes.io/control-plane
</span></span></span><span class=line><span class=cl><span class=s>  operator: Equal
</span></span></span><span class=line><span class=cl><span class=s>  effect: NoSchedule
</span></span></span><span class=line><span class=cl><span class=s>- key: node-role.kubernetes.io/master
</span></span></span><span class=line><span class=cl><span class=s>  operator: Equal
</span></span></span><span class=line><span class=cl><span class=s>  effect: NoSchedule
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>fullnameOverride: kong
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace kong <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  kong-proxy kong/kong <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f kong-values.yaml
</span></span></code></pre></div><h2 id=step-3-install-a-postgres-database>Step 3. Install a Postgres database<a class=td-heading-self-link href=#step-3-install-a-postgres-database aria-label="Heading self-link"></a></h2><p>Run the following to deploy an Postgres deployment:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_USER</span><span class=o>=</span><span class=s2>&#34;admin_user&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_PASSWORD</span><span class=o>=</span><span class=s2>&#34;secret_password&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace postgres <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  postgres oci://registry-1.docker.io/bitnamicharts/postgresql <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=nv>nameOverride</span><span class=o>=</span>postgres <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.database<span class=o>=</span>ps_db <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.username<span class=o>=</span><span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_USER</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.password<span class=o>=</span><span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_PASSWORD</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>Set the environmental variables so that LLMariner can later access the Postgres database.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_ADDR</span><span class=o>=</span>postgres.postgres
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_PORT</span><span class=o>=</span><span class=m>5432</span>
</span></span></code></pre></div><h2 id=step-4-install-an-s3-compatible-object-store>Step 4. Install an S3-compatible object store<a class=td-heading-self-link href=#step-4-install-an-s3-compatible-object-store aria-label="Heading self-link"></a></h2><p>LLMariner requires an S3-compatible object store such as <a href=https://min.io/>MinIO</a> or <a href=https://seaweedfs.com>SeaweedFS</a>.</p><p>First set environmental variables to specify installation configuration:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Bucket name and the dummy region.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_BUCKET_NAME</span><span class=o>=</span>llmariner
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_REGION</span><span class=o>=</span>dummy
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Credentials for accessing the S3 bucket.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>AWS_ACCESS_KEY_ID</span><span class=o>=</span>llmariner-key
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>AWS_SECRET_ACCESS_KEY</span><span class=o>=</span>llmariner-secret
</span></span></code></pre></div><p>Then install an object store. Here are the example installation commands for MinIO and SeaweedFS.</p><ul class="nav nav-tabs" id=tabs-2 role=tablist><li class=nav-item><button class="nav-link disabled" id=tabs-02-00-tab data-bs-toggle=tab data-bs-target=#tabs-02-00 role=tab aria-controls=tabs-02-00 aria-selected=false>
<strong>Install</strong>:</button></li><li class=nav-item><button class="nav-link active" id=tabs-02-01-tab data-bs-toggle=tab data-bs-target=#tabs-02-01 role=tab data-td-tp-persist=minio aria-controls=tabs-02-01 aria-selected=true>
MinIO</button></li><li class=nav-item><button class=nav-link id=tabs-02-02-tab data-bs-toggle=tab data-bs-target=#tabs-02-02 role=tab data-td-tp-persist=seaweedfs aria-controls=tabs-02-02 aria-selected=false>
SeaweedFS</button></li></ul><div class=tab-content id=tabs-2-content><div class="tab-body tab-pane fade" id=tabs-02-00 role=tabpanel aria-labelled-by=tabs-02-00-tab tabindex=2></div><div class="tab-body tab-pane fade show active" id=tabs-02-01 role=tabpanel aria-labelled-by=tabs-02-01-tab tabindex=2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace minio <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  minio oci://registry-1.docker.io/bitnamicharts/minio <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.rootUser<span class=o>=</span>minioadmin <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.rootPassword<span class=o>=</span>minioadmin <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=nv>defaultBuckets</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>${</span><span class=nv>S3_BUCKET_NAME</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl port-forward -n minio service/minio <span class=m>9001</span> <span class=p>&amp;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Wait until the port-forwarding connection is established.</span>
</span></span><span class=line><span class=cl>sleep <span class=m>5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Obtain the cookie and store in cookies.txt.</span>
</span></span><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:9001/api/v1/login <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cookie-jar cookies.txt <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s1>&#39;Content-Type: application/json&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data @- <span class=s>&lt;&lt; EOF
</span></span></span><span class=line><span class=cl><span class=s>{
</span></span></span><span class=line><span class=cl><span class=s>  &#34;accessKey&#34;: &#34;minioadmin&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;secretKey&#34;: &#34;minioadmin&#34;
</span></span></span><span class=line><span class=cl><span class=s>}
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a new API key.</span>
</span></span><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:9001/api/v1/service-account-credentials <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cookie cookies.txt <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data @- <span class=s>&lt;&lt; EOF &gt;/dev/null
</span></span></span><span class=line><span class=cl><span class=s>{
</span></span></span><span class=line><span class=cl><span class=s>  &#34;name&#34;: &#34;LLMariner&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;accessKey&#34;: &#34;$AWS_ACCESS_KEY_ID&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;secretKey&#34;: &#34;$AWS_SECRET_ACCESS_KEY&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;description&#34;: &#34;&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;comment&#34;: &#34;&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;policy&#34;: &#34;&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;expiry&#34;: null
</span></span></span><span class=line><span class=cl><span class=s>}
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>rm cookies.txt
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>kill</span> %1
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-02-02 role=tabpanel aria-labelled-by=tabs-02-02-tab tabindex=2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl create namespace seaweedfs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a secret.</span>
</span></span><span class=line><span class=cl><span class=c1># See https://github.com/seaweedfs/seaweedfs/wiki/Amazon-S3-API#public-access-with-anonymous-download for details.</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; s3-config.json
</span></span></span><span class=line><span class=cl><span class=s>{
</span></span></span><span class=line><span class=cl><span class=s>  &#34;identities&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>    {
</span></span></span><span class=line><span class=cl><span class=s>      &#34;name&#34;: &#34;me&#34;,
</span></span></span><span class=line><span class=cl><span class=s>      &#34;credentials&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>        {
</span></span></span><span class=line><span class=cl><span class=s>          &#34;accessKey&#34;: &#34;${AWS_ACCESS_KEY_ID}&#34;,
</span></span></span><span class=line><span class=cl><span class=s>          &#34;secretKey&#34;: &#34;${AWS_SECRET_ACCESS_KEY}&#34;
</span></span></span><span class=line><span class=cl><span class=s>        }
</span></span></span><span class=line><span class=cl><span class=s>      ],
</span></span></span><span class=line><span class=cl><span class=s>      &#34;actions&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>        &#34;Admin&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;Read&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;ReadAcp&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;List&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;Tagging&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;Write&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;WriteAcp&#34;
</span></span></span><span class=line><span class=cl><span class=s>      ]
</span></span></span><span class=line><span class=cl><span class=s>    }
</span></span></span><span class=line><span class=cl><span class=s>  ]
</span></span></span><span class=line><span class=cl><span class=s>}
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl create secret generic -n seaweedfs seaweedfs --from-file<span class=o>=</span>s3-config.json
</span></span><span class=line><span class=cl>rm s3-config.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># deploy seaweedfs</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | kubectl apply -n seaweedfs -f -
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: v1
</span></span></span><span class=line><span class=cl><span class=s>kind: PersistentVolume
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: seaweedfs-volume
</span></span></span><span class=line><span class=cl><span class=s>  labels:
</span></span></span><span class=line><span class=cl><span class=s>    type: local
</span></span></span><span class=line><span class=cl><span class=s>    app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  storageClassName: manual
</span></span></span><span class=line><span class=cl><span class=s>  capacity:
</span></span></span><span class=line><span class=cl><span class=s>    storage: 500Mi
</span></span></span><span class=line><span class=cl><span class=s>  accessModes:
</span></span></span><span class=line><span class=cl><span class=s>  - ReadWriteMany
</span></span></span><span class=line><span class=cl><span class=s>  hostPath:
</span></span></span><span class=line><span class=cl><span class=s>    path: /data/seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: v1
</span></span></span><span class=line><span class=cl><span class=s>kind: PersistentVolumeClaim
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: seaweedfs-volume-claim
</span></span></span><span class=line><span class=cl><span class=s>  labels:
</span></span></span><span class=line><span class=cl><span class=s>    app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  storageClassName: manual
</span></span></span><span class=line><span class=cl><span class=s>  accessModes:
</span></span></span><span class=line><span class=cl><span class=s>  - ReadWriteMany
</span></span></span><span class=line><span class=cl><span class=s>  resources:
</span></span></span><span class=line><span class=cl><span class=s>    requests:
</span></span></span><span class=line><span class=cl><span class=s>      storage: 500Mi
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: apps/v1
</span></span></span><span class=line><span class=cl><span class=s>kind: Deployment
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  replicas: 1
</span></span></span><span class=line><span class=cl><span class=s>  selector:
</span></span></span><span class=line><span class=cl><span class=s>    matchLabels:
</span></span></span><span class=line><span class=cl><span class=s>      app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>  template:
</span></span></span><span class=line><span class=cl><span class=s>    metadata:
</span></span></span><span class=line><span class=cl><span class=s>      labels:
</span></span></span><span class=line><span class=cl><span class=s>        app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>    spec:
</span></span></span><span class=line><span class=cl><span class=s>      containers:
</span></span></span><span class=line><span class=cl><span class=s>      - name: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>        image: chrislusf/seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>        args:
</span></span></span><span class=line><span class=cl><span class=s>        - server -s3 -s3.config=/etc/config/s3-config.json -dir=/data
</span></span></span><span class=line><span class=cl><span class=s>        ports:
</span></span></span><span class=line><span class=cl><span class=s>        - name: master
</span></span></span><span class=line><span class=cl><span class=s>          containerPort: 9333
</span></span></span><span class=line><span class=cl><span class=s>          protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>        - name: s3
</span></span></span><span class=line><span class=cl><span class=s>          containerPort: 8333
</span></span></span><span class=line><span class=cl><span class=s>          protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>        volumeMounts:
</span></span></span><span class=line><span class=cl><span class=s>        - name: seaweedfsdata
</span></span></span><span class=line><span class=cl><span class=s>          mountPath: /data
</span></span></span><span class=line><span class=cl><span class=s>        - name: config
</span></span></span><span class=line><span class=cl><span class=s>          mountPath: /etc/config
</span></span></span><span class=line><span class=cl><span class=s>      volumes:
</span></span></span><span class=line><span class=cl><span class=s>      - name: seaweedfsdata
</span></span></span><span class=line><span class=cl><span class=s>        persistentVolumeClaim:
</span></span></span><span class=line><span class=cl><span class=s>          claimName: seaweedfs-volume-claim
</span></span></span><span class=line><span class=cl><span class=s>      - name: config
</span></span></span><span class=line><span class=cl><span class=s>        secret:
</span></span></span><span class=line><span class=cl><span class=s>          secretName: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: v1
</span></span></span><span class=line><span class=cl><span class=s>kind: Service
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>  labels:
</span></span></span><span class=line><span class=cl><span class=s>    app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  type: NodePort
</span></span></span><span class=line><span class=cl><span class=s>  ports:
</span></span></span><span class=line><span class=cl><span class=s>  - port: 9333
</span></span></span><span class=line><span class=cl><span class=s>    targetPort: master
</span></span></span><span class=line><span class=cl><span class=s>    protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>    name: master
</span></span></span><span class=line><span class=cl><span class=s>    nodePort: 31238
</span></span></span><span class=line><span class=cl><span class=s>  - port: 8333
</span></span></span><span class=line><span class=cl><span class=s>    targetPort: s3
</span></span></span><span class=line><span class=cl><span class=s>    protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>    name: s3
</span></span></span><span class=line><span class=cl><span class=s>    nodePort: 31239
</span></span></span><span class=line><span class=cl><span class=s>  selector:
</span></span></span><span class=line><span class=cl><span class=s>    app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl <span class=nb>wait</span> --timeout<span class=o>=</span>60s --for<span class=o>=</span><span class=nv>condition</span><span class=o>=</span>ready pod -n seaweedfs -l <span class=nv>app</span><span class=o>=</span>seaweedfs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl port-forward -n seaweedfs service/seaweedfs <span class=m>8333</span> <span class=p>&amp;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Wait until the port-forwarding connection is established.</span>
</span></span><span class=line><span class=cl>sleep <span class=m>5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create the bucket.</span>
</span></span><span class=line><span class=cl>aws --endpoint-url http://localhost:8333 s3 mb s3://<span class=si>${</span><span class=nv>S3_BUCKET_NAME</span><span class=si>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>kill</span> %1
</span></span></code></pre></div></div></div><p>Then set environmental variable <code>S3_ENDPOINT_URL</code> to the URL of the object store. The URL should be accessible from LLMariner pods that will run on the same cluster.</p><ul class="nav nav-tabs" id=tabs-3 role=tablist><li class=nav-item><button class="nav-link disabled" id=tabs-03-00-tab data-bs-toggle=tab data-bs-target=#tabs-03-00 role=tab aria-controls=tabs-03-00 aria-selected=false>
<strong>Set up the endpoint URL with</strong>:</button></li><li class=nav-item><button class="nav-link active" id=tabs-03-01-tab data-bs-toggle=tab data-bs-target=#tabs-03-01 role=tab data-td-tp-persist=minio aria-controls=tabs-03-01 aria-selected=true>
MinIO</button></li><li class=nav-item><button class=nav-link id=tabs-03-02-tab data-bs-toggle=tab data-bs-target=#tabs-03-02 role=tab data-td-tp-persist=seaweedfs aria-controls=tabs-03-02 aria-selected=false>
SeaweedFs</button></li></ul><div class=tab-content id=tabs-3-content><div class="tab-body tab-pane fade" id=tabs-03-00 role=tabpanel aria-labelled-by=tabs-03-00-tab tabindex=3></div><div class="tab-body tab-pane fade show active" id=tabs-03-01 role=tabpanel aria-labelled-by=tabs-03-01-tab tabindex=3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_ENDPOINT_URL</span><span class=o>=</span>http://minio.minio:9000
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-03-02 role=tabpanel aria-labelled-by=tabs-03-02-tab tabindex=3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_ENDPOINT_URL</span><span class=o>=</span>http://seaweedfs.seaweedfs:8333
</span></span></code></pre></div></div></div><h2 id=step-5-install-milvus>Step 5. Install Milvus<a class=td-heading-self-link href=#step-5-install-milvus aria-label="Heading self-link"></a></h2><p>Install <a href=https://milvus.io/>Milvus</a> as it is used a backend vector database for RAG.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF &gt; milvus-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>cluster:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>etcd:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>pulsar:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>minio:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>  tls:
</span></span></span><span class=line><span class=cl><span class=s>    enabled: false
</span></span></span><span class=line><span class=cl><span class=s>extraConfigFiles:
</span></span></span><span class=line><span class=cl><span class=s>  user.yaml: |+
</span></span></span><span class=line><span class=cl><span class=s>    etcd:
</span></span></span><span class=line><span class=cl><span class=s>      use:
</span></span></span><span class=line><span class=cl><span class=s>        embed: true
</span></span></span><span class=line><span class=cl><span class=s>      data:
</span></span></span><span class=line><span class=cl><span class=s>        dir: /var/lib/milvus/etcd
</span></span></span><span class=line><span class=cl><span class=s>    common:
</span></span></span><span class=line><span class=cl><span class=s>      storageType: local
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm repo add zilliztech https://zilliztech.github.io/milvus-helm/
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace milvus <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  milvus zilliztech/milvus <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f milvus-values.yaml
</span></span></code></pre></div><p>Set the environmental variables so that LLMariner can later access the Postgres database.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>MILVUS_ADDR</span><span class=o>=</span>milvus.milvus
</span></span></code></pre></div><h2 id=step-6-install-llmariner>Step 6. Install LLMariner<a class=td-heading-self-link href=#step-6-install-llmariner aria-label="Heading self-link"></a></h2><p>Run the following command to set up a <code>values.yaml</code> and install LLMariner with Helm.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Set the endpoint URL of LLMariner. Please change if you are using a different ingress controller.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>INGRESS_CONTROLLER_URL</span><span class=o>=</span>http://localhost:8080
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst &gt; llmariner-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>global:
</span></span></span><span class=line><span class=cl><span class=s>  # This is an ingress configuration with Kong. Please change if you are using a different ingress controller.
</span></span></span><span class=line><span class=cl><span class=s>  ingress:
</span></span></span><span class=line><span class=cl><span class=s>    ingressClassName: kong
</span></span></span><span class=line><span class=cl><span class=s>    # The URL of the ingress controller. this can be a port-forwarding URL (e.g., http://localhost:8080) if there is
</span></span></span><span class=line><span class=cl><span class=s>    # no URL that is reachable from the outside of the EKS cluster.
</span></span></span><span class=line><span class=cl><span class=s>    controllerUrl: &#34;${INGRESS_CONTROLLER_URL}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    annotations:
</span></span></span><span class=line><span class=cl><span class=s>      # To remove the buffering from the streaming output of chat completion.
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/response-buffering: &#34;false&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  database:
</span></span></span><span class=line><span class=cl><span class=s>    host: &#34;${POSTGRES_ADDR}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    port: ${POSTGRES_PORT}
</span></span></span><span class=line><span class=cl><span class=s>    username: &#34;${POSTGRES_USER}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    ssl:
</span></span></span><span class=line><span class=cl><span class=s>      mode: disable
</span></span></span><span class=line><span class=cl><span class=s>    createDatabase: true
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  databaseSecret:
</span></span></span><span class=line><span class=cl><span class=s>    name: postgres
</span></span></span><span class=line><span class=cl><span class=s>    key: password
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  objectStore:
</span></span></span><span class=line><span class=cl><span class=s>    s3:
</span></span></span><span class=line><span class=cl><span class=s>      endpointUrl: &#34;${S3_ENDPOINT_URL}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      bucket: &#34;${S3_BUCKET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      region: &#34;${S3_REGION}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  awsSecret:
</span></span></span><span class=line><span class=cl><span class=s>    name: aws
</span></span></span><span class=line><span class=cl><span class=s>    accessKeyIdKey: accessKeyId
</span></span></span><span class=line><span class=cl><span class=s>    secretAccessKeyKey: secretAccessKey
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>prepare:
</span></span></span><span class=line><span class=cl><span class=s>  database:
</span></span></span><span class=line><span class=cl><span class=s>    createSecret: true
</span></span></span><span class=line><span class=cl><span class=s>    secret:
</span></span></span><span class=line><span class=cl><span class=s>      password: &#34;${POSTGRES_PASSWORD}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  objectStore:
</span></span></span><span class=line><span class=cl><span class=s>    createSecret: true
</span></span></span><span class=line><span class=cl><span class=s>    secret:
</span></span></span><span class=line><span class=cl><span class=s>      accessKeyId: &#34;${AWS_ACCESS_KEY_ID}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      secretAccessKey: &#34;${AWS_SECRET_ACCESS_KEY}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>dex-server:
</span></span></span><span class=line><span class=cl><span class=s>  staticPasswords:
</span></span></span><span class=line><span class=cl><span class=s>  - email: admin@example.com
</span></span></span><span class=line><span class=cl><span class=s>    # bcrypt hash of the string: $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)
</span></span></span><span class=line><span class=cl><span class=s>    hash: &#34;\$2a\$10\$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W&#34;
</span></span></span><span class=line><span class=cl><span class=s>    username: admin-user
</span></span></span><span class=line><span class=cl><span class=s>    userID: admin-id
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>inference-manager-engine:
</span></span></span><span class=line><span class=cl><span class=s>  model:
</span></span></span><span class=line><span class=cl><span class=s>    default:
</span></span></span><span class=line><span class=cl><span class=s>      runtimeName: vllm
</span></span></span><span class=line><span class=cl><span class=s>      preloaded: true
</span></span></span><span class=line><span class=cl><span class=s>      resources:
</span></span></span><span class=line><span class=cl><span class=s>        limits:
</span></span></span><span class=line><span class=cl><span class=s>          nvidia.com/gpu: 1
</span></span></span><span class=line><span class=cl><span class=s>    overrides:
</span></span></span><span class=line><span class=cl><span class=s>      meta-llama/Meta-Llama-3.1-8B-Instruct-q4_0:
</span></span></span><span class=line><span class=cl><span class=s>        contextLength: 16384
</span></span></span><span class=line><span class=cl><span class=s>      google/gemma-2b-it-q4_0:
</span></span></span><span class=line><span class=cl><span class=s>        runtimeName: ollama
</span></span></span><span class=line><span class=cl><span class=s>        resources:
</span></span></span><span class=line><span class=cl><span class=s>         limits:
</span></span></span><span class=line><span class=cl><span class=s>           nvidia.com/gpu: 0
</span></span></span><span class=line><span class=cl><span class=s>      sentence-transformers/all-MiniLM-L6-v2-f16:
</span></span></span><span class=line><span class=cl><span class=s>        runtimeName: ollama
</span></span></span><span class=line><span class=cl><span class=s>        resources:
</span></span></span><span class=line><span class=cl><span class=s>         limits:
</span></span></span><span class=line><span class=cl><span class=s>           nvidia.com/gpu: 0
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>inference-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  service:
</span></span></span><span class=line><span class=cl><span class=s>    annotations:
</span></span></span><span class=line><span class=cl><span class=s>      # These annotations are only meaningful for Kong ingress controller to extend the timeout.
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/connect-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/read-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/write-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>job-manager-dispatcher:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  notebook:
</span></span></span><span class=line><span class=cl><span class=s>    # Used to set the base URL of the API endpoint. This can be same as global.ingress.controllerUrl
</span></span></span><span class=line><span class=cl><span class=s>    # if the URL is reachable from the inside cluster. Otherwise you can change this to the
</span></span></span><span class=line><span class=cl><span class=s>    # to the URL of the ingress controller that is reachable inside the K8s cluster.
</span></span></span><span class=line><span class=cl><span class=s>    llmarinerBaseUrl: &#34;${INGRESS_CONTROLLER_URL}/v1&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>model-manager-loader:
</span></span></span><span class=line><span class=cl><span class=s>  baseModels:
</span></span></span><span class=line><span class=cl><span class=s>  - meta-llama/Meta-Llama-3.1-8B-Instruct-q4_0
</span></span></span><span class=line><span class=cl><span class=s>  - google/gemma-2b-it-q4_0
</span></span></span><span class=line><span class=cl><span class=s>  - sentence-transformers/all-MiniLM-L6-v2-f16
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s># Required when RAG is used.
</span></span></span><span class=line><span class=cl><span class=s>vector-store-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  vectorDatabase:
</span></span></span><span class=line><span class=cl><span class=s>    host: &#34;${MILVUS_ADDR}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  llmEngineAddr: ollama-sentence-transformers-all-minilm-l6-v2-f16:11434
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  llmariner oci://public.ecr.aws/cloudnatix/llmariner-charts/llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f llmariner-values.yaml
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Starting from Helm v3.8.0, the OCI registry is supported by default. If you are using an older version, please upgrade to v3.8.0 or later. For more details, please refer to <a href=https://helm.sh/docs/topics/registries/>Helm OCI-based registries</a>.</div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>If you are getting a 403 forbidden error, please try <code>docker logout public.ecr.aws</code>. Please see <a href=https://docs.aws.amazon.com/AmazonECR/latest/public/public-troubleshooting.html>AWS document</a> for more details.</div><p>If you would like to install only the control-plane components or the worker-plane components, please see <code>multi_cluster_deployment</code>{.interpreted-text role=&ldquo;doc&rdquo;}.</p><h2 id=step-7-verify-the-installation>Step 7. Verify the installation<a class=td-heading-self-link href=#step-7-verify-the-installation aria-label="Heading self-link"></a></h2><p>You can verify the installation by sending sample chat completion requests.</p><p>Note, if you have used LLMariner in other cases before you may need to delete the previous config by running <code>rm -rf ~/.config/llmariner</code></p><p>The default login user name is <code>admin@example.com</code> and the password is
<code>password</code>. You can change this by updating the Dex configuration
(<a href=/docs/features/user_management/>link</a>).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;This is your endpoint URL: </span><span class=si>${</span><span class=nv>INGRESS_CONTROLLER_URL</span><span class=si>}</span><span class=s2>/v1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma auth login
</span></span><span class=line><span class=cl><span class=c1># Type the above endpoint URL.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma models list
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion <span class=s2>&#34;what is k8s?&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model meta-llama-Meta-Llama-3.1-8B-Instruct-q4_0 --role user --completion <span class=s2>&#34;hello&#34;</span>
</span></span></code></pre></div><h2 id=optional-monitor-gpu-utilization>Optional: Monitor GPU utilization<a class=td-heading-self-link href=#optional-monitor-gpu-utilization aria-label="Heading self-link"></a></h2><p>If you would like to install Prometheus and Grafana to see GPU utilization, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Add Prometheus</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; prom-scrape-configs.yaml
</span></span></span><span class=line><span class=cl><span class=s>- job_name: nvidia-dcgm
</span></span></span><span class=line><span class=cl><span class=s>  scrape_interval: 5s
</span></span></span><span class=line><span class=cl><span class=s>  static_configs:
</span></span></span><span class=line><span class=cl><span class=s>  - targets: [&#39;nvidia-dcgm-exporter.nvidia.svc:9400&#39;]
</span></span></span><span class=line><span class=cl><span class=s>- job_name: inference-manager-engine-metrics
</span></span></span><span class=line><span class=cl><span class=s>  scrape_interval: 5s
</span></span></span><span class=line><span class=cl><span class=s>  static_configs:
</span></span></span><span class=line><span class=cl><span class=s>  - targets: [&#39;inference-manager-server-http.llmariner.svc:8083&#39;]
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace monitoring <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set-file <span class=nv>extraScrapeConfigs</span><span class=o>=</span>prom-scrape-configs.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  prometheus prometheus-community/prometheus
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add Grafana with DCGM dashboard</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; grafana-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>datasources:
</span></span></span><span class=line><span class=cl><span class=s> datasources.yaml:
</span></span></span><span class=line><span class=cl><span class=s>   apiVersion: 1
</span></span></span><span class=line><span class=cl><span class=s>   datasources:
</span></span></span><span class=line><span class=cl><span class=s>   - name: Prometheus
</span></span></span><span class=line><span class=cl><span class=s>     type: prometheus
</span></span></span><span class=line><span class=cl><span class=s>     url: http://prometheus-server
</span></span></span><span class=line><span class=cl><span class=s>     isDefault: true
</span></span></span><span class=line><span class=cl><span class=s>dashboardProviders:
</span></span></span><span class=line><span class=cl><span class=s>  dashboardproviders.yaml:
</span></span></span><span class=line><span class=cl><span class=s>    apiVersion: 1
</span></span></span><span class=line><span class=cl><span class=s>    providers:
</span></span></span><span class=line><span class=cl><span class=s>    - name: &#39;default&#39;
</span></span></span><span class=line><span class=cl><span class=s>      orgId: 1
</span></span></span><span class=line><span class=cl><span class=s>      folder: &#39;default&#39;
</span></span></span><span class=line><span class=cl><span class=s>      type: file
</span></span></span><span class=line><span class=cl><span class=s>      disableDeletion: true
</span></span></span><span class=line><span class=cl><span class=s>      editable: true
</span></span></span><span class=line><span class=cl><span class=s>      options:
</span></span></span><span class=line><span class=cl><span class=s>        path: /var/lib/grafana/dashboards/standard
</span></span></span><span class=line><span class=cl><span class=s>dashboards:
</span></span></span><span class=line><span class=cl><span class=s>  default:
</span></span></span><span class=line><span class=cl><span class=s>    nvidia-dcgm-exporter:
</span></span></span><span class=line><span class=cl><span class=s>      gnetId: 12239
</span></span></span><span class=line><span class=cl><span class=s>      datasource: Prometheus
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>helm repo add grafana https://grafana.github.io/helm-charts
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace monitoring <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f grafana-values.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  grafana grafana/grafana
</span></span></code></pre></div><h2 id=optional-enable-tls>Optional: Enable TLS<a class=td-heading-self-link href=#optional-enable-tls aria-label="Heading self-link"></a></h2><p>First follow the <a href=https://cert-manager.io/docs/installation/>cert-manager installation document</a> and install cert-manager to your K8s cluster if you don&rsquo;t have one. Then create a <code>ClusterIssuer</code> for your domain. Here is an example manifest that uses Let's Encrypt.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert-manager.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIssuer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>acme</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>server</span><span class=p>:</span><span class=w> </span><span class=l>https://acme-v02.api.letsencrypt.org/directory</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>user@mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>privateKeySecretRef</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>solvers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>http01</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>       </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>ingressClassName</span><span class=p>:</span><span class=w> </span><span class=l>kong</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>dnsZones</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dns01</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div><p>Then you can add the following to <code>values.yaml</code> of LLMariner to enable TLS.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span></code></pre></div><p>The ingresses created from the Helm chart will have the following annotation and spec:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nn>...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-830457b369082df60146f171d2fecbac>2.1.6 - Install across Multiple Clusters</h1><div class=lead>Install LLMarinr across multiple Kubernetes clusters.</div><p>LLMariner deploys Kubernetes deployments to provision the LLM stack. In a typical configuration, all the services are deployed into a single Kubernetes cluster, but you can also deploy these services on multiple Kubernetes clusters. For example, you can deploy a control plane component in a CPU K8s cluster and deploy the rest of the components in GPU compute clusters.</p><p>LLMariner can be deployed into multiple GPU clusters, and the clusters can span across multiple cloud providers (including GPU specific clouds like CoreWeave) and on-prem.</p><p class="mt-4 mb-4 text-center"><img src=/images/multi_cluster.png width=1027 height=817></p><h2 id=deploying-control-plane-components>Deploying Control Plane Components<a class=td-heading-self-link href=#deploying-control-plane-components aria-label="Heading self-link"></a></h2><p>You can deploy only Control Plane components by specifying additional parameters the LLMariner helm chart.</p><p>In the <code>values.yaml</code>, you need to set <code>tag.worker</code> to <code>false</code>, <code>global.workerServiceIngress.create</code> to <code>true</code>, and set other values so that an ingress and a service are created to receive requests from worker nodes.</p><p>Here is an example <code>values.yaml</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>tags</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>worker</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ingressClassName</span><span class=p>:</span><span class=w> </span><span class=l>kong</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>controllerUrl</span><span class=p>:</span><span class=w> </span><span class=l>https://api.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>konghq.com/response-buffering</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;false&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c># Enable TLS for the ingresses.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c># Create ingress for gRPC requests coming from worker clusters.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceIngress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>create</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>konghq.com/protocols</span><span class=p>:</span><span class=w> </span><span class=l>grpc,grpcs</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceGrpcService</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>konghq.com/protocol</span><span class=p>:</span><span class=w> </span><span class=l>grpc</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># Create a separate load balancer for gRPC streaming requests from inference-manager-engine.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>inference-manager-server</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceTls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>inference-cert</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceGrpcService</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>LoadBalancer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>external-dns.alpha.kubernetes.io/hostname</span><span class=p>:</span><span class=w> </span><span class=l>inference.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># Create a separate load balancer for HTTPS requests from session-manager-agent.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>session-manager-server</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceTls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>session-cert</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceHttpService</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>LoadBalancer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>externalTrafficPolicy</span><span class=p>:</span><span class=w> </span><span class=l>Local</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>service.beta.kubernetes.io/aws-load-balancer-type</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;nlb&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>external-dns.alpha.kubernetes.io/hostname</span><span class=p>:</span><span class=w> </span><span class=l>session.llm.mydomain.com</span><span class=w>
</span></span></span></code></pre></div><h2 id=deploying-worker-components>Deploying Worker Components<a class=td-heading-self-link href=#deploying-worker-components aria-label="Heading self-link"></a></h2><p>To deploy LLMariner to a worker GPU cluster, you first need to obtain a registration key for the cluster.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin clusters register &lt;cluster-name&gt;
</span></span></code></pre></div><p>The following is an example command that sets the registration key to the environment variable.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>REGISTRATION_KEY</span><span class=o>=</span><span class=k>$(</span>llma admin clusters register &lt;cluster-name&gt; <span class=p>|</span> sed -n <span class=s1>&#39;s/.*Registration Key: &#34;\([^&#34;]*\)&#34;.*/\1/p&#39;</span><span class=k>)</span>
</span></span></code></pre></div><p>The command generates a new registration key.</p><p>Then you need to make LLMariner worker components to use the registration key when making gRPC calls to the control plane.</p><p>To make that happen, you first need to create a K8s secret.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>REGISTRATION_KEY</span><span class=o>=</span>clusterkey-...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl create secret generic <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -n llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  cluster-registration-key <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-literal<span class=o>=</span><span class=nv>regKey</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>${</span><span class=nv>REGISTRATION_KEY</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>The secret needs to be created in a namespace where LLMariner will be deployed.</p><p>When installing the Helm chart for the worker components, you need to specify addition configurations in <code>values.yaml</code>. Here is an example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>tags</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>control-plane</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>objectStore</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>s3</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>endpointUrl</span><span class=p>:</span><span class=w> </span><span class=l>&lt;S3 endpoint&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>region</span><span class=p>:</span><span class=w> </span><span class=l>&lt;S3 regiona&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>bucket</span><span class=p>:</span><span class=w> </span><span class=l>&lt;S3 bucket name&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>awsSecret</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>aws</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>accessKeyIdKey</span><span class=p>:</span><span class=w> </span><span class=l>accessKeyId</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretAccessKeyKey</span><span class=p>:</span><span class=w> </span><span class=l>secretAccessKey</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>worker</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>controlPlaneAddr</span><span class=p>:</span><span class=w> </span><span class=l>api.llm.mydomain.com:443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>registrationKeySecret</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cluster-registration-key</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>key</span><span class=p>:</span><span class=w> </span><span class=l>regKey</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>inference-manager-engine</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>inferenceManagerServerWorkerServiceAddr</span><span class=p>:</span><span class=w> </span><span class=l>inference.llm.mydomain.com:443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>job-manager-dispatcher</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>notebook</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>llmarinerBaseUrl</span><span class=p>:</span><span class=w> </span><span class=l>https://api.llm.mydomain.com/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>session-manager-agent</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>sessionManagerServerWorkerServiceAddr</span><span class=p>:</span><span class=w> </span><span class=l>session.llm.mydomain.com:443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>model-manager-loader</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>baseModels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>&lt;model name, e.g. google/gemma-2b-it-q4_0&gt;</span><span class=w>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-aa1273494c214f035a8e9386ce6707b0>2.1.7 - Hosted Control Plane</h1><div class=lead>Install just the worker plane and use it with the hosted control plane.</div><p>CloudNatix provides a hosted control plane of LLMariner.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Work-in-progress. This is not fully ready yet, and the terms and conditions are subject to change as we might limit the usage based on the number of API calls or the number of GPU nodes.</div><p>CloudNatix provides a hosted control plane of LLMariner. End users can use the full functionality of LLMariner just by registering their worker GPU clusters to this hosted control plane.</p><h2 id=step-1-create-a-cloudnatix-account>Step 1. Create a CloudNatix account<a class=td-heading-self-link href=#step-1-create-a-cloudnatix-account aria-label="Heading self-link"></a></h2><p>Create a CloudNatix account if you haven't. Please visit <a href=https://app.cloudnatix.com>https://app.cloudnatix.com</a>. You can click one of the "Sign in or sing up" buttons for SSO login or you can click "Sign up" at the bottom for the email & password login.</p><h2 id=step-2-deploy-the-worker-plane-components>Step 2. Deploy the worker plane components<a class=td-heading-self-link href=#step-2-deploy-the-worker-plane-components aria-label="Heading self-link"></a></h2><p>Deploy the worker plane components LLMariner into your GPU cluster.</p><p>The API endpoint of the hosted control plane is <a href=https://api.llm.cloudnatix.com/v1>https://api.llm.cloudnatix.com/v1</a>.</p><p>Run <code>llma auth login</code> and use the above for the endpoint URL. Then follow <code>multi_cluster_deployment</code>{.interpreted-text role=&ldquo;doc&rdquo;} to obtain a cluster registration key and deploy LLMariner.</p><p>TODO: Add an example <code>values.yaml</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-270067265d5e2225da8588f16967a603>2.2 - Tutorials</h1><div class=lead>Here are the links to the tutorials. You can download the Jupyter Notebooks and exercise the tutorials.</div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Work-in-progress.</div><ul><li><a href=https://github.com/llmariner/llmariner/blob/main/tutorials/getting_started.ipynb>https://github.com/llmariner/llmariner/blob/main/tutorials/getting_started.ipynb</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-56dc2b1c45b2c6b0983526530ba75d01>3 - Features</h1><div class=lead>LLMariner features</div></div><div class=td-content><h1 id=pg-e254f2bb7611309715fbd117e4ceb3b7>3.1 - Inference with Open Models</h1><div class=lead>Users can run chat completion with open models such as Google Gemma, LLama, Mistral, etc. To run chat completion, users can use the OpenAI Python library, <code>llma</code> CLI, or API endpoint.</div><h2 id=chat-completion>Chat Completion<a class=td-heading-self-link href=#chat-completion aria-label="Heading self-link"></a></h2><p>Here is an example chat completion command with the <code>llma</code> CLI.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion <span class=s2>&#34;What is k8s?&#34;</span>
</span></span></code></pre></div><p>If you want to use the Python library, you first need to create an API key:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma auth api-keys create &lt;key name&gt;
</span></span></code></pre></div><p>You can then pass the API key to initialize the OpenAI client and run the completion:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;&lt;Base URL (e.g., http://localhost:8080/v1)&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;&lt;API key secret&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it-q4_0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;What is k8s?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>response</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>You can also just call ``client = OpenAI()<code>if you set environment variables</code>OPENAI_BASE_URL<code>and</code>OPENAI_API_KEY`.</p><p>If you want to hit the API endpoint directly, you can use <code>curl</code>. Here is an example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data <span class=s1>&#39;{&#34;model&#34;: &#34;google-gemma-2b-it-q4_0&#34;, &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is k8s?&#34;}]}&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:8080/v1/chat/completions
</span></span></code></pre></div><p>Please see <a href=/docs/features/fine_tuning/>the fine-tuning page</a> if you want to generate a fine-tuning model and use that for chat completion.</p><h2 id=audio-to-text>Audio-to-Text<a class=td-heading-self-link href=#audio-to-text aria-label="Heading self-link"></a></h2><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Work-in-progress.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-a90036a92e39d96ca98dbb924258b27b>3.2 - Supported Open Models</h1><div class=lead>The following shows the supported models.</div><h2 id=models-that-are-officially-supported>Models that are Officially Supported<a class=td-heading-self-link href=#models-that-are-officially-supported aria-label="Heading self-link"></a></h2><p>The following is a list of supported models where we have validated.</p><table><thead><tr><th>Model</th><th>Quantizations</th><th>Supporting runtimes</th></tr></thead><tbody><tr><td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td><td>None</td><td>vLLM</td></tr><tr><td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td><td>AWQ</td><td>vLLM</td></tr><tr><td>deepseek-ai/DeepSeek-Coder-V2-Lite-Base</td><td>Q2_K, Q3_K_M, Q3_K_S, Q4_0</td><td>Ollama</td></tr><tr><td>deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct</td><td>Q2_K, Q3_K_M, Q3_K_S, Q4_0</td><td>Ollama</td></tr><tr><td>deepseek-ai/deepseek-coder-6.7b-base</td><td>None</td><td>vLLM, Ollama</td></tr><tr><td>deepseek-ai/deepseek-coder-6.7b-base</td><td>AWQ</td><td>vLLM</td></tr><tr><td>deepseek-ai/deepseek-coder-6.7b-base</td><td>Q4_0</td><td>vLLM, Ollama</td></tr><tr><td>fixie-ai/ultravox-v0_3</td><td>None</td><td>vLLM</td></tr><tr><td>google/gemma-2b-it</td><td>None</td><td>Ollama</td></tr><tr><td>google/gemma-2b-it</td><td>Q4_0</td><td>Ollama</td></tr><tr><td>intfloat/e5-mistral-7b-instruct</td><td>None</td><td>vLLM</td></tr><tr><td>meta-llama/Meta-Llama-3.3-70B-Instruct</td><td>AWQ, FP8-Dynamic</td><td>vLLM</td></tr><tr><td>meta-llama/Meta-Llama-3.1-70B-Instruct</td><td>AWQ</td><td>vLLM</td></tr><tr><td>meta-llama/Meta-Llama-3.1-70B-Instruct</td><td>Q2_K, Q3_K_M, Q3_K_S, Q4_0</td><td>vLLM, Ollama</td></tr><tr><td>meta-llama/Meta-Llama-3.1-8B-Instruct</td><td>None</td><td>vLLM</td></tr><tr><td>meta-llama/Meta-Llama-3.1-8B-Instruct</td><td>AWQ</td><td>vLLM, Triton</td></tr><tr><td>meta-llama/Meta-Llama-3.1-8B-Instruct</td><td>Q4_0</td><td>vLLM, Ollama</td></tr><tr><td>nvidia/Llama-3.1-Nemotron-70B-Instruct</td><td>Q2_K, Q3_K_M, Q3_K_S, Q4_0</td><td>vLLM</td></tr><tr><td>nvidia/Llama-3.1-Nemotron-70B-Instruct</td><td>FP8-Dynamic</td><td>vLLM</td></tr><tr><td>mistralai/Mistral-7B-Instruct-v0.2</td><td>Q4_0</td><td>Ollama</td></tr><tr><td>sentence-transformers/all-MiniLM-L6-v2-f16</td><td>None</td><td>Ollama</td></tr></tbody></table><p>Please note that some models work only with specific inference runtimes.</p><h2 id=using-other-models-in-huggingface>Using Other Models in HuggingFace<a class=td-heading-self-link href=#using-other-models-in-huggingface aria-label="Heading self-link"></a></h2><p>First, create a k8s secret that contains the HuggingFace API key.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl create secret generic <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  huggingface-key <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -n llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-literal<span class=o>=</span><span class=nv>apiKey</span><span class=o>=</span><span class=si>${</span><span class=nv>HUGGING_FACE_HUB_TOKEN</span><span class=si>}</span>
</span></span></code></pre></div><p>The above command assumes that LLMarine runs in the <code>llmariner</code> namespace.</p><p>Then deploy LLMariner with the following <code>values.yaml</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>model-manager-loader</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>downloader</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>huggingFace</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>huggingFace</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cacheDir</span><span class=p>:</span><span class=w> </span><span class=l>/tmp/.cache/huggingface/hub</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>huggingFaceSecret</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>huggingface-key</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>apiKeyKey</span><span class=p>:</span><span class=w> </span><span class=l>apiKey</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>baseModels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>Qwen/Qwen2-7B</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>TheBloke/TinyLlama-1.1B-Chat-v1.0-AWQ</span><span class=w>
</span></span></span></code></pre></div><p>Then the model should be loaded by <code>model-manager-loader</code>. Once the loading completes, the model name
should show up in the output of <code>llma models list</code>.</p><p>When you use a GGUF model with vLLM, please specify <code>--tokenizer=&lt;original model></code> in <code>vllmExtraFlags</code>. Here is an example
configuration for Phi 4.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>inference-manager-engine</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>model</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>default</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>runtimeName</span><span class=p>:</span><span class=w> </span><span class=l>vllm</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>overrides</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>lmstudio-community/phi-4-GGUF/phi-4-Q6_K.gguf</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>preloaded</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>limits</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>            </span><span class=nt>nvidia.com/gpu</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>vllmExtraFlags</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- --<span class=l>tokenizer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>microsoft/phi-4</span><span class=w>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-559ba074f66298c7e9908d15750ab530>3.3 - Retrieval-Augmented Generation (RAG)</h1><div class=lead>This page describes how to use RAG with LLMariner.</div><h2 id=an-example-flow>An Example Flow<a class=td-heading-self-link href=#an-example-flow aria-label="Heading self-link"></a></h2><p>The first step is to create a vector store and create files in the vector store. Here is an example script with the OpenAI Python library:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;&lt;LLMariner Endpoint URL&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;&lt;LLMariner API key&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>filename</span> <span class=o>=</span> <span class=s2>&#34;llmariner_overview.txt&#34;</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s2>&#34;w&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>fp</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>fp</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s2>&#34;LLMariner builds a software stack that provides LLM as a service. It provides the OpenAI-compatible API.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>file</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>files</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>file</span><span class=o>=</span><span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>  <span class=n>purpose</span><span class=o>=</span><span class=s2>&#34;assistants&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Uploaded file. ID=</span><span class=si>%s</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=n>file</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>vs</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>beta</span><span class=o>.</span><span class=n>vector_stores</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>name</span><span class=o>=</span><span class=s1>&#39;Test vector store&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Created vector store. ID=</span><span class=si>%s</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=n>vs</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>vfs</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>beta</span><span class=o>.</span><span class=n>vector_stores</span><span class=o>.</span><span class=n>files</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>vector_store_id</span><span class=o>=</span><span class=n>vs</span><span class=o>.</span><span class=n>id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>file_id</span><span class=o>=</span><span class=n>file</span><span class=o>.</span><span class=n>id</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Created vector store file. ID=</span><span class=si>%s</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=n>vfs</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
</span></span></code></pre></div><p>Once the files are added into vector store, you can run the completion request with the RAG model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;&lt;Base URL (e.g., http://localhost:8080/v1)&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;&lt;API key secret&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it-q4_0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;What is LLMariner?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>tool_choice</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;choice&#34;</span><span class=p>:</span> <span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;function&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;function&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>     <span class=s2>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;rag&#34;</span>
</span></span><span class=line><span class=cl>   <span class=p>}</span>
</span></span><span class=line><span class=cl> <span class=p>},</span>
</span></span><span class=line><span class=cl> <span class=n>tools</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>   <span class=p>{</span>
</span></span><span class=line><span class=cl>     <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;function&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>     <span class=s2>&#34;function&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>       <span class=s2>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;rag&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=s2>&#34;parameters&#34;</span><span class=p>:</span> <span class=s2>&#34;{</span><span class=se>\&#34;</span><span class=s2>vector_store_name</span><span class=se>\&#34;</span><span class=s2>:</span><span class=se>\&#34;</span><span class=s2>Test vector store</span><span class=se>\&#34;</span><span class=s2>}&#34;</span>
</span></span><span class=line><span class=cl>     <span class=p>}</span>
</span></span><span class=line><span class=cl>   <span class=p>}</span>
</span></span><span class=line><span class=cl> <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>response</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>If you want to hit the API endpoint directly, you can use <code>curl</code>. Here is an example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;model&#34;: &#34;google-gemma-2b-it-q4_0&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is LLMariner?&#34;}],
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;tool_choice&#34;: {
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;choice&#34;: &#34;auto&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;type&#34;: &#34;function&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;function&#34;: {
</span></span></span><span class=line><span class=cl><span class=s1>       &#34;name&#34;: &#34;rag&#34;
</span></span></span><span class=line><span class=cl><span class=s1>     }
</span></span></span><span class=line><span class=cl><span class=s1>   },
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;tools&#34;: [{
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;type&#34;: &#34;function&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;function&#34;: {
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;name&#34;: &#34;rag&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>       &#34;parameters&#34;: &#34;{\&#34;vector_store_name\&#34;:\&#34;Test vector store\&#34;}&#34;
</span></span></span><span class=line><span class=cl><span class=s1>     }
</span></span></span><span class=line><span class=cl><span class=s1> }]}&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span> http://localhost:8080/v1/chat/completions
</span></span></code></pre></div><h2 id=embedding-api>Embedding API<a class=td-heading-self-link href=#embedding-api aria-label="Heading self-link"></a></h2><p>If you want to just generate embeddings, you can use the Embedding API, which is compatible with the OpenAI API.</p><p>Here are examples:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma embeddings create --model intfloat-e5-mistral-7b-instruct --input <span class=s2>&#34;sample text&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;model&#34;: &#34;sentence-transformers-all-MiniLM-L6-v2-f16&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;input&#34;: &#34;sample text&#34;
</span></span></span><span class=line><span class=cl><span class=s1> }&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span> http://localhost:8080/v1/embeddings
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e543d8263acb53250256de185d42cd21>3.4 - Model Fine-tuning</h1><div class=lead>This page describes how to fine-tune models with LLMariner.</div><h2 id=submitting-a-fine-tuning-job>Submitting a Fine-Tuning Job<a class=td-heading-self-link href=#submitting-a-fine-tuning-job aria-label="Heading self-link"></a></h2><p>You can use the OpenAI Python library to submit a fine-tuning job. Here is an example snippet that uploads a training file and uses that to run a fine-tuning job.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;&lt;LLMariner Endpoint URL&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;&lt;LLMariner API key&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>file</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>files</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>file</span><span class=o>=</span><span class=nb>open</span><span class=p>(</span><span class=n>training_filename</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>  <span class=n>purpose</span><span class=o>=</span><span class=s2>&#34;fine-tune&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>job</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>fine_tuning</span><span class=o>.</span><span class=n>jobs</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>suffix</span><span class=o>=</span><span class=s2>&#34;fine-tuning&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>training_file</span><span class=o>=</span><span class=n>file</span><span class=o>.</span><span class=n>id</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Created job. ID=</span><span class=si>%s</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>job</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
</span></span></code></pre></div><p>Once a fine-tuning job is submitted, a k8s Job is created. A Job runs in a namespace where a user's project is associated.</p><p>You can check the status of the job with the Python script or the <code>llma</code> CLI.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>client</span><span class=o>.</span><span class=n>fine_tuning</span><span class=o>.</span><span class=n>jobs</span><span class=o>.</span><span class=n>list</span><span class=p>())</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma fine-tuning <span class=nb>jobs</span> list
</span></span><span class=line><span class=cl>llma fine-tuning <span class=nb>jobs</span> get &lt;job-id&gt;
</span></span></code></pre></div><p>Once the job completes, you can check the generated models.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>fine_tuned_model</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>fine_tuning</span><span class=o>.</span><span class=n>jobs</span><span class=o>.</span><span class=n>list</span><span class=p>()</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>fine_tuned_model</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>fine_tuned_model</span><span class=p>)</span>
</span></span></code></pre></div><p>Then you can get the model ID and use that for the chat completion request.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=n>fine_tuned_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=o>...</span>
</span></span></code></pre></div><h2 id=debugging-a-fine-tuning-job>Debugging a Fine-Tuning Job<a class=td-heading-self-link href=#debugging-a-fine-tuning-job aria-label="Heading self-link"></a></h2><p>You can use the <code>llma</code> CLI to check the logs and exec into the pod.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma fine-tuning <span class=nb>jobs</span> logs &lt;job-id&gt;
</span></span><span class=line><span class=cl>llma fine-tuning <span class=nb>jobs</span> <span class=nb>exec</span> &lt;job-id&gt;
</span></span></code></pre></div><h2 id=managing-quota>Managing Quota<a class=td-heading-self-link href=#managing-quota aria-label="Heading self-link"></a></h2><p>LLMariner allows users to manage GPU quotas with integration with <a href=https://kueue.sigs.k8s.io/>Kueue</a>.</p><p>You can install Kueue with the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>VERSION</span><span class=o>=</span>v0.6.2
</span></span><span class=line><span class=cl>kubectl apply -f https://github.com/kubernetes-sigs/kueue/releases/download/<span class=nv>$VERSION</span>/manifests.yaml
</span></span></code></pre></div><p>Once the install completes, you should see <code>kueue-controller-manager</code> in the <code>kueue-system</code> namespace.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl get po -n kueue-system
</span></span><span class=line><span class=cl>NAME                                        READY   STATUS    RESTARTS   AGE
</span></span><span class=line><span class=cl>kueue-controller-manager-568995d897-bzxg6   2/2     Running   <span class=m>0</span>          161m
</span></span></code></pre></div><p>You can then define <code>ResourceFlavor</code>, <code>ClusterQueue</code>, and <code>LocalQueue</code> to manage quota. For example, when you want to allocate 10 GPUs to <code>team-a</code> whose project namespace is <code>team-a-ns</code>, you can define <code>ClusterQueue</code> and <code>LocalQueue</code> as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>kueue.x-k8s.io/v1beta1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterQueue</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>team-a</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespaceSelector</span><span class=p>:</span><span class=w> </span>{}<span class=w> </span><span class=c># match all.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>cohort</span><span class=p>:</span><span class=w> </span><span class=l>org-x</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>resourceGroups</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>coveredResources</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>gpu]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>flavors</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>gpu-flavor</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>gpu</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nominalQuota</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nn>---</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>kueue.x-k8s.io/v1beta1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>LocalQueue</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>team-a-ns</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>team-a-queue</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>clusterQueue</span><span class=p>:</span><span class=w> </span><span class=l>team-a</span><span class=w>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-0eae5360aca6a618ec933d565b825e7e>3.5 - General-purpose Training</h1><div class=lead>LLMariner allows users to run general-purpose training jobs in their Kubernetes clusters.</div><h2 id=creating-a-training-job>Creating a Training Job<a class=td-heading-self-link href=#creating-a-training-job aria-label="Heading self-link"></a></h2><p>You can create a training job from the local pytorch code by running the following command.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma batch <span class=nb>jobs</span> create <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --image<span class=o>=</span><span class=s2>&#34;pytorch-2.1&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-file<span class=o>=</span>my-pytorch-script.py <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-file<span class=o>=</span>requirements.txt <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --file-id<span class=o>=</span>&lt;file-id&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --command <span class=s2>&#34;python -u /scripts/my-pytorch-script.py&#34;</span>
</span></span></code></pre></div><p>Once a training job is created, a k8s Job is created. The job runs the command specified in the <code>--command</code> flag, and files specified in the <code>--from-file</code> flag are mounted to the /scripts directory in the container. If you specify the <code>--file-id</code> flag (optional), the file will be download to the /data directory in the container.</p><p>You can check the status of the job by running the following command.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma batch <span class=nb>jobs</span> list
</span></span><span class=line><span class=cl>llma batch <span class=nb>jobs</span> get &lt;job-id&gt;
</span></span></code></pre></div><h2 id=debugging-a-training-job>Debugging a Training Job<a class=td-heading-self-link href=#debugging-a-training-job aria-label="Heading self-link"></a></h2><p>You can use the <code>llma</code> CLI to check the logs of a training job.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma batch <span class=nb>jobs</span> logs &lt;job-id&gt;
</span></span></code></pre></div><h2 id=pytorch-distributed-data-parallel>PyTorch Distributed Data Parallel<a class=td-heading-self-link href=#pytorch-distributed-data-parallel aria-label="Heading self-link"></a></h2><p>LLMariner supports PyTorch Distributed Data Parallel (DDP) training. You can run a DDP training job by specifying the number of per-node GPUs and the number of workers in the <code>--gpu</code> and <code>--workers</code> flags, respectively.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma batch <span class=nb>jobs</span> create <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --image<span class=o>=</span><span class=s2>&#34;pytorch-2.1&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-file<span class=o>=</span>my-pytorch-ddp-script.py <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --gpu<span class=o>=</span><span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --workers<span class=o>=</span><span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --command <span class=s2>&#34;python -u /scripts/my-pytorch-ddp-script.py&#34;</span>
</span></span></code></pre></div><p>Created training job is pre-configured some DDP environment variables; MASTER_ADDR, MASTER_PORT, WORLD_SIZE, and RANK.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-28b6f41d7083500232434001016b060d>3.6 - Jupyter Notebook</h1><div class=lead>LLMariner allows users to run a Jupyter Notebook in a Kubernetes cluster. This functionality is useful when users want to run ad-hoc Python scripts that require GPU.</div><h2 id=creating-a-jupyter-notebook>Creating a Jupyter Notebook<a class=td-heading-self-link href=#creating-a-jupyter-notebook aria-label="Heading self-link"></a></h2><p>To create a Jupyter Notebook, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks create my-notebook
</span></span></code></pre></div><p>By default, there is no GPU allocated to the Jupyter Notebook. If you want to allocate a GPU to the Jupyter Notebook, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks create my-gpu-notebook --gpu <span class=m>1</span>
</span></span></code></pre></div><p>There are other options that you can specify when creating a Jupyter Notebook, such as environment. You can see the list of options by using the <code>--help</code> flag.</p><p>Once the Jupyter Notebook is created, you can access it by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Open the Jupyter Notebook in your browser</span>
</span></span><span class=line><span class=cl>llma workspace notebooks open my-notebook
</span></span></code></pre></div><h2 id=stopping-and-restarting-a-jupyter-notebook>Stopping and Restarting a Jupyter Notebook<a class=td-heading-self-link href=#stopping-and-restarting-a-jupyter-notebook aria-label="Heading self-link"></a></h2><p>To stop a Jupyter Notebook, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks stop my-notebook
</span></span></code></pre></div><p>To restart a Jupyter Notebook, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks start my-notebook
</span></span></code></pre></div><p>You can check the current status of the Jupyter Notebook by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks list
</span></span><span class=line><span class=cl>llma workspace notebooks get my-notebook
</span></span></code></pre></div><h2 id=openai-api-integration>OpenAI API Integration<a class=td-heading-self-link href=#openai-api-integration aria-label="Heading self-link"></a></h2><p>Jupyter Notebook can be integrated with OpenAI API. Created Jupyter Notebook is pre-configured with OpenAI API URL and API key. All you need to do is to install the <code>openai</code> package.</p><p>To install <code>openai</code> package, run the following command in the Jupyter Notebook terminal:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install openai
</span></span></code></pre></div><p class="mt-4 mb-4 text-center"><img src=/images/jupyter_notebook_terminal.png width=1500 height=1085></p><p>Now, you can use the OpenAI API in the Jupyter Notebook. Here is an example of using OpenAI API in the Jupyter Notebook:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it-q4_0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;What is k8s?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>response</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p class="mt-4 mb-4 text-center"><img src=/images/jupyter_notebook_ipynb.png width=1500 height=1085></p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>By default, pre-configured API key is a JWT and it can expire. You can also pass your API key to the <code>OpenAI</code> client.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-801a2e5cd9c3c1c32f39935005c3f24e>3.7 - API and GPU Usage Optimization</h1><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Work-in-progress.</div><h2 id=api-usage-visibility>API Usage Visibility<a class=td-heading-self-link href=#api-usage-visibility aria-label="Heading self-link"></a></h2><h2 id=inference-request-rate-limiting>Inference Request Rate-limiting<a class=td-heading-self-link href=#inference-request-rate-limiting aria-label="Heading self-link"></a></h2><h2 id=optimize-gpu-utilization>Optimize GPU Utilization<a class=td-heading-self-link href=#optimize-gpu-utilization aria-label="Heading self-link"></a></h2><h3 id=auto-scaling-of-inference-runtimes>Auto-scaling of Inference Runtimes<a class=td-heading-self-link href=#auto-scaling-of-inference-runtimes aria-label="Heading self-link"></a></h3><h3 id=scheduled-scale-up-and-down-of-inference-runtimes>Scheduled Scale Up and Down of Inference Runtimes<a class=td-heading-self-link href=#scheduled-scale-up-and-down-of-inference-runtimes aria-label="Heading self-link"></a></h3></div><div class=td-content style=page-break-before:always><h1 id=pg-728e0b08215c55af1b72e7bae479d8a2>3.8 - User Management</h1><div class=lead>Describes the way to manage users</div><p>LLMariner installs <a href=https://github.com/dexidp/dex>Dex</a> by default. Dex is an identity service that uses <a href=https://openid.net/developers/how-connect-works/>OpenID Connect</a> for authentication.</p><p>The Helm chart for Dex is located at <a href=https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server>https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server</a>. It uses a <a href=https://dexidp.io/docs/connectors/local/>built-in local connector</a> and has the following configuration by default:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>staticPasswords</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>userID</span><span class=p>:</span><span class=w> </span><span class=l>08a8684b-db88-4b73-90a9-3cd1661f5466</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>username</span><span class=p>:</span><span class=w> </span><span class=l>admin</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>admin@example.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c># bcrypt hash of the string: $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hash</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W&#34;</span><span class=w>
</span></span></span></code></pre></div><p>You can switch a connector to an IdP in your environment (e.g., LDAP, GitHub). Here is an example connector configuration with Okta:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>auth</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>oidcIssuerUrl</span><span class=p>:</span><span class=w> </span><span class=l>https://&lt;LLMariner endpoint URL&gt;/v1/dex</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>dex-server</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>oauth2</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>passwordConnector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>responseTypes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>code</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>connectors</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>oidc</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>id</span><span class=p>:</span><span class=w> </span><span class=l>okta</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>okta</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>issuer</span><span class=p>:</span><span class=w> </span><span class=l>&lt;Okta issuer URL&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>clientID</span><span class=p>:</span><span class=w> </span><span class=l>&lt;Client ID of an Okta application&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>clientSecret</span><span class=p>:</span><span class=w> </span><span class=l>&lt;Client secret of an Okta application&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>redirectURI</span><span class=p>:</span><span class=w> </span><span class=l>https://&lt;LLMariner endpoint URL&gt;/v1/dex/callback</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>insecureSkipEmailVerified</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>enablePasswordDb</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>staticPassword</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span></code></pre></div><p>Please refer to the <a href=https://dexidp.io/docs/connectors/>Dec documentations</a> for more details.</p><p>The Helm chart for Dex creates an ingress so that HTTP requests to <code>v1/dex</code> are routed to Dex. This endpoint URL works as the OIDC issuer URL that CLI and backend servers use.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-dac07794e5c1cd0aa1170dedef9d101d>3.9 - Access Control with Organizations and Projects</h1><div class=lead>The way to configure access control using organizations and projects</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><h3 id=basic-concepts>Basic Concepts<a class=td-heading-self-link href=#basic-concepts aria-label="Heading self-link"></a></h3><p>LLMariner provides access control with two concepts: <code>Organizations</code> and <code>Projects</code>. The basic concept follows <a href=https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects>OpenAI API</a>.</p><p>You can define one or more than one organization. In each organization, you can define one or more than one project. For example, you can create an organization for each team in your company, and each team can create individual projects based on their needs.</p><p>A project controls the visibility of resources such as models, fine-tuning jobs. For example, a model that is generated by a fine-tuned job in project <code>P</code> is only visible from project members in <code>P</code>.</p><p>A project is also associated with a Kubernetes namespace. Fine-tuning jobs for project <code>P</code> run in the Kubernetes namespace associated with <code>P</code> (and quota management is applied).</p><h3 id=roles>Roles<a class=td-heading-self-link href=#roles aria-label="Heading self-link"></a></h3><p>Each user has an <code>organization role</code> and a <code>project role</code>, and these roles control resources that a user can access and actions that a user can take.</p><p>An organization role is either <code>owner</code> or <code>reader</code>. A project role is either <code>owner</code> or <code>member</code>. If you want to allow a user to use LLMariner without any organization/project management privilege, you can grant the organization role <code>reader</code> and the project role <code>member</code>. If you want to allow a user to manage the project, you can grant the project role <code>owner</code>.</p><p>Here is an diagram shows an example role assignment.</p><p class="mt-4 mb-4 text-center"><img src=/images/org_and_project.png width=923 height=420></p><p>The following summarizes how these role implements the access control:</p><ul><li>A user can access resources in project <code>P</code> in organization <code>O</code> if the user is a <code>member</code> of <code>P</code>, <code>owner</code> of <code>P</code>, or <code>owner</code> of <code>O</code>.</li><li>A user can manage project <code>P</code> (e.g., add a new member) in organization <code>O</code> if the user is an <code>owner</code> of <code>P</code> or <code>owner</code> of <code>O</code>.</li><li>A user can manage organization <code>O</code> (e.g., add a new member) if the user is an <code>owner</code> of <code>O</code>.</li><li>A user can create a new organization if the user is an <code>owner</code> of the initial organization that is created by default.</li></ul><p>Please note that a user who has the <code>reader</code> organization role cannot access resources in the organization unless the user is added to a project in the organization.</p><h2 id=creating-organizations-and-projects>Creating Organizations and Projects<a class=td-heading-self-link href=#creating-organizations-and-projects aria-label="Heading self-link"></a></h2><p>You can use CLI <code>llma</code> to create a new organization and a project.</p><h3 id=creating-a-new-organization>Creating a new Organization<a class=td-heading-self-link href=#creating-a-new-organization aria-label="Heading self-link"></a></h3><p>You can run the following command to create a new organization.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin organizations create &lt;organization title&gt;
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>You can also type <code>llm auth orgs</code> instead of <code>llm auth organizations</code>.</div><p>You can confirm that the new organization is created by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin organizations list
</span></span></code></pre></div><p>Then you can add a user member to the organization.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin organizations add-member &lt;organization title&gt; --email &lt;email-address of the member&gt; --role &lt;role&gt;
</span></span></code></pre></div><p>The role can be either <code>owner</code> or <code>reader</code>.</p><p>You can confirm organization members by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin organizations list-members &lt;organization title&gt;
</span></span></code></pre></div><h3 id=creating-a-new-project>Creating a new Project<a class=td-heading-self-link href=#creating-a-new-project aria-label="Heading self-link"></a></h3><p>You can take a similar flow to create a new project. To create a new project, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin projects create --title &lt;project title&gt; --organization-title &lt;organization title&gt;
</span></span></code></pre></div><p>To confirm the project is created, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin projects list
</span></span></code></pre></div><p>Then you can add a user member to the project.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin projects add-member &lt;project title&gt; --email &lt;email-address of the member&gt; --role &lt;role&gt;
</span></span></code></pre></div><p>The role can be either <code>owner</code> or <code>member</code>.</p><p>You can confirm project members by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin projects list-members --title &lt;project title&gt; --organization-title &lt;organization title&gt;
</span></span></code></pre></div><p>If you want to manage a project in a different organization, you can pass <code>--organization-title &lt;title></code> in each command. Otherwise, the organization in the current context is used. You can also change the current context by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma context <span class=nb>set</span>
</span></span></code></pre></div><h2 id=choosing-an-organization-and-a-project>Choosing an Organization and a Project<a class=td-heading-self-link href=#choosing-an-organization-and-a-project aria-label="Heading self-link"></a></h2><p>You can use <code>llma context set</code> to set the current context.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma context <span class=nb>set</span>
</span></span></code></pre></div><p>Then the selected context is applied to CLI commands (e.g., <code>llma models list</code>).</p><p>When you create a new API key, the key will be associated with the project in the current context. Suppose that a user runs the following commands:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma context <span class=nb>set</span> <span class=c1># Choose project my-project</span>
</span></span><span class=line><span class=cl>llma auth api-keys create my-key
</span></span></code></pre></div><p>The newly created API key is associated with project <code>my-project</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-52f3e6a6d26d58b3ca63890586812ed7>4 - Integration</h1><div class=lead>Integrate with other projects</div></div><div class=td-content><h1 id=pg-3262aec6d92a36c90b5ff177e164086a>4.1 - Open WebUI</h1><div class=lead>Integrate with Open WebUI and get the web UI for the AI assistant.</div><p><a href=https://docs.openwebui.com/>Open WebUI</a> provides a web UI that works with OpenAI-compatible APIs. You can run Openn WebUI locally or run in a Kubernetes cluster.</p><p>Here is an instruction for running Open WebUI in a Kubernetes cluster.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>OPENAI_API_KEY</span><span class=o>=</span>&lt;LLMariner API key&gt;
</span></span><span class=line><span class=cl><span class=nv>OPEN_API_BASE_URL</span><span class=o>=</span>&lt;LLMariner API endpoint&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl create namespace open-webui
</span></span><span class=line><span class=cl>kubectl create secret generic -n open-webui llmariner-api-key --from-literal<span class=o>=</span><span class=nv>key</span><span class=o>=</span><span class=si>${</span><span class=nv>OPENAI_API_KEY</span><span class=si>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl apply -f - <span class=s>&lt;&lt;EOF
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: apps/v1
</span></span></span><span class=line><span class=cl><span class=s>kind: Deployment
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: open-webui
</span></span></span><span class=line><span class=cl><span class=s>  namespace: open-webui
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  selector:
</span></span></span><span class=line><span class=cl><span class=s>    matchLabels:
</span></span></span><span class=line><span class=cl><span class=s>      name: open-webui
</span></span></span><span class=line><span class=cl><span class=s>  template:
</span></span></span><span class=line><span class=cl><span class=s>    metadata:
</span></span></span><span class=line><span class=cl><span class=s>      labels:
</span></span></span><span class=line><span class=cl><span class=s>        name: open-webui
</span></span></span><span class=line><span class=cl><span class=s>    spec:
</span></span></span><span class=line><span class=cl><span class=s>      containers:
</span></span></span><span class=line><span class=cl><span class=s>      - name: open-webui
</span></span></span><span class=line><span class=cl><span class=s>        image: ghcr.io/open-webui/open-webui:main
</span></span></span><span class=line><span class=cl><span class=s>        ports:
</span></span></span><span class=line><span class=cl><span class=s>        - name: http
</span></span></span><span class=line><span class=cl><span class=s>          containerPort: 8080
</span></span></span><span class=line><span class=cl><span class=s>          protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>        env:
</span></span></span><span class=line><span class=cl><span class=s>        - name: OPENAI_API_BASE_URLS
</span></span></span><span class=line><span class=cl><span class=s>          value: ${OPEN_API_BASE_URL}
</span></span></span><span class=line><span class=cl><span class=s>        - name: WEBUI_AUTH
</span></span></span><span class=line><span class=cl><span class=s>          value: &#34;false&#34;
</span></span></span><span class=line><span class=cl><span class=s>        - name: OPENAI_API_KEYS
</span></span></span><span class=line><span class=cl><span class=s>          valueFrom:
</span></span></span><span class=line><span class=cl><span class=s>            secretKeyRef:
</span></span></span><span class=line><span class=cl><span class=s>              name: llmariner-api-key
</span></span></span><span class=line><span class=cl><span class=s>              key: key
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: v1
</span></span></span><span class=line><span class=cl><span class=s>kind: Service
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: open-webui
</span></span></span><span class=line><span class=cl><span class=s>  namespace: open-webui
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  type: ClusterIP
</span></span></span><span class=line><span class=cl><span class=s>  selector:
</span></span></span><span class=line><span class=cl><span class=s>    name: open-webui
</span></span></span><span class=line><span class=cl><span class=s>  ports:
</span></span></span><span class=line><span class=cl><span class=s>  - port: 8080
</span></span></span><span class=line><span class=cl><span class=s>    name: http
</span></span></span><span class=line><span class=cl><span class=s>    targetPort: http
</span></span></span><span class=line><span class=cl><span class=s>    protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span></code></pre></div><p>You can then access Open WebUI with port forwarding:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl port-forward -n open-webui service/open-webui <span class=m>8080</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-acd15456d1ec0db7c8fc89d3e94c2d27>4.2 - Continue</h1><div class=lead>Integrate with Continue and provide an open source AI code assistant.</div><p><a href=https://www.continue.dev/>Continue</a> provides an open source AI code assistant. You can use LLMariner as a backend endpoint for Continue.</p><p>As LLMariner provides the OpenAI compatible API, you can set the <code>provider</code> to <code>"openai"</code>. <code>apiKey</code> is set to an API key generated by LLMariner, and <code>apiBase</code> is set to the endpoint URL of LLMariner (e.g., <a href=http://localhost:8080/v1>http://localhost:8080/v1</a>).</p><p>Here is an example configuration that you can put at <code>~/.continue/config.json</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;models&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;Meta-Llama-3.1-8B-Instruct-q4&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;provider&#34;</span><span class=p>:</span> <span class=s2>&#34;openai&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;model&#34;</span><span class=p>:</span> <span class=s2>&#34;meta-llama-Meta-Llama-3.1-8B-Instruct-q4&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;apiKey&#34;</span><span class=p>:</span> <span class=s2>&#34;&lt;LLMariner API key&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;apiBase&#34;</span><span class=p>:</span> <span class=s2>&#34;&lt;LLMariner endpoint&gt;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;tabAutocompleteModel&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;Auto complete&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;provider&#34;</span><span class=p>:</span> <span class=s2>&#34;openai&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;model&#34;</span><span class=p>:</span> <span class=s2>&#34;deepseek-ai-deepseek-coder-6.7b-base-q4&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;apiKey&#34;</span><span class=p>:</span> <span class=s2>&#34;&lt;LLMariner API key&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;apiBase&#34;</span><span class=p>:</span> <span class=s2>&#34;&lt;LLMariner endpoint&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;completionOptions&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;presencePenalty&#34;</span><span class=p>:</span> <span class=mf>1.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;frequencyPenalty&#34;</span><span class=p>:</span> <span class=mf>1.1</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;allowAnonymousTelemetry&#34;</span><span class=p>:</span> <span class=kc>false</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The following is a demo video that shows the Continue integration that enables the coding assistant with Llama-3.1-Nemotron-70B-Instruct.</p><div style="padding:53.84% 0 0;position:relative"><iframe src="https://player.vimeo.com/video/1024806457?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder=0 allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style=position:absolute;top:0;left:0;width:100%;height:100% title="LLMariner demo with Continue"></iframe></div><script src=https://player.vimeo.com/api/player.js></script></div><div class=td-content style=page-break-before:always><h1 id=pg-9da4b3679d05a1e4de898c134966c56e>4.3 - Aider</h1><div class=lead>Integrate with Aider for AI pair programming</div><p><a href=https://aider.chat/>Aider</a> is AI pair programming in your terminal or browser.</p><p>Aider supports the OpenAI compatible API, and you can configure the endpoint
and the API key with environment variables.</p><p>Here is an example installation and configuration procedure.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python -m pip install -U aider-chat
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>OPENAI_API_BASE</span><span class=o>=</span>&lt;Base URL <span class=o>(</span>e.g., http://localhost:8080/v1<span class=o>)</span>&gt;
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>OPENAI_API_KEY</span><span class=o>=</span>&lt;API key&gt;
</span></span></code></pre></div><p>You can then run Aider in your terminal or browser. Here is an example command
that launches Aider in your browser with Llama 3.1 70B.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>&lt;Move to your github repo directory&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aider --model openai/meta-llama-Meta-Llama-3.1-70B-Instruct-awq --browser
</span></span></code></pre></div><p>Please note that the model name requires the <code>openai/</code> prefix.</p><p><a href=https://aider.chat/examples/README.html>https://aider.chat/examples/README.html</a> has example chat transcripts
for building applications (e.g., &ldquo;make a flask app with a /hello endpoint that returns hello world&rdquo;).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-923663fb245c62d0612c79e0f6b52ba5>4.4 - AI Shell</h1><div class=lead>Integrate with AI Shell to power your shell with the AI assistant.</div><p><a href=https://github.com/BuilderIO/ai-shell>AI Shell</a> is an open source tool that converts natural language to shell commands.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>npm install -g @builder.io/ai-shell
</span></span><span class=line><span class=cl>ai config <span class=nb>set</span> <span class=nv>OPENAI_API_ENDPOINT</span><span class=o>=</span>&lt;Base URL <span class=o>(</span>e.g., http://localhost:8080/v1<span class=o>)</span>&gt;
</span></span><span class=line><span class=cl>ai config <span class=nb>set</span> <span class=nv>OPENAI_KEY</span><span class=o>=</span>&lt;API key&gt;
</span></span><span class=line><span class=cl>ai config <span class=nb>set</span> <span class=nv>MODEL</span><span class=o>=</span>&lt;model name&gt;
</span></span></code></pre></div><p>Then you can run the <code>ai</code> command and ask what you want in plain English and generate a shell command with a human readable explanation of it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ai what is my ip address
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-aa09c8195c9d19a9308971cb82ac7e53>4.5 - k8sgpt</h1><div class=lead>Integrate with k8sgpt to diagnose and triage issues in your Kubernetes clusters.</div><p><a href=https://github.com/k8sgpt-ai/k8sgpt>k8sgpt</a> is a tool for scanning your
Kubernetes clusters, diagnosing, and triaging issues in simple English.</p><p>You can use LLMariner as a backend of k8sgpt by running the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>k8sgpt auth add <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --backend openai <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --baseurl &lt;LLMariner base URL <span class=o>(</span>e.g., http://localhost:8080/v1/<span class=o>)</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --password &lt;LLMariner API Key&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --model &lt;Model ID&gt;
</span></span></code></pre></div><p>Then you can a command like <code>k8sgpt analyze</code> to inspect your Kubernetes cluster.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>k8sgpt analyze --explain
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-55c6586ef1140acfb8e48058350d103a>4.6 - Dify</h1><div class=lead>Integrate with Dify for LLM application development.</div><p><a href=https://dify.ai/>Dify</a> is is an open-source LLM app development platform.
It can orchestrate LLM apps from agents to complex AI workflows, with an RAG engine.</p><p>You can add LLMariner as one of Dify&rsquo;s model providers with the following steps:</p><ol><li>Click the user profile icon.</li><li>Click &ldquo;Settings&rdquo;</li><li>Click &ldquo;Model Provider&rdquo;</li><li>Search &ldquo;OpenAI-API-compatible&rdquo; and click &ldquo;Add model&rdquo;</li><li>Configure a model name, API key,a nd API endpoint URL.</li></ol><p class="mt-4 mb-4 text-center"><img src=/images/dify.png width=2242 height=1774></p><p>You can then use the registered model from your LLM applications. For example, you
can create a new application by &ldquo;Create from Template&rdquo; and replace the use of an OpenAI
model with the configured model.</p><p>If you want to deploy Dify in your Kubernetes clusters, follow
<a href="https://github.com/langgenius/dify/tree/716576043de3bdaab5aeac28a83d31bb054f8ec4?tab=readme-ov-file#advanced-setup">README.md</a>
in the Dify GitHub repository.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-66e3bf8dad85a9252eb8b996c935ef64>4.7 - n8n</h1><div class=lead>Integrate with n8n and get the web UI for the AI assistant.</div><p><a href=https://n8n.io/>n8n</a> is a no-code platform for workload automation. You can deploy
n8n to your Kubernetes clusters. Here is an example command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl apply -f - <span class=s>&lt;&lt;EOF
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: apps/v1
</span></span></span><span class=line><span class=cl><span class=s>kind: Deployment
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: n8n
</span></span></span><span class=line><span class=cl><span class=s>  namespace: n8n
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  selector:
</span></span></span><span class=line><span class=cl><span class=s>    matchLabels:
</span></span></span><span class=line><span class=cl><span class=s>      name: n8n
</span></span></span><span class=line><span class=cl><span class=s>  template:
</span></span></span><span class=line><span class=cl><span class=s>    metadata:
</span></span></span><span class=line><span class=cl><span class=s>      labels:
</span></span></span><span class=line><span class=cl><span class=s>        name: n8n
</span></span></span><span class=line><span class=cl><span class=s>    spec:
</span></span></span><span class=line><span class=cl><span class=s>      containers:
</span></span></span><span class=line><span class=cl><span class=s>      - name: n8n
</span></span></span><span class=line><span class=cl><span class=s>        image: docker.n8n.io/n8nio/n8n
</span></span></span><span class=line><span class=cl><span class=s>        ports:
</span></span></span><span class=line><span class=cl><span class=s>        - name: http
</span></span></span><span class=line><span class=cl><span class=s>          containerPort: 8080
</span></span></span><span class=line><span class=cl><span class=s>          protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: v1
</span></span></span><span class=line><span class=cl><span class=s>kind: Service
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: n8n
</span></span></span><span class=line><span class=cl><span class=s>  namespace: n8n
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  type: ClusterIP
</span></span></span><span class=line><span class=cl><span class=s>  selector:
</span></span></span><span class=line><span class=cl><span class=s>    name: n8n
</span></span></span><span class=line><span class=cl><span class=s>  ports:
</span></span></span><span class=line><span class=cl><span class=s>  - port: 5678
</span></span></span><span class=line><span class=cl><span class=s>    name: http
</span></span></span><span class=line><span class=cl><span class=s>    targetPort: http
</span></span></span><span class=line><span class=cl><span class=s>    protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span></code></pre></div><p>You can then access n8n with port forwarding:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl port-forward -n n8n service/n8n <span class=m>5678</span>
</span></span></code></pre></div><p>You can create an &ldquo;OpenAI Model&rdquo; node and configure its base URL and credential to hit LLMariner.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-382415db5d1ad482de2bd54181991589>4.8 - Slackbot</h1><div class=lead>Build a Slackbot that integrates with LLMariner</div><p>You can build a Slackbot that is integrated with LLMariner. The bot can provide a chat UI with Slack
and answer questions from end users.</p><p>An example implementation can be found in <a href=https://github.com/llmariner/slackbot>https://github.com/llmariner/slackbot</a>. You can deploy it in your
Kubernetes clusters and build a Slack app with the following configuration:</p><ul><li>Create an app-level token whose scope is <code>connections:write</code>.</li><li>Enable the socket mode. Enable event subscription with the <code>app_mentions:read</code> scope.</li><li>Add the following scopes in &ldquo;OAuth & Permissions&rdquo;: <code>app_mentions:read</code>, <code>chat:write</code>, <code>chat:write.customize</code>, and <code>links:write</code></li></ul><p>You can install the Slack application to your workspace and interact.</p><p class="mt-4 mb-4 text-center"><img src=/images/slackbot.png width=1442 height=808></p></div><div class=td-content style=page-break-before:always><h1 id=pg-e6225eb8b68621a923dbfdca6dd573ca>4.9 - MLflow</h1><div class=lead>Integrate with MLflow.</div><p><a href=https://mlflow.org/>MLflow</a> is an open-source tool for managing the machine learning lifecycle. It has various features for LLMs (<a href=https://mlflow.org/docs/latest/llms/index.html>link</a>) and integration with OpenAI. We can apply these MLflow features to the LLM endpoints provided by LLMariner.</p><p>For example, you can deploy a <a href=https://mlflow.org/docs/latest/llms/index.html#id1>MLflow Deployments Server for LLMs</a> and use <a href=https://mlflow.org/docs/latest/llms/index.html#id3>Prompt Engineering UI</a>.</p><h2 id=deploying-mlflow-tracking-server>Deploying MLflow Tracking Server<a class=td-heading-self-link href=#deploying-mlflow-tracking-server aria-label="Heading self-link"></a></h2><p>Bitmani provides a <a href=https://github.com/bitnami/charts/tree/main/bitnami/mlflow>Helm chart</a> for MLflow.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm upgrade <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --install <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -n mlflow <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  mlflow oci://registry-1.docker.io/bitnamicharts/mlflow <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f values.yaml
</span></span></code></pre></div><p>An example <code>values.yaml</code> is following:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>tracking</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>extraEnvVars</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>MLFLOW_DEPLOYMENTS_TARGET</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>http://deployment-server:7000</span><span class=w>
</span></span></span></code></pre></div><p>We set <code>MLFLOW_DEPLOYMENTS_TARGET</code> to the address of a MLflow Deployments Server that we will deploy in the next section.</p><p>Once deployed, you can set up port-forwarding and access <a href=http://localhost:9000>http://localhost:9000</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl port-forward -n mlflow service/mlflow-tracking 9000:80
</span></span></code></pre></div><p>The login credentials are obtained by the following commands:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># User</span>
</span></span><span class=line><span class=cl>kubectl get secret --namespace mlflow mlflow-tracking -o <span class=nv>jsonpath</span><span class=o>=</span><span class=s2>&#34;{ .data.admin-user }&#34;</span> <span class=p>|</span> base64 -d
</span></span><span class=line><span class=cl><span class=c1># Password</span>
</span></span><span class=line><span class=cl>kubectl get secret --namespace mlflow mlflow-tracking -o <span class=nv>jsonpath</span><span class=o>=</span><span class=s2>&#34;{.data.admin-password }&#34;</span> <span class=p>|</span> base64 -d
</span></span></code></pre></div><h2 id=deploying-mlflow-deployments-server-for-llms>Deploying MLflow Deployments Server for LLMs<a class=td-heading-self-link href=#deploying-mlflow-deployments-server-for-llms aria-label="Heading self-link"></a></h2><p>We have an example K8s YAML for deploying a MLflow deployments server <a href=https://raw.githubusercontent.com/llmariner/llmariner/main/hack/mlflow/deployment-server.yaml>here</a>.</p><p>You can save it locally, up <code>openai_api_base</code> in the <code>ConfigMap</code> definition based on your ingress controller address, and then run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl create secret generic -n mlflow llmariner-api-key <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-literal<span class=o>=</span><span class=nv>secret</span><span class=o>=</span>&lt;Your API key&gt;
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl apply -n mlflow -f deployment-server.yaml
</span></span></code></pre></div><p>You can then access the MLflow Tracking Server, click "New run", and choose "using Prompt Engineering".</p><p class="mt-4 mb-4 text-center"><img src=/images/mlflow.png width=2914 height=1832></p><h2 id=other-features>Other Features<a class=td-heading-self-link href=#other-features aria-label="Heading self-link"></a></h2><p>Please visit <a href=https://mlflow.org/docs/latest/llms/>MLflow page for more information</a> for other LLM related features provided by MLflow.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-484e4e9dc5d71bd6e9a10c85edd5bc6e>4.10 - Langfuse</h1><div class=lead>Integrate with Langfuse for LLM engineering.</div><p><a href=https://github.com/langfuse/langfuse>Langfuse</a> is an open source LLM engineering platform. You can integrate Langfuse
with LLMariner as Langfuse provides an SDK for the OpenAI API.</p><p>Here is an example procedure for running Langfuse locally:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/langfuse/langfuse.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> langfuse
</span></span><span class=line><span class=cl>docker compose up -d
</span></span></code></pre></div><p>You can sign up and create your account. Then you can generate API keys
and put them in environmental variables.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LANGFUSE_SECRET_KEY</span><span class=o>=</span>...
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LANGFUSE_PUBLIC_KEY</span><span class=o>=</span>...
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LANGFUSE_HOST</span><span class=o>=</span><span class=s2>&#34;http://localhost:3000&#34;</span>
</span></span></code></pre></div><p>You can then use <code>langfuse.openai</code> instead of <code>openai</code> in your Python scripts
to record traces in Langfuse.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langfuse.openai</span> <span class=kn>import</span> <span class=n>openai</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>openai</span><span class=o>.</span><span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;&lt;Base URL (e.g., http://localhost:8080/v1)&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;&lt;API key secret&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it-q4_0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;What is k8s?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>response</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Here is an example screenshot.</p><p class="mt-4 mb-4 text-center"><img src=/images/langfuse.png width=2862 height=1864></p></div><div class=td-content style=page-break-before:always><h1 id=pg-5a61ba5d4376fbae7610a65a0d32db3b>4.11 - Weights & Biases (W&amp;B)</h1><div class=lead>Integration with W&amp;B and see the progress of your fine-tuning jobs.</div><p><a href=https://wandb.ai/>Weights and Biases (W&amp;B)</a> is an AI developer platform. LLMariner provides the integration with W&amp;B so that metrics for fine-tuning jobs are reported to W&amp;B. With the integration, you can easily see the progress of your fine-tuning jobs, such as training epoch, loss, etc.</p><p>Please take the following steps to enable the integration.</p><p>First, obtain the API key of W&amp;B and create a Kubernetes secret.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl create secret generic wandb
</span></span><span class=line><span class=cl>  -n &lt;fine-tuning job namespace&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-literal<span class=o>=</span><span class=nv>apiKey</span><span class=o>=</span><span class=si>${</span><span class=nv>WANDB_API_KEY</span><span class=si>}</span>
</span></span></code></pre></div><p>The secret needs to be created in a namespace where fine-tuning jobs run. Individual projects specify namespaces for fine-tuning jobs, and the default project runs fine-tuning jobs in the "default" namespace.</p><p>Then you can enable the integration by adding the following to your Helm <code>values.yaml</code> and re-deploying LLMariner.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>job-manager-dispatcher</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>job</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>wandbApiKeySecret</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>wandb</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>key</span><span class=p>:</span><span class=w> </span><span class=l>apiKey</span><span class=w>
</span></span></span></code></pre></div><p>A fine-tuning job will report to W&amp;B when the <code>integration</code> parameter is specified.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>job</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>fine_tuning</span><span class=o>.</span><span class=n>jobs</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>suffix</span><span class=o>=</span><span class=s2>&#34;fine-tuning&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>training_file</span><span class=o>=</span><span class=n>tfile</span><span class=o>.</span><span class=n>id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>validation_file</span><span class=o>=</span><span class=n>vfile</span><span class=o>.</span><span class=n>id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>integrations</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;wandb&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=s2>&#34;wandb&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>         <span class=s2>&#34;project&#34;</span><span class=p>:</span> <span class=s2>&#34;my-test-project&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Here is an example screenshot. You can see metrics like <code>train/loss</code> in the W&amp;B dashboard.</p><p class="mt-4 mb-4 text-center"><img src=/images/wandb.png width=3362 height=2162></p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>The W&amp;B API key is attached to fine-tuning jobs as an environment variable. Please be careful of its visibility.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-cde904b13b4c787d29f40d57853861f5>5 - Development</h1><div class=lead>Documents related to development</div></div><div class=td-content><h1 id=pg-c174fff464205008bddc116b984cc877>5.1 - Technical Details</h1><div class=lead>Understand LLMariner technical details.</div><h2 id=components>Components<a class=td-heading-self-link href=#components aria-label="Heading self-link"></a></h2><p>LLMariner provisions the LLM stack consisting of the following micro services:</p><ul><li>Inference Manager</li><li>Job Manager</li><li>Model Manager</li><li>File Manager</li><li>Vector Store Server</li><li>User Manager</li><li>Cluster Manager</li><li>Session Manager</li><li>RBAC Manager</li><li>API Usage</li></ul><p>Each manager is responsible for the specific feature of LLM services as their names indicate. The following diagram shows the high-level architecture:</p><p class="mt-4 mb-4 text-center"><img src=/images/architecture_diagram.png width=3166 height=2188></p><p>LLMariner has dependency to the following components:</p><ul><li>Ingress controller</li><li>SQL database</li><li>S3-compatible object store</li><li><a href=https://github.com/dexidp/dex>Dex</a></li><li><a href=https://milvus.io/>Milvus</a></li></ul><p>Ingress controller is required to route traffic to each service. SQL database and S3-compatible object store are used to persist metadata (e.g., fine-tuning jobs), fine-tuned models, and training/validation files. Dex is used to provide authentication.</p><h2 id=key-technologies>Key Technologies<a class=td-heading-self-link href=#key-technologies aria-label="Heading self-link"></a></h2><h3 id=autoscaling-and-dynamic-model-loading-in-inference>Autoscaling and Dynamic Model Loading in Inference<a class=td-heading-self-link href=#autoscaling-and-dynamic-model-loading-in-inference aria-label="Heading self-link"></a></h3><p>Inference Manager dynamically loads models up on requests it receives. It also dynamically auto-scales pods based on demand.</p><h3 id=session-manager-secure-access-to-kubernetes-api-server>Session Manager: Secure Access to Kubernetes API Server<a class=td-heading-self-link href=#session-manager-secure-access-to-kubernetes-api-server aria-label="Heading self-link"></a></h3><p>LLMariner internally accesses Kubernetes API server to allow end users to access logs of fine-tuning jobs, exec into a Jupyter Notebook, etc. As end users might not have direct access to a Kubernetes API server, LLMariner uses Session Manager to provide a secure tunnel between end users and Kubernetes API server.</p><p>Session Manager consists of two components: <code>server</code> and <code>agent</code>. The <code>agent</code> establishes HTTP(S) connections to the <code>server</code> and keeps the connections. Upon receiving a request from end users, the <code>server</code> forwards the request to the <code>agent</code> using one of the established connections. Then the <code>agent</code> forwards the request to the Kubernetes API server.</p><p>This architecture enables the deployment where the <code>server</code> and the <code>agent</code> can run in separate Kubernetes clusters. As the <code>agent</code> initiates a connection (not the <code>server</code>), there is no need to open incoming traffic at the cluster where the <code>agent</code> runs. An ingress controller is still the only place where incoming traffic is sent.</p><h3 id=quota-management-for-fine-tuning-jobs>Quota Management for Fine-tuning Jobs<a class=td-heading-self-link href=#quota-management-for-fine-tuning-jobs aria-label="Heading self-link"></a></h3><p>LLMariner allows users to manage GPU quotas with integration with <a href=https://github.com/kubernetes-sigs/kueue>Kueue</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-998d0585ca29638d7ea6bebdd5056382>5.2 - Roadmap</h1><div class=lead>Future plans</div><h2 id=milestone-0-completed>Milestone 0 (Completed)<a class=td-heading-self-link href=#milestone-0-completed aria-label="Heading self-link"></a></h2><ul><li><input checked disabled type=checkbox> OpenAI compatible API</li><li><input checked disabled type=checkbox> Models: <code>google-gemma-2b-it</code></li></ul><h2 id=milestone-1-completed>Milestone 1 (Completed)<a class=td-heading-self-link href=#milestone-1-completed aria-label="Heading self-link"></a></h2><ul><li><input checked disabled type=checkbox> API authorization with Dex</li><li><input checked disabled type=checkbox> API key management</li><li><input checked disabled type=checkbox> Quota management for fine-tuning jobs</li><li><input checked disabled type=checkbox> Inference autoscaling with GPU utilization</li><li><input checked disabled type=checkbox> Models: <code>Mistral-7B-Instruct</code>, <code>Meta-Llama-3-8B-Instruct</code>, and <code>google-gemma-7b-it</code></li></ul><h2 id=milestone-2-completed>Milestone 2 (Completed)<a class=td-heading-self-link href=#milestone-2-completed aria-label="Heading self-link"></a></h2><ul><li><input checked disabled type=checkbox> Jupyter Notebook workspace creation</li><li><input checked disabled type=checkbox> Dynamic model loading & offloading in inference (initial version)</li><li><input checked disabled type=checkbox> Organization & project management</li><li><input checked disabled type=checkbox> MLflow integration</li><li><input checked disabled type=checkbox> Weights & Biases integration for fine-tuning jobs</li><li><input checked disabled type=checkbox> VectorDB installation and RAG</li><li><input checked disabled type=checkbox> Multi k8s cluster deployment (initial version)</li></ul><h2 id=milestone-3-completed>Milestone 3 (Completed)<a class=td-heading-self-link href=#milestone-3-completed aria-label="Heading self-link"></a></h2><ul><li><input checked disabled type=checkbox> Object store other than MinIO</li><li><input checked disabled type=checkbox> Multi-GPU general-purpose training jobs</li><li><input checked disabled type=checkbox> Inference optimization (e.g., vLLM)</li><li><input checked disabled type=checkbox> Models: <code>Meta-Llama-3-8B-Instruct</code>, <code>Meta-Llama-3-70B-Instruct</code>, <code>deepseek-coder-6.7b-base</code></li></ul><h2 id=milestone-4-completed>Milestone 4 (Completed)<a class=td-heading-self-link href=#milestone-4-completed aria-label="Heading self-link"></a></h2><ul><li><input checked disabled type=checkbox> Embedding API</li><li><input checked disabled type=checkbox> API usage visibility</li><li><input checked disabled type=checkbox> Fine-tuning support with vLLM</li><li><input checked disabled type=checkbox> API key encryption</li><li><input checked disabled type=checkbox> Nvidia Triton Inference Server (experimental)</li><li><input checked disabled type=checkbox> Release flow</li></ul><h2 id=milestone-5-in-progress>Milestone 5 (In-progress)<a class=td-heading-self-link href=#milestone-5-in-progress aria-label="Heading self-link"></a></h2><ul><li><input disabled type=checkbox> Frontend</li><li><input disabled type=checkbox> GPU showback</li><li><input disabled type=checkbox> Non-Nvidia GPU support</li><li><input disabled type=checkbox> Multi k8s cluster deployment (file and vector store management)</li><li><input disabled type=checkbox> High availability</li><li><input disabled type=checkbox> Monitoring & alerting</li><li><input disabled type=checkbox> More models</li></ul><h2 id=milestone-6>Milestone 6<a class=td-heading-self-link href=#milestone-6 aria-label="Heading self-link"></a></h2><ul><li><input disabled type=checkbox> Multi-GPU LLM fine-tuning jobs</li><li><input disabled type=checkbox> Events and metrics for fine-tuning jobs</li></ul></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/llmariner/llmariner aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://join.slack.com/t/llmariner/shared_invite/zt-2rbwooslc-LIrUCmK9kklfKsMEirUZbg aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2025
<span class=td-footer__authors>CloudNatix, Inc.</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.f72d00502781aaf278a14088eb2356b1ba2a05ad698a0c43fe86314d74ceab56.js integrity="sha256-9y0AUCeBqvJ4oUCI6yNWsboqBa1pigxD/oYxTXTOq1Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>