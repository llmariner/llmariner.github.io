<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Advanced Topics on LLMariner</title><link>https://llmariner.ai/docs/advanced/</link><description>Recent content in Advanced Topics on LLMariner</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmariner.ai/docs/advanced/index.xml" rel="self" type="application/rss+xml"/><item><title>Technical Architecture</title><link>https://llmariner.ai/docs/advanced/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/advanced/architecture/</guid><description>&lt;h2 id="components">Components&lt;a class="td-heading-self-link" href="#components" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLMariner provisions the LLM stack consisting of the following micro services:&lt;/p>
&lt;ul>
&lt;li>Inference Manager&lt;/li>
&lt;li>Job Manager&lt;/li>
&lt;li>Model Manager&lt;/li>
&lt;li>File Manager&lt;/li>
&lt;li>Vector Store Server&lt;/li>
&lt;li>User Manager&lt;/li>
&lt;li>Cluster Manager&lt;/li>
&lt;li>Session Manager&lt;/li>
&lt;li>RBAC Manager&lt;/li>
&lt;li>API Usage&lt;/li>
&lt;/ul>
&lt;p>Each manager is responsible for the specific feature of LLM services as their names indicate. The following diagram shows the high-level architecture:&lt;/p>
&lt;!-- original file is located at ./architecture.excalidraw -->


&lt;p>&lt;img src="https://llmariner.ai/images/architecture_diagram.png" width="3165" height="2288">&lt;/p>


&lt;p>LLMariner has dependency to the following components:&lt;/p></description></item><item><title>Multi-Cluster and Multi-Cloud Deployment</title><link>https://llmariner.ai/docs/advanced/multi_cluster_deployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/advanced/multi_cluster_deployment/</guid><description>&lt;p>LLMariner deploys Kubernetes deployments to provision the LLM stack. In a typical configuration, all the services are deployed into a single Kubernetes cluster, but you can also deploy these services on multiple Kubernetes clusters. For example, you can deploy a control plane component in a CPU K8s cluster and deploy the rest of the components in GPU compute clusters.&lt;/p>
&lt;p>LLMariner can be deployed into multiple GPU clusters, and the clusters can span across multiple cloud providers (including GPU specific clouds like CoreWeave) and on-prem.&lt;/p></description></item><item><title>Hosted Control Plane</title><link>https://llmariner.ai/docs/advanced/hosted_control_plane/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/advanced/hosted_control_plane/</guid><description>&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Work-in-progress. This is not fully ready yet, and the terms and conditions are subject to change as we might limit the usage based on the number of API calls or the number of GPU nodes.

&lt;/div>

&lt;p>CloudNatix provides a hosted control plane of LLMariner. End users can use the full functionality of LLMariner just by registering their worker GPU clusters to this hosted control plane.&lt;/p>
&lt;h2 id="step-1-create-a-cloudnatix-account">Step 1. Create a CloudNatix account&lt;a class="td-heading-self-link" href="#step-1-create-a-cloudnatix-account" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Create a CloudNatix account if you haven't. Please visit &lt;a href="https://app.cloudnatix.com">https://app.cloudnatix.com&lt;/a>. You can click one of the &amp;quot;Sign in or sing up&amp;quot; buttons for SSO login or you can click &amp;quot;Sign up&amp;quot; at the bottom for the email &amp;amp; password login.&lt;/p></description></item></channel></rss>