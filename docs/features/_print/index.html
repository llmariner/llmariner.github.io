<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://llmariner.ai/docs/features/><link rel=alternate type=application/rss+xml href=https://llmariner.ai/docs/features/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Features | LLMariner</title>
<meta name=description content="LLMariner features"><meta property="og:url" content="https://llmariner.ai/docs/features/"><meta property="og:site_name" content="LLMariner"><meta property="og:title" content="Features"><meta property="og:description" content="LLMariner features"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Features"><meta itemprop=description content="LLMariner features"><meta itemprop=dateModified content="2024-10-25T23:16:09+09:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Features"><meta name=twitter:description content="LLMariner features"><link rel=preload href=/scss/main.min.f5f1bfbe5d5d7a6f0bb1b9d8ae3a3aee20f3a7a4d3e278a18d40b0d46e2b0ddb.css as=style><link href=/scss/main.min.f5f1bfbe5d5d7a6f0bb1b9d8ae3a3aee20f3a7a4d3e278a18d40b0d46e2b0ddb.css rel=stylesheet><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-JBN3K7Y529"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JBN3K7Y529")}</script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg id="Layer_2" viewBox="0 0 426 426"><defs><style>.cls-1{fill:none}.cls-2{fill:#2491eb}.cls-3{fill:#fff}.cls-4{fill:#102c3f}</style></defs><g id="Layer_4"><rect class="cls-1" width="426" height="426"/></g><g id="Layer_1-2" data-name="Layer_1"><g><path class="cls-3" d="M297.78 142.6c-.58-.78-.73-1.54-.75-2.49.1-6.62-.3-28.22.19-34.77.36-1.5 3.4-3.12 5.17-4.29 23.46-14.42 32.1-37.79 13.17-57.48-18.33-21.77-74.32-35.96-118.14-32.11-43.09 1.55-97.3 22.81-99.25 52.59-1.55 18.53 13.46 31.7 28.95 40.28 2.39 1.31 5.2 2.63 4.84 5.73.0 5.84.02 20.42-.02 25.02-.07 1.61-.28 2.3-1.29 3.29-15.69 14.17-34.21 39.77-40.92 64.65-26.6 91.07 58.97 179.78 151.47 157.86 101.16-23.37 134.6-150.45 56.63-218.18l-.07-.09z"/><path class="cls-2" d="M295.05 136.36c-6.98 7.43-15.99 13.84-26.47 18.86 25.74 17.44 42.65 46.92 42.65 80.36.0 53.57-43.43 97-97 97s-97-43.43-97-97c0-33.35 16.83-62.76 42.46-80.22-11.37-5.39-21.01-12.43-28.24-20.63-29.98 23.81-49.21 60.58-49.21 101.85.0 71.8 58.2 130 130 130s130-58.2 130-130c0-40.33-18.36-76.37-47.19-100.21z"/><path class="cls-4" d="M328.22 322.29c2.26 9.28 22.08 15.7 27.56 26.74 11.43 17.87-8.84 37.22-26.58 25.78-9.21-5.77-12.56-16.75-18.27-25.5-3.07-5.09-5.42-6.97-9.88-2.77-19.67 16.58-42.54 24.87-68.13 27.94-1.54.19-3.63.48-4.53 1.54-.85.88-.95 2.29-.64 3.96.56 2.75 1.39 5.54 2.13 8.24 1.08 3.89 2.14 7.73 2.63 11.69 1.59 10.12-3.55 22.67-14.13 25.25-11.45 2.91-22.27-4.78-24.06-16.58-3.76-15.85 17.86-32.59.2-34.5-25.37-3.81-49.39-12.98-69.68-28.64-1.18-.89-2.75-2.08-4.07-2.05-1.47-.08-2.74 1.36-3.58 2.59-7.88 11-15.62 26.79-28.27 32.28-13.22 4.53-26.6-10.3-22.79-23.2 1.49-4.91 4.92-9.41 8.61-12.86 7.37-6.76 15.69-8.23 23.69-15.72 2.93-2.48 2.89-5.82.44-8.61-12.26-16.48-20.13-36.23-23.49-56.51-.51-2.23-.77-5.47-2.6-6.59-3.89-2.56-13.17 3.97-23.23 5.6-10.93 2.71-26.13-7.21-26.02-17.93-.5-13.11 16.04-25.99 31.83-20.82 3.52.69 15.32 7.66 17.66 4.91.88-.89 1-2.9 1.13-4.31 1.68-22.16 9.69-40.84 22.64-57.67 2.95-3.16 2.15-6.87-.94-9.62-14.13-13.06-31.37-10.71-32.06-31.83.23-11.9 13.31-21.57 24.72-18.3 9.61 2.95 19.88 19.48 26.82 27.24 4.28 5.52 6.39 1.56 6.47-3.47.11-2.18.0-4.38-.03-6.56-.18-5.29.97-8.91-2.32-12.32-4.35-4.56-14.08-6.82-21.3-16.79-11.66-16.14-9.9-34.68 3.64-49.08 59.69-57.99 166.35-58.78 223.98 2.58 13.71 17.62 9.88 44.2-10.26 56.44-4.38 2.85-10.32 5.22-11.59 10.72-.57 2.66-2.2 15.99-.5 18.44 1.58 2.49 5.29 2.97 7.15.49 5.61-7.86 9.47-16.84 16.24-23.76 9.24-9.24 24.45-5.21 30.22 5.39 4.16 6.74 1.47 16.87-4.27 22.63-6.46 6.77-16.19 6.77-20.56 13.87-2.22 2.98-2.68 6.78-.19 10.38 6.9 11.29 12.76 21.3 16.83 33.96 6.86 19.21 1.69 38.69 16.61 30.76 3.53-1.54 7.43-3.03 11.22-3.8 12.14-3.09 24.91 2.93 28.49 15.03 1.76 6.25-1.28 12.83-6.08 16.92-8.93 8.27-22.48 7.22-33.02 1.67-9.96-4.57-11.47-1.12-12.86 8.35-2.66 19.7-13.47 36.69-23.43 53.55-.84 1.42-1.67 3.13-1.57 4.69v.16zM255.37 172.65c-4.73-1.2-9.2 1.26-13.84 1.99-5.54 1.28-11.12 2.49-16.77 3.25-15.09 2.06-30.02.48-44.74-3.24-3.57-1-7.36-1.42-10.69.49-27.47 16.28-42.82 49.15-34.71 80.26 2.06 8.06 5.88 15.95 10.9 22.79 74.63 104.21 220.91-27.66 110.04-105.47l-.2-.06zm-65.68-72.44c17.39.68 66.75-.25 86.69-1.01 3.66-.14 6.9-.4 10.37-1.58 8.23-2.85 17.06-8.72 22.57-15.18 8.14-9.47 3.88-19.85-4.65-27.71-31.19-26.75-72.1-37.53-112.62-32.33-27.92 3.81-91.19 31.28-80.66 52.87 4.98 8.33 15.55 19.06 24.6 23.03 3.06 1.17 6.58.96 9.91 1.01 15.65.15 28.01.8 43.66.89h.12zm-55.96 50.81c-1.49.03-3.11 1.28-4.29 2.38-16.62 15.03-27.34 34.45-32.39 55.24-11.92 47.94 7.66 103.54 51.97 128.04 123.31 73.93 249.68-75.84 149.57-185.06-2.96-3.34-5.63-3.99-8.6-.76-2.09 2.01-4.17 4.02-6.25 6.03-1.89 1.89-4.4 3.82-4.29 5.74.08 1.7 1.47 2.83 2.66 3.96 16.81 14.72 27.59 33.1 31.89 52.56 6.32 27.91-1.54 57.55-20.26 80.02-92.83 106.7-253.39-33.94-146.88-136.66.07-2.41-3.46-4.51-5.37-6.36-2.41-1.76-4.48-4.86-7.6-5.12h-.15zm10.66-29.55c12.91 49.6 121.56 47.77 138.36.78-.08-1.08-1.12-1.6-2.78-1.73-44.34-.17-87.73.0-131.3-.06-1.37-.02-3.56.03-4.21.92l-.07.09z"/><path class="cls-1" d="M255.06 172.51c3.11 1 5.1 3.89 7.68 5.87 12.2 9.18 22.72 21.19 27.47 34.59 9.65 25.89 4.17 59.63-16.42 79.2-49.47 48.7-138.36 17.5-141.36-53.5-.51-21.56 10.48-45.01 29-57.42 3.79-2.39 6.89-6.14 11.27-7.25 2.39-.53 5.1.12 7.53.72 14.71 3.7 29.65 5.25 44.74 3.16 5.77-.79 11.48-2.04 17.15-3.36 4.2-.7 8.58-2.83 12.78-2.08l.16.04zM253.92 217.15c-5.98 5.08-11.51 13.48-9.39 21.64 2.63 11.24 15.82 17.72 26.77 13.38 11.89-4.27 15.5-19.61 7.45-29.2-5.5-6.95-17.27-11.5-24.68-5.94l-.15.11zM157.76 215.76c.73 1.63 3.03 2.37 4.11 3.83 3.84 4.02.04 10.35-4.72 11.56-2.8.8-6.03-1.11-7.71-3.69-.91-1.06-1.58-4.08-3.32-3.24-2.49 2.4-2.98 6.35-3.1 9.73-.16 22.97 30.48 26.53 38.81 6.32 4.3-11.15-4.19-22.18-14.85-25.35-2.13-.78-8.67-1.52-9.24.77v.08zm55.66 46.66c11.38.44 23.08-10.59 22.79-22.87-.5-5.19-7.28-9.61-11.86-6.63-2.51 1.94-3.32 6.23-5.23 8.83-2.44 3.85-8.81 3.79-11.32.07-1.87-2.44-2.4-6.42-4.56-8.34-2.61-2.41-7.59-1.5-9.91.91-2.13 2.07-2.78 5.35-2.58 8.26.68 11.08 11.22 19.81 22.46 19.76h.2z"/><path class="cls-1" d="M130.11 155.2c3.26-4.15 5.35-3.62 8.84-.31 2.31 2.1 5.05 4.13 7.21 6.44 1.89 1.98.89 4.33-.89 6.15-3.73 4.16-7.59 8.22-11.4 12.5-11.73 13.2-19.15 30.13-21.28 47.31-5.87 47.02 22.26 88.14 67.08 103.74 35.43 13.95 80.64 5.14 108.39-24.7 11.27-12.45 20.44-26.88 24.75-43.06 7.53-29.79 2.72-62.48-20.45-84.92-4.08-4.31-8.43-8.22-12.26-12.74-2.34-3.17.92-5.59 3.5-8.11 1.85-1.79 3.7-3.57 5.55-5.35 1.86-1.82 4.31-4.52 6.77-2.11 9.58 12.05 19.26 24.15 26.69 37.6 30.94 56.42 1.86 132.89-56.2 157.48-31.35 15.05-66.94 15.14-99.04.85-35.74-14.59-62.55-43.95-70.7-81.16"/><path class="cls-4" d="M213.22 262.42c-11.24.05-21.78-8.68-22.46-19.76-.2-2.91.45-6.19 2.58-8.26 2.32-2.41 7.3-3.32 9.91-.91 2.17 1.92 2.69 5.9 4.56 8.34 2.51 3.72 8.88 3.78 11.32-.07 1.92-2.6 2.72-6.89 5.23-8.83 4.58-2.99 11.35 1.44 11.86 6.62.28 12.29-11.41 23.31-22.79 22.87h-.2z"/><path class="cls-3" d="M254.27 217.19c4.9-.94 10.11 4.79 8.25 9.59-.66 1.93-1.85 4.11-3.76 4.96-2.74.93-7.01-.47-8.16-3.28-.92-3.73-.33-9.42 3.49-11.22l.17-.06z"/><path class="cls-2" d="M195.48 62.7c-.11-2.8.2-5.67.48-8.47.37-3.4.72-7.12 2.6-10.02 2.07-3.27 6.23-5.17 10.03-4.41 8.61 2.16 7.02 13.36 7.15 20.23-1.12 15.67 7.22 6.68 16 10.39 5.24 2.35 4.12 9.37.69 12.85-6.99 7.97-23.36 10.29-31.51 1.67-5.8-5.63-5.22-14.71-5.43-22.05v-.18z"/></g><circle class="cls-4" cx="162.5" cy="234.5" r="19.5"/><circle class="cls-4" cx="262.5" cy="234.5" r="19.5"/><circle class="cls-3" cx="254.5" cy="224.5" r="7.5"/><circle class="cls-3" cx="154.5" cy="223.5" r="7.5"/></g></svg></span><span class=navbar-brand__name>LLMariner</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class="nav-link active" href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/llmariner/llmariner target=_blank rel=noopener><span>GitHub</span><sup><i class="ps-1 fa-solid fa-up-right-from-square fa-xs" aria-hidden=true></i></sup></a></li><li class="td-light-dark-menu nav-item dropdown"><svg class="d-none"><symbol id="check2" viewBox="0 0 16 16"><path d="M13.854 3.646a.5.5.0 010 .708l-7 7a.5.5.0 01-.708.0l-3.5-3.5a.5.5.0 11.708-.708L6.5 10.293l6.646-6.647a.5.5.0 01.708.0z"/></symbol><symbol id="circle-half" viewBox="0 0 16 16"><path d="M8 15A7 7 0 108 1v14zm0 1A8 8 0 118 0a8 8 0 010 16z"/></symbol><symbol id="moon-stars-fill" viewBox="0 0 16 16"><path d="M6 .278a.768.768.0 01.08.858 7.208 7.208.0 00-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527.0 1.04-.055 1.533-.16a.787.787.0 01.81.316.733.733.0 01-.031.893A8.349 8.349.0 018.344 16C3.734 16 0 12.286.0 7.71.0 4.266 2.114 1.312 5.124.06A.752.752.0 016 .278z"/><path d="M10.794 3.148a.217.217.0 01.412.0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217.0 010 .412l-1.162.387A1.734 1.734.0 0011.593 7.69l-.387 1.162a.217.217.0 01-.412.0l-.387-1.162A1.734 1.734.0 009.31 6.593l-1.162-.387a.217.217.0 010-.412l1.162-.387a1.734 1.734.0 001.097-1.097l.387-1.162zM13.863.099a.145.145.0 01.274.0l.258.774c.115.346.386.617.732.732l.774.258a.145.145.0 010 .274l-.774.258a1.156 1.156.0 00-.732.732l-.258.774a.145.145.0 01-.274.0l-.258-.774a1.156 1.156.0 00-.732-.732l-.774-.258a.145.145.0 010-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"/></symbol><symbol id="sun-fill" viewBox="0 0 16 16"><path d="M8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 0zm0 13a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 13zm8-5a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2a.5.5.0 01.5.5zM3 8a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2A.5.5.0 013 8zm10.657-5.657a.5.5.0 010 .707l-1.414 1.415a.5.5.0 11-.707-.708l1.414-1.414a.5.5.0 01.707.0zm-9.193 9.193a.5.5.0 010 .707L3.05 13.657a.5.5.0 01-.707-.707l1.414-1.414a.5.5.0 01.707.0zm9.193 2.121a.5.5.0 01-.707.0l-1.414-1.414a.5.5.0 01.707-.707l1.414 1.414a.5.5.0 010 .707zM4.464 4.465a.5.5.0 01-.707.0L2.343 3.05a.5.5.0 11.707-.707l1.414 1.414a.5.5.0 010 .708z"/></symbol></svg>
<button class="btn btn-link nav-link dropdown-toggle d-flex align-items-center" id=bd-theme type=button aria-expanded=false data-bs-toggle=dropdown data-bs-display=static aria-label="Toggle theme (auto)"><svg class="bi my-1 theme-icon-active"><use href="#circle-half"/></svg></button><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=bd-theme-text><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=light aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#sun-fill"/></svg>
Light<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=dark aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#moon-stars-fill"/></svg>
Dark<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center active" data-bs-theme-value=auto aria-pressed=true>
<svg class="bi me-2 opacity-50"><use href="#circle-half"/></svg>
Auto<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li></ul></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.bf79adda07fcc5d320af94d941a21d7a.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/features/>Return to the regular view of this page</a>.</p></div><h1 class=title>Features</h1><div class=lead>LLMariner features</div><ul><li>1: <a href=#pg-e254f2bb7611309715fbd117e4ceb3b7>Inference with Open Models</a></li><li>2: <a href=#pg-a90036a92e39d96ca98dbb924258b27b>Supported Open Models</a></li><li>3: <a href=#pg-559ba074f66298c7e9908d15750ab530>Retrieval-Augmented Generation (RAG)</a></li><li>4: <a href=#pg-e543d8263acb53250256de185d42cd21>Model Fine-tuning</a></li><li>5: <a href=#pg-0eae5360aca6a618ec933d565b825e7e>General-purpose Training</a></li><li>6: <a href=#pg-28b6f41d7083500232434001016b060d>Jupyter Notebook</a></li><li>7: <a href=#pg-801a2e5cd9c3c1c32f39935005c3f24e>API and GPU Usage Visibility</a></li><li>8: <a href=#pg-728e0b08215c55af1b72e7bae479d8a2>User Management</a></li><li>9: <a href=#pg-dac07794e5c1cd0aa1170dedef9d101d>Access Control with Organizations and Projects</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-e254f2bb7611309715fbd117e4ceb3b7>1 - Inference with Open Models</h1><div class=lead>Users can run chat completion with open models such as Google Gemma, LLama, Mistral, etc. To run chat completion, users can use the OpenAI Python library, <code>llma</code> CLI, or API endpoint.</div><p>Here is an example chat completion command with the <code>llma</code> CLI.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion <span class=s2>&#34;What is k8s?&#34;</span>
</span></span></code></pre></div><p>If you want to use the Python library, you first need to create an API key:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma auth api-keys create &lt;key name&gt;
</span></span></code></pre></div><p>You can then pass the API key to initialize the OpenAI client and run the completion:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;&lt;Base URL (e.g., http://localhost:8080/v1)&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;&lt;API key secret&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it-q4_0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;What is k8s?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>response</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>You can also just call ``client = OpenAI()<code>if you set environment variables</code>OPENAI_BASE_URL<code>and</code>OPENAI_API_KEY`.</p><p>If you want to hit the API endpoint directly, you can use <code>curl</code>. Here is an example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data <span class=s1>&#39;{&#34;model&#34;: &#34;google-gemma-2b-it-q4_0&#34;, &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is k8s?&#34;}]}&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:8080/v1/chat/completions
</span></span></code></pre></div><p>Please see <a href=/docs/features/fine_tuning/>the fine-tuning page</a> if you want to generate a fine-tuning model and use that for chat completion.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a90036a92e39d96ca98dbb924258b27b>2 - Supported Open Models</h1><div class=lead>The following shows the supported models.</div><p>Please note that some models work only with specific inference runtimes.</p><table><thead><tr><th>Model</th><th>Quantizations</th><th>Supporting runtimes</th></tr></thead><tbody><tr><td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td><td>None</td><td>vLLM</td></tr><tr><td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td><td>AWQ</td><td>vLLM</td></tr><tr><td>deepseek-ai/DeepSeek-Coder-V2-Lite-Base</td><td>Q2_K, Q3_K_M, Q3_K_S, Q4_0</td><td>Ollama</td></tr><tr><td>deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct</td><td>Q2_K, Q3_K_M, Q3_K_S, Q4_0</td><td>Ollama</td></tr><tr><td>deepseek-ai/deepseek-coder-6.7b-base</td><td>None</td><td>vLLM, Ollama</td></tr><tr><td>deepseek-ai/deepseek-coder-6.7b-base</td><td>AWQ</td><td>vLLM</td></tr><tr><td>deepseek-ai/deepseek-coder-6.7b-base</td><td>Q4_0</td><td>vLLM, Ollama</td></tr><tr><td>google/gemma-2b-it</td><td>None</td><td>Ollama</td></tr><tr><td>google/gemma-2b-it</td><td>Q4_0</td><td>Ollama</td></tr><tr><td>intfloat/e5-mistral-7b-instruct</td><td>None</td><td>vLLM</td></tr><tr><td>meta-llama/Meta-Llama-3.1-70B-Instruct</td><td>AWQ</td><td>vLLM</td></tr><tr><td>meta-llama/Meta-Llama-3.1-70B-Instruct</td><td>Q2_K, Q3_K_M, Q3_K_S, Q4_0</td><td>vLLM, Ollama</td></tr><tr><td>meta-llama/Meta-Llama-3.1-8B-Instruct</td><td>None</td><td>vLLM</td></tr><tr><td>meta-llama/Meta-Llama-3.1-8B-Instruct</td><td>AWQ</td><td>vLLM, Triton</td></tr><tr><td>meta-llama/Meta-Llama-3.1-8B-Instruct</td><td>Q4_0</td><td>vLLM, Ollama</td></tr><tr><td>nvidia/Llama-3.1-Nemotron-70B-Instruct</td><td>Q2_K, Q3_K_M, Q3_K_S, Q4_0</td><td>vLLM</td></tr><tr><td>nvidia/Llama-3.1-Nemotron-70B-Instruct</td><td>FP8-Dynamic</td><td>vLLM</td></tr><tr><td>mistralai/Mistral-7B-Instruct-v0.2</td><td>Q4_0</td><td>Ollama</td></tr><tr><td>sentence-transformers/all-MiniLM-L6-v2-f16</td><td>None</td><td>Ollama</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-559ba074f66298c7e9908d15750ab530>3 - Retrieval-Augmented Generation (RAG)</h1><div class=lead>This page describes how to use RAG with LLMariner.</div><h2 id=an-example-flow>An Example Flow<a class=td-heading-self-link href=#an-example-flow aria-label="Heading self-link"></a></h2><p>The first step is to create a vector store and create files in the vector store. Here is an example script with the OpenAI Python library:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;&lt;LLMariner Endpoint URL&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;&lt;LLMariner API key&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>filename</span> <span class=o>=</span> <span class=s2>&#34;llmariner_overview.txt&#34;</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s2>&#34;w&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>fp</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>fp</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s2>&#34;LLMariner builds a software stack that provides LLM as a service. It provides the OpenAI-compatible API.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>file</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>files</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>file</span><span class=o>=</span><span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>  <span class=n>purpose</span><span class=o>=</span><span class=s2>&#34;assistants&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Uploaded file. ID=</span><span class=si>%s</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=n>file</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>vs</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>beta</span><span class=o>.</span><span class=n>vector_stores</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>name</span><span class=o>=</span><span class=s1>&#39;Test vector store&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Created vector store. ID=</span><span class=si>%s</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=n>vs</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>vfs</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>beta</span><span class=o>.</span><span class=n>vector_stores</span><span class=o>.</span><span class=n>files</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>vector_store_id</span><span class=o>=</span><span class=n>vs</span><span class=o>.</span><span class=n>id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>file_id</span><span class=o>=</span><span class=n>file</span><span class=o>.</span><span class=n>id</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Created vector store file. ID=</span><span class=si>%s</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=n>vfs</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
</span></span></code></pre></div><p>Once the files are added into vector store, you can run the completion request with the RAG model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;&lt;Base URL (e.g., http://localhost:8080/v1)&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;&lt;API key secret&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it-q4_0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;What is LLMariner?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>tool_choice</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;choice&#34;</span><span class=p>:</span> <span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;function&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;function&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>     <span class=s2>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;rag&#34;</span>
</span></span><span class=line><span class=cl>   <span class=p>}</span>
</span></span><span class=line><span class=cl> <span class=p>},</span>
</span></span><span class=line><span class=cl> <span class=n>tools</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>   <span class=p>{</span>
</span></span><span class=line><span class=cl>     <span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;function&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>     <span class=s2>&#34;function&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>       <span class=s2>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;rag&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=s2>&#34;parameters&#34;</span><span class=p>:</span> <span class=s2>&#34;{</span><span class=se>\&#34;</span><span class=s2>vector_store_name</span><span class=se>\&#34;</span><span class=s2>:</span><span class=se>\&#34;</span><span class=s2>Test vector store</span><span class=se>\&#34;</span><span class=s2>}&#34;</span>
</span></span><span class=line><span class=cl>     <span class=p>}</span>
</span></span><span class=line><span class=cl>   <span class=p>}</span>
</span></span><span class=line><span class=cl> <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>response</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>If you want to hit the API endpoint directly, you can use <code>curl</code>. Here is an example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;model&#34;: &#34;google-gemma-2b-it-q4_0&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is LLMariner?&#34;}],
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;tool_choice&#34;: {
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;choice&#34;: &#34;auto&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;type&#34;: &#34;function&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;function&#34;: {
</span></span></span><span class=line><span class=cl><span class=s1>       &#34;name&#34;: &#34;rag&#34;
</span></span></span><span class=line><span class=cl><span class=s1>     }
</span></span></span><span class=line><span class=cl><span class=s1>   },
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;tools&#34;: [{
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;type&#34;: &#34;function&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;function&#34;: {
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;name&#34;: &#34;rag&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>       &#34;parameters&#34;: &#34;{\&#34;vector_store_name\&#34;:\&#34;Test vector store\&#34;}&#34;
</span></span></span><span class=line><span class=cl><span class=s1>     }
</span></span></span><span class=line><span class=cl><span class=s1> }]}&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span> http://localhost:8080/v1/chat/completions
</span></span></code></pre></div><h2 id=embedding-api>Embedding API<a class=td-heading-self-link href=#embedding-api aria-label="Heading self-link"></a></h2><p>If you want to just generate embeddings, you can use the Embedding API, which is compatible with the OpenAI API.</p><p>Here are examples:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma embeddings create --model intfloat-e5-mistral-7b-instruct --input <span class=s2>&#34;sample text&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;model&#34;: &#34;sentence-transformers-all-MiniLM-L6-v2-f16&#34;,
</span></span></span><span class=line><span class=cl><span class=s1>   &#34;input&#34;: &#34;&#34;sample text,
</span></span></span><span class=line><span class=cl><span class=s1> }&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span> http://localhost:8080/v1/embeddings
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-e543d8263acb53250256de185d42cd21>4 - Model Fine-tuning</h1><div class=lead>This page describes how to fine-tune models with LLMariner.</div><h2 id=submitting-a-fine-tuning-job>Submitting a Fine-Tuning Job<a class=td-heading-self-link href=#submitting-a-fine-tuning-job aria-label="Heading self-link"></a></h2><p>You can use the OpenAI Python library to submit a fine-tuning job. Here is an example snippet that uploads a training file and uses that to run a fine-tuning job.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;&lt;LLMariner Endpoint URL&gt;&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;&lt;LLMariner API key&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>file</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>files</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>file</span><span class=o>=</span><span class=nb>open</span><span class=p>(</span><span class=n>training_filename</span><span class=p>,</span> <span class=s2>&#34;rb&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>  <span class=n>purpose</span><span class=o>=</span><span class=s2>&#34;fine-tune&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>job</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>fine_tuning</span><span class=o>.</span><span class=n>jobs</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>suffix</span><span class=o>=</span><span class=s2>&#34;fine-tuning&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>training_file</span><span class=o>=</span><span class=n>file</span><span class=o>.</span><span class=n>id</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Created job. ID=</span><span class=si>%s</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>job</span><span class=o>.</span><span class=n>id</span><span class=p>)</span>
</span></span></code></pre></div><p>Once a fine-tuning job is submitted, a k8s Job is created. A Job runs in a namespace where a user's project is associated.</p><p>You can check the status of the job with the Python script or the <code>llma</code> CLI.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>client</span><span class=o>.</span><span class=n>fine_tuning</span><span class=o>.</span><span class=n>jobs</span><span class=o>.</span><span class=n>list</span><span class=p>())</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma fine-tuning <span class=nb>jobs</span> list
</span></span><span class=line><span class=cl>llma fine-tuning <span class=nb>jobs</span> get &lt;job-id&gt;
</span></span></code></pre></div><p>Once the job completes, you can check the generated models.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>fine_tuned_model</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>fine_tuning</span><span class=o>.</span><span class=n>jobs</span><span class=o>.</span><span class=n>list</span><span class=p>()</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>fine_tuned_model</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>fine_tuned_model</span><span class=p>)</span>
</span></span></code></pre></div><p>Then you can get the model ID and use that for the chat completion request.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=n>fine_tuned_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=o>...</span>
</span></span></code></pre></div><h2 id=debugging-a-fine-tuning-job>Debugging a Fine-Tuning Job<a class=td-heading-self-link href=#debugging-a-fine-tuning-job aria-label="Heading self-link"></a></h2><p>You can use the <code>llma</code> CLI to check the logs and exec into the pod.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma fine-tuning <span class=nb>jobs</span> logs &lt;job-id&gt;
</span></span><span class=line><span class=cl>llma fine-tuning <span class=nb>jobs</span> <span class=nb>exec</span> &lt;job-id&gt;
</span></span></code></pre></div><h2 id=managing-quota>Managing Quota<a class=td-heading-self-link href=#managing-quota aria-label="Heading self-link"></a></h2><p>LLMariner allows users to manage GPU quotas with integration with <a href=https://kueue.sigs.k8s.io/>Kueue</a>.</p><p>You can install Kueue with the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>VERSION</span><span class=o>=</span>v0.6.2
</span></span><span class=line><span class=cl>kubectl apply -f https://github.com/kubernetes-sigs/kueue/releases/download/<span class=nv>$VERSION</span>/manifests.yaml
</span></span></code></pre></div><p>Once the install completes, you should see <code>kueue-controller-manager</code> in the <code>kueue-system</code> namespace.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl get po -n kueue-system
</span></span><span class=line><span class=cl>NAME                                        READY   STATUS    RESTARTS   AGE
</span></span><span class=line><span class=cl>kueue-controller-manager-568995d897-bzxg6   2/2     Running   <span class=m>0</span>          161m
</span></span></code></pre></div><p>You can then define <code>ResourceFlavor</code>, <code>ClusterQueue</code>, and <code>LocalQueue</code> to manage quota. For example, when you want to allocate 10 GPUs to <code>team-a</code> whose project namespace is <code>team-a-ns</code>, you can define <code>ClusterQueue</code> and <code>LocalQueue</code> as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>kueue.x-k8s.io/v1beta1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterQueue</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>team-a</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespaceSelector</span><span class=p>:</span><span class=w> </span>{}<span class=w> </span><span class=c># match all.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>cohort</span><span class=p>:</span><span class=w> </span><span class=l>org-x</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>resourceGroups</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>coveredResources</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>gpu]</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>flavors</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>gpu-flavor</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>gpu</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>nominalQuota</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nn>---</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>kueue.x-k8s.io/v1beta1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>LocalQueue</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>team-a-ns</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>team-a-queue</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>clusterQueue</span><span class=p>:</span><span class=w> </span><span class=l>team-a</span><span class=w>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-0eae5360aca6a618ec933d565b825e7e>5 - General-purpose Training</h1><div class=lead>LLMariner allows users to run general-purpose training jobs in their Kubernetes clusters.</div><h2 id=creating-a-training-job>Creating a Training Job<a class=td-heading-self-link href=#creating-a-training-job aria-label="Heading self-link"></a></h2><p>You can create a training job from the local pytorch code by running the following command.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma batch <span class=nb>jobs</span> create <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --image<span class=o>=</span><span class=s2>&#34;pytorch-2.1&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-file<span class=o>=</span>my-pytorch-script.py <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-file<span class=o>=</span>requirements.txt <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --file-id<span class=o>=</span>&lt;file-id&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --command <span class=s2>&#34;python -u /scripts/my-pytorch-script.py&#34;</span>
</span></span></code></pre></div><p>Once a training job is created, a k8s Job is created. The job runs the command specified in the <code>--command</code> flag, and files specified in the <code>--from-file</code> flag are mounted to the /scripts directory in the container. If you specify the <code>--file-id</code> flag (optional), the file will be download to the /data directory in the container.</p><p>You can check the status of the job by running the following command.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma batch <span class=nb>jobs</span> list
</span></span><span class=line><span class=cl>llma batch <span class=nb>jobs</span> get &lt;job-id&gt;
</span></span></code></pre></div><h2 id=debugging-a-training-job>Debugging a Training Job<a class=td-heading-self-link href=#debugging-a-training-job aria-label="Heading self-link"></a></h2><p>You can use the <code>llma</code> CLI to check the logs of a training job.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma batch <span class=nb>jobs</span> logs &lt;job-id&gt;
</span></span></code></pre></div><h2 id=pytorch-distributed-data-parallel>PyTorch Distributed Data Parallel<a class=td-heading-self-link href=#pytorch-distributed-data-parallel aria-label="Heading self-link"></a></h2><p>LLMariner supports PyTorch Distributed Data Parallel (DDP) training. You can run a DDP training job by specifying the number of per-node GPUs and the number of workers in the <code>--gpu</code> and <code>--workers</code> flags, respectively.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma batch <span class=nb>jobs</span> create <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --image<span class=o>=</span><span class=s2>&#34;pytorch-2.1&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-file<span class=o>=</span>my-pytorch-ddp-script.py <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --gpu<span class=o>=</span><span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --workers<span class=o>=</span><span class=m>3</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --command <span class=s2>&#34;python -u /scripts/my-pytorch-ddp-script.py&#34;</span>
</span></span></code></pre></div><p>Created training job is pre-configured some DDP environment variables; MASTER_ADDR, MASTER_PORT, WORLD_SIZE, and RANK.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-28b6f41d7083500232434001016b060d>6 - Jupyter Notebook</h1><div class=lead>LLMariner allows users to run a Jupyter Notebook in a Kubernetes cluster. This functionality is useful when users want to run ad-hoc Python scripts that require GPU.</div><h2 id=creating-a-jupyter-notebook>Creating a Jupyter Notebook<a class=td-heading-self-link href=#creating-a-jupyter-notebook aria-label="Heading self-link"></a></h2><p>To create a Jupyter Notebook, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks create my-notebook
</span></span></code></pre></div><p>By default, there is no GPU allocated to the Jupyter Notebook. If you want to allocate a GPU to the Jupyter Notebook, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks create my-gpu-notebook --gpu <span class=m>1</span>
</span></span></code></pre></div><p>There are other options that you can specify when creating a Jupyter Notebook, such as environment. You can see the list of options by using the <code>--help</code> flag.</p><p>Once the Jupyter Notebook is created, you can access it by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Open the Jupyter Notebook in your browser</span>
</span></span><span class=line><span class=cl>llma workspace notebooks open my-notebook
</span></span></code></pre></div><h2 id=stopping-and-restarting-a-jupyter-notebook>Stopping and Restarting a Jupyter Notebook<a class=td-heading-self-link href=#stopping-and-restarting-a-jupyter-notebook aria-label="Heading self-link"></a></h2><p>To stop a Jupyter Notebook, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks stop my-notebook
</span></span></code></pre></div><p>To restart a Jupyter Notebook, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks start my-notebook
</span></span></code></pre></div><p>You can check the current status of the Jupyter Notebook by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma workspace notebooks list
</span></span><span class=line><span class=cl>llma workspace notebooks get my-notebook
</span></span></code></pre></div><h2 id=openai-api-integration>OpenAI API Integration<a class=td-heading-self-link href=#openai-api-integration aria-label="Heading self-link"></a></h2><p>Jupyter Notebook can be integrated with OpenAI API. Created Jupyter Notebook is pre-configured with OpenAI API URL and API key. All you need to do is to install the <code>openai</code> package.</p><p>To install <code>openai</code> package, run the following command in the Jupyter Notebook terminal:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install openai
</span></span></code></pre></div><p class="mt-4 mb-4 text-center"><img src=/images/jupyter_notebook_terminal.png width=1500 height=1085></p><p>Now, you can use the OpenAI API in the Jupyter Notebook. Here is an example of using OpenAI API in the Jupyter Notebook:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it-q4_0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;What is k8s?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>response</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p class="mt-4 mb-4 text-center"><img src=/images/jupyter_notebook_ipynb.png width=1500 height=1085></p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>By default, pre-configured API key is a JWT and it can expire. You can also pass your API key to the <code>OpenAI</code> client.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-801a2e5cd9c3c1c32f39935005c3f24e>7 - API and GPU Usage Visibility</h1><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Work-in-progress.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-728e0b08215c55af1b72e7bae479d8a2>8 - User Management</h1><div class=lead>Describes the way to manage users</div><p>LLMariner installs <a href=https://github.com/dexidp/dex>Dex</a> by default. Dex is an identity service that uses <a href=https://openid.net/developers/how-connect-works/>OpenID Connect</a> for authentication.</p><p>The Helm chart for Dex is located at <a href=https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server>https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server</a>. It uses a <a href=https://dexidp.io/docs/connectors/local/>built-in local connector</a> and has the following configuration by default:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>staticPasswords</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span>- <span class=nt>userID</span><span class=p>:</span><span class=w> </span><span class=l>08a8684b-db88-4b73-90a9-3cd1661f5466</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>username</span><span class=p>:</span><span class=w> </span><span class=l>admin</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>admin@example.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c># bcrypt hash of the string: $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hash</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W&#34;</span><span class=w>
</span></span></span></code></pre></div><p>You can switch a connector to an IdP in your environment (e.g., LDAP, GitHub). Here is an example connector configuration with Okta:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>auth</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>oidcIssuerUrl</span><span class=p>:</span><span class=w> </span><span class=l>https://&lt;LLMariner endpoint URL&gt;/v1/dex</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>dex-server</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>oauth2</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>passwordConnector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>responseTypes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>code</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>connectors</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>oidc</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>id</span><span class=p>:</span><span class=w> </span><span class=l>okta</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>okta</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>config</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>issuer</span><span class=p>:</span><span class=w> </span><span class=l>&lt;Okta issuer URL&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>clientID</span><span class=p>:</span><span class=w> </span><span class=l>&lt;Client ID of an Okta application&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>clientSecret</span><span class=p>:</span><span class=w> </span><span class=l>&lt;Client secret of an Okta application&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>redirectURI</span><span class=p>:</span><span class=w> </span><span class=l>https://&lt;LLMariner endpoint URL&gt;/v1/dex/callback</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>insecureSkipEmailVerified</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>enablePasswordDb</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>staticPassword</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span></code></pre></div><p>Please refer to the <a href=https://dexidp.io/docs/connectors/>Dec documentations</a> for more details.</p><p>The Helm chart for Dex creates an ingress so that HTTP requests to <code>v1/dex</code> are routed to Dex. This endpoint URL works as the OIDC issuer URL that CLI and backend servers use.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-dac07794e5c1cd0aa1170dedef9d101d>9 - Access Control with Organizations and Projects</h1><div class=lead>The way to configure access control using organizations and projects</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><h3 id=basic-concepts>Basic Concepts<a class=td-heading-self-link href=#basic-concepts aria-label="Heading self-link"></a></h3><p>LLMariner provides access control with two concepts: <code>Organizations</code> and <code>Projects</code>. The basic concept follows <a href=https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects>OpenAI API</a>.</p><p>You can define one or more than one organization. In each organization, you can define one or more than one project. For example, you can create an organization for each team in your company, and each team can create individual projects based on their needs.</p><p>A project controls the visibility of resources such as models, fine-tuning jobs. For example, a model that is generated by a fine-tuned job in project <code>P</code> is only visible from project members in <code>P</code>.</p><p>A project is also associated with a Kubernetes namespace. Fine-tuning jobs for project <code>P</code> run in the Kubernetes namespace associated with <code>P</code> (and quota management is applied).</p><h3 id=roles>Roles<a class=td-heading-self-link href=#roles aria-label="Heading self-link"></a></h3><p>Each user has an <code>organization role</code> and a <code>project role</code>, and these roles control resources that a user can access and actions that a user can take.</p><p>An organization role is either <code>owner</code> or <code>reader</code>. A project role is either <code>owner</code> or <code>member</code>. If you want to allow a user to use LLMariner without any organization/project management privilege, you can grant the organization role <code>reader</code> and the project role <code>member</code>. If you want to allow a user to manage the project, you can grant the project role <code>owner</code>.</p><p>Here is an diagram shows an example role assignment.</p><p class="mt-4 mb-4 text-center"><img src=/images/org_and_project.png width=923 height=420></p><p>The following summarizes how these role implements the access control:</p><ul><li>A user can access resources in project <code>P</code> in organization <code>O</code> if the user is a <code>member</code> of <code>P</code>, <code>owner</code> of <code>P</code>, or <code>owner</code> of <code>O</code>.</li><li>A user can manage project <code>P</code> (e.g., add a new member) in organization <code>O</code> if the user is an <code>owner</code> of <code>P</code> or <code>owner</code> of <code>O</code>.</li><li>A user can manage organization <code>O</code> (e.g., add a new member) if the user is an <code>owner</code> of <code>O</code>.</li><li>A user can create a new organization if the user is an <code>owner</code> of the initial organization that is created by default.</li></ul><p>Please note that a user who has the <code>reader</code> organization role cannot access resources in the organization unless the user is added to a project in the organization.</p><h2 id=creating-organizations-and-projects>Creating Organizations and Projects<a class=td-heading-self-link href=#creating-organizations-and-projects aria-label="Heading self-link"></a></h2><p>You can use CLI <code>llma</code> to create a new organization and a project.</p><h3 id=creating-a-new-organization>Creating a new Organization<a class=td-heading-self-link href=#creating-a-new-organization aria-label="Heading self-link"></a></h3><p>You can run the following command to create a new organization.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin organizations create &lt;organization title&gt;
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>You can also type <code>llm auth orgs</code> instead of <code>llm auth organizations</code>.</div><p>You can confirm that the new organization is created by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin organizations list
</span></span></code></pre></div><p>Then you can add a user member to the organization.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin organizations add-member &lt;organization title&gt; --email &lt;email-address of the member&gt; --role &lt;role&gt;
</span></span></code></pre></div><p>The role can be either <code>owner</code> or <code>reader</code>.</p><p>You can confirm organization members by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin organizations list-members &lt;organization title&gt;
</span></span></code></pre></div><h3 id=creating-a-new-project>Creating a new Project<a class=td-heading-self-link href=#creating-a-new-project aria-label="Heading self-link"></a></h3><p>You can take a similar flow to create a new project. To create a new project, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin projects create --title &lt;project title&gt; --organization-title &lt;organization title&gt;
</span></span></code></pre></div><p>To confirm the project is created, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin projects list
</span></span></code></pre></div><p>Then you can add a user member to the project.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin projects add-member &lt;project title&gt; --email &lt;email-address of the member&gt; --role &lt;role&gt;
</span></span></code></pre></div><p>The role can be either <code>owner</code> or <code>member</code>.</p><p>You can confirm project members by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin projects list-members --title &lt;project title&gt; --organization-title &lt;organization title&gt;
</span></span></code></pre></div><p>If you want to manage a project in a different organization, you can pass <code>--organization-title &lt;title></code> in each command. Otherwise, the organization in the current context is used. You can also change the current context by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma context <span class=nb>set</span>
</span></span></code></pre></div><h2 id=choosing-an-organization-and-a-project>Choosing an Organization and a Project<a class=td-heading-self-link href=#choosing-an-organization-and-a-project aria-label="Heading self-link"></a></h2><p>You can use <code>llma context set</code> to set the current context.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma context <span class=nb>set</span>
</span></span></code></pre></div><p>Then the selected context is applied to CLI commands (e.g., <code>llma models list</code>).</p><p>When you create a new API key, the key will be associated with the project in the current context. Suppose that a user runs the following commands:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma context <span class=nb>set</span> <span class=c1># Choose project my-project</span>
</span></span><span class=line><span class=cl>llma auth api-keys create my-key
</span></span></code></pre></div><p>The newly created API key is associated with project <code>my-project</code>.</p></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/llmariner/llmariner aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://join.slack.com/t/llmariner/shared_invite/zt-2rbwooslc-LIrUCmK9kklfKsMEirUZbg aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2024
<span class=td-footer__authors>CloudNatix, Inc.</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.f72d00502781aaf278a14088eb2356b1ba2a05ad698a0c43fe86314d74ceab56.js integrity="sha256-9y0AUCeBqvJ4oUCI6yNWsboqBa1pigxD/oYxTXTOq1Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>