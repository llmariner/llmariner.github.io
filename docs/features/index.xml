<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Features on LLMariner</title><link>https://llmariner.ai/docs/features/</link><description>Recent content in Features on LLMariner</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmariner.ai/docs/features/index.xml" rel="self" type="application/rss+xml"/><item><title>Inference with Open Models</title><link>https://llmariner.ai/docs/features/inference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/inference/</guid><description>&lt;p>Here is an example chat completion command with the &lt;code>llma&lt;/code> CLI.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion &lt;span class="s2">&amp;#34;What is k8s?&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you want to use the Python library, you first need to create an API key:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma auth api-keys create &amp;lt;key name&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can then pass the API key to initialize the OpenAI client and run the completion:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">openai&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAI&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAI&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">base_url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;Base URL (e.g., http://localhost:8080/v1)&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">api_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;API key secret&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">completion&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">chat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">completions&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;google-gemma-2b-it-q4_0&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">messages&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;role&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;user&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;content&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;What is k8s?&amp;#34;&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stream&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">response&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">completion&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">choices&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">delta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">end&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can also just call ``client = OpenAI()&lt;code>if you set environment variables&lt;/code>OPENAI_BASE_URL&lt;code>and&lt;/code>OPENAI_API_KEY`.&lt;/p></description></item><item><title>Supported Open Models</title><link>https://llmariner.ai/docs/features/models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/models/</guid><description>&lt;p>Please note that some models work only with specific inference runtimes.&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>Model&lt;/th>
 &lt;th>Quantizations&lt;/th>
 &lt;th>Supporting runtimes&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>TinyLlama/TinyLlama-1.1B-Chat-v1.0&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>TinyLlama/TinyLlama-1.1B-Chat-v1.0&lt;/td>
 &lt;td>AWQ&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/DeepSeek-Coder-V2-Lite-Base&lt;/td>
 &lt;td>Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&lt;/td>
 &lt;td>Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/deepseek-coder-6.7b-base&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/deepseek-coder-6.7b-base&lt;/td>
 &lt;td>AWQ&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>deepseek-ai/deepseek-coder-6.7b-base&lt;/td>
 &lt;td>Q4_0&lt;/td>
 &lt;td>vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>google/gemma-2b-it&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>google/gemma-2b-it&lt;/td>
 &lt;td>Q4_0&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>intfloat/e5-mistral-7b-instruct&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-70B-Instruct&lt;/td>
 &lt;td>AWQ&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-70B-Instruct&lt;/td>
 &lt;td>Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td>vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/td>
 &lt;td>AWQ&lt;/td>
 &lt;td>vLLM, Triton&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/td>
 &lt;td>Q4_0&lt;/td>
 &lt;td>vLLM, Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>nvidia/Llama-3.1-Nemotron-70B-Instruct&lt;/td>
 &lt;td>Q2_K, Q3_K_M, Q3_K_S, Q4_0&lt;/td>
 &lt;td>vLLM&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>mistralai/Mistral-7B-Instruct-v0.2&lt;/td>
 &lt;td>Q4_0&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>sentence-transformers/all-MiniLM-L6-v2-f16&lt;/td>
 &lt;td>None&lt;/td>
 &lt;td>Ollama&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table></description></item><item><title>Retrieval-Augmented Generation (RAG)</title><link>https://llmariner.ai/docs/features/rag/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/rag/</guid><description>&lt;h2 id="an-example-flow">An Example Flow&lt;a class="td-heading-self-link" href="#an-example-flow" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The first step is to create a vector store and create files in the vector store. Here is an example script with the OpenAI Python library:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">openai&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAI&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAI&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">base_url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner Endpoint URL&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">api_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">filename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;llmariner_overview.txt&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;w&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">fp&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">write&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;LLMariner builds a software stack that provides LLM as a service. It provides the OpenAI-compatible API.&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">files&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;rb&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">purpose&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;assistants&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Uploaded file. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">vs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">beta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vector_stores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;Test vector store&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Created vector store. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">vs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">vfs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">beta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vector_stores&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">files&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vector_store_id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">vs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">file_id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Created vector store file. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">vfs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once the files are added into vector store, you can run the completion request with the RAG model.&lt;/p></description></item><item><title>Model Fine-tuning</title><link>https://llmariner.ai/docs/features/fine_tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/fine_tuning/</guid><description>&lt;h2 id="submitting-a-fine-tuning-job">Submitting a Fine-Tuning Job&lt;a class="td-heading-self-link" href="#submitting-a-fine-tuning-job" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>You can use the OpenAI Python library to submit a fine-tuning job. Here is an example snippet that uploads a training file and uses that to run a fine-tuning job.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">openai&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">OpenAI&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">client&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">OpenAI&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">base_url&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner Endpoint URL&amp;gt;&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">api_key&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;lt;LLMariner API key&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">files&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">training_filename&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;rb&amp;#34;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">purpose&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;fine-tune&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">job&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">client&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fine_tuning&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">jobs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">create&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;google-gemma-2b-it&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">suffix&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;fine-tuning&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">training_file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Created job. ID=&lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">job&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once a fine-tuning job is submitted, a k8s Job is created. A Job runs in a namespace where a user's project is associated.&lt;/p></description></item><item><title>Jupyter Notebook</title><link>https://llmariner.ai/docs/features/jupyter_notebook/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/jupyter_notebook/</guid><description>&lt;h2 id="creating-a-jupyter-notebook">Creating a Jupyter Notebook&lt;a class="td-heading-self-link" href="#creating-a-jupyter-notebook" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>To create a Jupyter Notebook, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma workspace notebooks create my-notebook
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>By default, there is no GPU allocated to the Jupyter Notebook. If you want to allocate a GPU to the Jupyter Notebook, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma workspace notebooks create my-gpu-notebook --gpu &lt;span class="m">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There are other options that you can specify when creating a Jupyter Notebook, such as environment. You can see the list of options by using the &lt;code>--help&lt;/code> flag.&lt;/p></description></item><item><title>General-purpose Training</title><link>https://llmariner.ai/docs/features/training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/training/</guid><description>&lt;h2 id="creating-a-training-job">Creating a Training Job&lt;a class="td-heading-self-link" href="#creating-a-training-job" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>You can create a training job from the local pytorch code by running the following command.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">llma batch &lt;span class="nb">jobs&lt;/span> create &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --image&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;pytorch-2.1&amp;#34;&lt;/span> &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --from-file&lt;span class="o">=&lt;/span>my-pytorch-script.py &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --from-file&lt;span class="o">=&lt;/span>requirements.txt &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --file-id&lt;span class="o">=&lt;/span>&amp;lt;file-id&amp;gt; &lt;span class="se">\
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="se">&lt;/span> --command &lt;span class="s2">&amp;#34;python -u /scripts/my-pytorch-script.py&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once a training job is created, a k8s Job is created. The job runs the command specified in the &lt;code>--command&lt;/code> flag, and files specified in the &lt;code>--from-file&lt;/code> flag are mounted to the /scripts directory in the container. If you specify the &lt;code>--file-id&lt;/code> flag (optional), the file will be download to the /data directory in the container.&lt;/p></description></item><item><title>API and GPU Usage Visibility</title><link>https://llmariner.ai/docs/features/visibility/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/visibility/</guid><description>&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Work-in-progress.

&lt;/div></description></item><item><title>User Management</title><link>https://llmariner.ai/docs/features/user_management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/user_management/</guid><description>&lt;p>LLMariner installs &lt;a href="https://github.com/dexidp/dex">Dex&lt;/a> by default. Dex is an identity service that uses &lt;a href="https://openid.net/developers/how-connect-works/">OpenID Connect&lt;/a> for authentication.&lt;/p>
&lt;p>The Helm chart for Dex is located at &lt;a href="https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server">https://github.com/llmariner/rbac-manager/tree/main/deployments/dex-server&lt;/a>. It uses a &lt;a href="https://dexidp.io/docs/connectors/local/">built-in local connector&lt;/a> and has the following configuration by default:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">staticPasswords&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>- &lt;span class="nt">userID&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">08a8684b-db88-4b73-90a9-3cd1661f5466&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">username&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">admin&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">email&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">admin@example.com&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># bcrypt hash of the string: $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">hash&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can switch a connector to an IdP in your environment (e.g., LDAP, GitHub). Here is an example connector configuration with Okta:&lt;/p></description></item><item><title>Access Control with Organizations and Projects</title><link>https://llmariner.ai/docs/features/access_control/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/features/access_control/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="basic-concepts">Basic Concepts&lt;a class="td-heading-self-link" href="#basic-concepts" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>LLMariner provides access control with two concepts: &lt;code>Organizations&lt;/code> and &lt;code>Projects&lt;/code>. The basic concept follows &lt;a href="https://help.openai.com/en/articles/9186755-managing-your-work-in-the-api-platform-with-projects">OpenAI API&lt;/a>.&lt;/p>
&lt;p>You can define one or more than one organization. In each organization, you can define one or more than one project. For example, you can create an organization for each team in your company, and each team can create individual projects based on their needs.&lt;/p>
&lt;p>A project controls the visibility of resources such as models, fine-tuning jobs. For example, a model that is generated by a fine-tuned job in project &lt;code>P&lt;/code> is only visible from project members in &lt;code>P&lt;/code>.&lt;/p></description></item></channel></rss>