<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://llmariner.ai/docs/setup/install/><link rel=alternate type=application/rss+xml href=https://llmariner.ai/docs/setup/install/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Installation | LLMariner</title>
<meta name=description content="Choose the guide that best suits your needs and platform."><meta property="og:url" content="https://llmariner.ai/docs/setup/install/"><meta property="og:site_name" content="LLMariner"><meta property="og:title" content="Installation"><meta property="og:description" content="Choose the guide that best suits your needs and platform."><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta itemprop=name content="Installation"><meta itemprop=description content="Choose the guide that best suits your needs and platform."><meta itemprop=dateModified content="2024-10-25T23:25:34+09:00"><meta itemprop=wordCount content="52"><meta name=twitter:card content="summary"><meta name=twitter:title content="Installation"><meta name=twitter:description content="Choose the guide that best suits your needs and platform."><link rel=preload href=/scss/main.min.f5f1bfbe5d5d7a6f0bb1b9d8ae3a3aee20f3a7a4d3e278a18d40b0d46e2b0ddb.css as=style><link href=/scss/main.min.f5f1bfbe5d5d7a6f0bb1b9d8ae3a3aee20f3a7a4d3e278a18d40b0d46e2b0ddb.css rel=stylesheet><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-JBN3K7Y529"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JBN3K7Y529")}</script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg id="Layer_2" viewBox="0 0 426 426"><defs><style>.cls-1{fill:none}.cls-2{fill:#2491eb}.cls-3{fill:#fff}.cls-4{fill:#102c3f}</style></defs><g id="Layer_4"><rect class="cls-1" width="426" height="426"/></g><g id="Layer_1-2" data-name="Layer_1"><g><path class="cls-3" d="M297.78 142.6c-.58-.78-.73-1.54-.75-2.49.1-6.62-.3-28.22.19-34.77.36-1.5 3.4-3.12 5.17-4.29 23.46-14.42 32.1-37.79 13.17-57.48-18.33-21.77-74.32-35.96-118.14-32.11-43.09 1.55-97.3 22.81-99.25 52.59-1.55 18.53 13.46 31.7 28.95 40.28 2.39 1.31 5.2 2.63 4.84 5.73.0 5.84.02 20.42-.02 25.02-.07 1.61-.28 2.3-1.29 3.29-15.69 14.17-34.21 39.77-40.92 64.65-26.6 91.07 58.97 179.78 151.47 157.86 101.16-23.37 134.6-150.45 56.63-218.18l-.07-.09z"/><path class="cls-2" d="M295.05 136.36c-6.98 7.43-15.99 13.84-26.47 18.86 25.74 17.44 42.65 46.92 42.65 80.36.0 53.57-43.43 97-97 97s-97-43.43-97-97c0-33.35 16.83-62.76 42.46-80.22-11.37-5.39-21.01-12.43-28.24-20.63-29.98 23.81-49.21 60.58-49.21 101.85.0 71.8 58.2 130 130 130s130-58.2 130-130c0-40.33-18.36-76.37-47.19-100.21z"/><path class="cls-4" d="M328.22 322.29c2.26 9.28 22.08 15.7 27.56 26.74 11.43 17.87-8.84 37.22-26.58 25.78-9.21-5.77-12.56-16.75-18.27-25.5-3.07-5.09-5.42-6.97-9.88-2.77-19.67 16.58-42.54 24.87-68.13 27.94-1.54.19-3.63.48-4.53 1.54-.85.88-.95 2.29-.64 3.96.56 2.75 1.39 5.54 2.13 8.24 1.08 3.89 2.14 7.73 2.63 11.69 1.59 10.12-3.55 22.67-14.13 25.25-11.45 2.91-22.27-4.78-24.06-16.58-3.76-15.85 17.86-32.59.2-34.5-25.37-3.81-49.39-12.98-69.68-28.64-1.18-.89-2.75-2.08-4.07-2.05-1.47-.08-2.74 1.36-3.58 2.59-7.88 11-15.62 26.79-28.27 32.28-13.22 4.53-26.6-10.3-22.79-23.2 1.49-4.91 4.92-9.41 8.61-12.86 7.37-6.76 15.69-8.23 23.69-15.72 2.93-2.48 2.89-5.82.44-8.61-12.26-16.48-20.13-36.23-23.49-56.51-.51-2.23-.77-5.47-2.6-6.59-3.89-2.56-13.17 3.97-23.23 5.6-10.93 2.71-26.13-7.21-26.02-17.93-.5-13.11 16.04-25.99 31.83-20.82 3.52.69 15.32 7.66 17.66 4.91.88-.89 1-2.9 1.13-4.31 1.68-22.16 9.69-40.84 22.64-57.67 2.95-3.16 2.15-6.87-.94-9.62-14.13-13.06-31.37-10.71-32.06-31.83.23-11.9 13.31-21.57 24.72-18.3 9.61 2.95 19.88 19.48 26.82 27.24 4.28 5.52 6.39 1.56 6.47-3.47.11-2.18.0-4.38-.03-6.56-.18-5.29.97-8.91-2.32-12.32-4.35-4.56-14.08-6.82-21.3-16.79-11.66-16.14-9.9-34.68 3.64-49.08 59.69-57.99 166.35-58.78 223.98 2.58 13.71 17.62 9.88 44.2-10.26 56.44-4.38 2.85-10.32 5.22-11.59 10.72-.57 2.66-2.2 15.99-.5 18.44 1.58 2.49 5.29 2.97 7.15.49 5.61-7.86 9.47-16.84 16.24-23.76 9.24-9.24 24.45-5.21 30.22 5.39 4.16 6.74 1.47 16.87-4.27 22.63-6.46 6.77-16.19 6.77-20.56 13.87-2.22 2.98-2.68 6.78-.19 10.38 6.9 11.29 12.76 21.3 16.83 33.96 6.86 19.21 1.69 38.69 16.61 30.76 3.53-1.54 7.43-3.03 11.22-3.8 12.14-3.09 24.91 2.93 28.49 15.03 1.76 6.25-1.28 12.83-6.08 16.92-8.93 8.27-22.48 7.22-33.02 1.67-9.96-4.57-11.47-1.12-12.86 8.35-2.66 19.7-13.47 36.69-23.43 53.55-.84 1.42-1.67 3.13-1.57 4.69v.16zM255.37 172.65c-4.73-1.2-9.2 1.26-13.84 1.99-5.54 1.28-11.12 2.49-16.77 3.25-15.09 2.06-30.02.48-44.74-3.24-3.57-1-7.36-1.42-10.69.49-27.47 16.28-42.82 49.15-34.71 80.26 2.06 8.06 5.88 15.95 10.9 22.79 74.63 104.21 220.91-27.66 110.04-105.47l-.2-.06zm-65.68-72.44c17.39.68 66.75-.25 86.69-1.01 3.66-.14 6.9-.4 10.37-1.58 8.23-2.85 17.06-8.72 22.57-15.18 8.14-9.47 3.88-19.85-4.65-27.71-31.19-26.75-72.1-37.53-112.62-32.33-27.92 3.81-91.19 31.28-80.66 52.87 4.98 8.33 15.55 19.06 24.6 23.03 3.06 1.17 6.58.96 9.91 1.01 15.65.15 28.01.8 43.66.89h.12zm-55.96 50.81c-1.49.03-3.11 1.28-4.29 2.38-16.62 15.03-27.34 34.45-32.39 55.24-11.92 47.94 7.66 103.54 51.97 128.04 123.31 73.93 249.68-75.84 149.57-185.06-2.96-3.34-5.63-3.99-8.6-.76-2.09 2.01-4.17 4.02-6.25 6.03-1.89 1.89-4.4 3.82-4.29 5.74.08 1.7 1.47 2.83 2.66 3.96 16.81 14.72 27.59 33.1 31.89 52.56 6.32 27.91-1.54 57.55-20.26 80.02-92.83 106.7-253.39-33.94-146.88-136.66.07-2.41-3.46-4.51-5.37-6.36-2.41-1.76-4.48-4.86-7.6-5.12h-.15zm10.66-29.55c12.91 49.6 121.56 47.77 138.36.78-.08-1.08-1.12-1.6-2.78-1.73-44.34-.17-87.73.0-131.3-.06-1.37-.02-3.56.03-4.21.92l-.07.09z"/><path class="cls-1" d="M255.06 172.51c3.11 1 5.1 3.89 7.68 5.87 12.2 9.18 22.72 21.19 27.47 34.59 9.65 25.89 4.17 59.63-16.42 79.2-49.47 48.7-138.36 17.5-141.36-53.5-.51-21.56 10.48-45.01 29-57.42 3.79-2.39 6.89-6.14 11.27-7.25 2.39-.53 5.1.12 7.53.72 14.71 3.7 29.65 5.25 44.74 3.16 5.77-.79 11.48-2.04 17.15-3.36 4.2-.7 8.58-2.83 12.78-2.08l.16.04zM253.92 217.15c-5.98 5.08-11.51 13.48-9.39 21.64 2.63 11.24 15.82 17.72 26.77 13.38 11.89-4.27 15.5-19.61 7.45-29.2-5.5-6.95-17.27-11.5-24.68-5.94l-.15.11zM157.76 215.76c.73 1.63 3.03 2.37 4.11 3.83 3.84 4.02.04 10.35-4.72 11.56-2.8.8-6.03-1.11-7.71-3.69-.91-1.06-1.58-4.08-3.32-3.24-2.49 2.4-2.98 6.35-3.1 9.73-.16 22.97 30.48 26.53 38.81 6.32 4.3-11.15-4.19-22.18-14.85-25.35-2.13-.78-8.67-1.52-9.24.77v.08zm55.66 46.66c11.38.44 23.08-10.59 22.79-22.87-.5-5.19-7.28-9.61-11.86-6.63-2.51 1.94-3.32 6.23-5.23 8.83-2.44 3.85-8.81 3.79-11.32.07-1.87-2.44-2.4-6.42-4.56-8.34-2.61-2.41-7.59-1.5-9.91.91-2.13 2.07-2.78 5.35-2.58 8.26.68 11.08 11.22 19.81 22.46 19.76h.2z"/><path class="cls-1" d="M130.11 155.2c3.26-4.15 5.35-3.62 8.84-.31 2.31 2.1 5.05 4.13 7.21 6.44 1.89 1.98.89 4.33-.89 6.15-3.73 4.16-7.59 8.22-11.4 12.5-11.73 13.2-19.15 30.13-21.28 47.31-5.87 47.02 22.26 88.14 67.08 103.74 35.43 13.95 80.64 5.14 108.39-24.7 11.27-12.45 20.44-26.88 24.75-43.06 7.53-29.79 2.72-62.48-20.45-84.92-4.08-4.31-8.43-8.22-12.26-12.74-2.34-3.17.92-5.59 3.5-8.11 1.85-1.79 3.7-3.57 5.55-5.35 1.86-1.82 4.31-4.52 6.77-2.11 9.58 12.05 19.26 24.15 26.69 37.6 30.94 56.42 1.86 132.89-56.2 157.48-31.35 15.05-66.94 15.14-99.04.85-35.74-14.59-62.55-43.95-70.7-81.16"/><path class="cls-4" d="M213.22 262.42c-11.24.05-21.78-8.68-22.46-19.76-.2-2.91.45-6.19 2.58-8.26 2.32-2.41 7.3-3.32 9.91-.91 2.17 1.92 2.69 5.9 4.56 8.34 2.51 3.72 8.88 3.78 11.32-.07 1.92-2.6 2.72-6.89 5.23-8.83 4.58-2.99 11.35 1.44 11.86 6.62.28 12.29-11.41 23.31-22.79 22.87h-.2z"/><path class="cls-3" d="M254.27 217.19c4.9-.94 10.11 4.79 8.25 9.59-.66 1.93-1.85 4.11-3.76 4.96-2.74.93-7.01-.47-8.16-3.28-.92-3.73-.33-9.42 3.49-11.22l.17-.06z"/><path class="cls-2" d="M195.48 62.7c-.11-2.8.2-5.67.48-8.47.37-3.4.72-7.12 2.6-10.02 2.07-3.27 6.23-5.17 10.03-4.41 8.61 2.16 7.02 13.36 7.15 20.23-1.12 15.67 7.22 6.68 16 10.39 5.24 2.35 4.12 9.37.69 12.85-6.99 7.97-23.36 10.29-31.51 1.67-5.8-5.63-5.22-14.71-5.43-22.05v-.18z"/></g><circle class="cls-4" cx="162.5" cy="234.5" r="19.5"/><circle class="cls-4" cx="262.5" cy="234.5" r="19.5"/><circle class="cls-3" cx="254.5" cy="224.5" r="7.5"/><circle class="cls-3" cx="154.5" cy="223.5" r="7.5"/></g></svg></span><span class=navbar-brand__name>LLMariner</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class="nav-link active" href=/docs/><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=https://github.com/llmariner/llmariner target=_blank rel=noopener><span>GitHub</span><sup><i class="ps-1 fa-solid fa-up-right-from-square fa-xs" aria-hidden=true></i></sup></a></li><li class="td-light-dark-menu nav-item dropdown"><svg class="d-none"><symbol id="check2" viewBox="0 0 16 16"><path d="M13.854 3.646a.5.5.0 010 .708l-7 7a.5.5.0 01-.708.0l-3.5-3.5a.5.5.0 11.708-.708L6.5 10.293l6.646-6.647a.5.5.0 01.708.0z"/></symbol><symbol id="circle-half" viewBox="0 0 16 16"><path d="M8 15A7 7 0 108 1v14zm0 1A8 8 0 118 0a8 8 0 010 16z"/></symbol><symbol id="moon-stars-fill" viewBox="0 0 16 16"><path d="M6 .278a.768.768.0 01.08.858 7.208 7.208.0 00-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527.0 1.04-.055 1.533-.16a.787.787.0 01.81.316.733.733.0 01-.031.893A8.349 8.349.0 018.344 16C3.734 16 0 12.286.0 7.71.0 4.266 2.114 1.312 5.124.06A.752.752.0 016 .278z"/><path d="M10.794 3.148a.217.217.0 01.412.0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217.0 010 .412l-1.162.387A1.734 1.734.0 0011.593 7.69l-.387 1.162a.217.217.0 01-.412.0l-.387-1.162A1.734 1.734.0 009.31 6.593l-1.162-.387a.217.217.0 010-.412l1.162-.387a1.734 1.734.0 001.097-1.097l.387-1.162zM13.863.099a.145.145.0 01.274.0l.258.774c.115.346.386.617.732.732l.774.258a.145.145.0 010 .274l-.774.258a1.156 1.156.0 00-.732.732l-.258.774a.145.145.0 01-.274.0l-.258-.774a1.156 1.156.0 00-.732-.732l-.774-.258a.145.145.0 010-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"/></symbol><symbol id="sun-fill" viewBox="0 0 16 16"><path d="M8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 0zm0 13a.5.5.0 01.5.5v2a.5.5.0 01-1 0v-2A.5.5.0 018 13zm8-5a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2a.5.5.0 01.5.5zM3 8a.5.5.0 01-.5.5h-2a.5.5.0 010-1h2A.5.5.0 013 8zm10.657-5.657a.5.5.0 010 .707l-1.414 1.415a.5.5.0 11-.707-.708l1.414-1.414a.5.5.0 01.707.0zm-9.193 9.193a.5.5.0 010 .707L3.05 13.657a.5.5.0 01-.707-.707l1.414-1.414a.5.5.0 01.707.0zm9.193 2.121a.5.5.0 01-.707.0l-1.414-1.414a.5.5.0 01.707-.707l1.414 1.414a.5.5.0 010 .707zM4.464 4.465a.5.5.0 01-.707.0L2.343 3.05a.5.5.0 11.707-.707l1.414 1.414a.5.5.0 010 .708z"/></symbol></svg>
<button class="btn btn-link nav-link dropdown-toggle d-flex align-items-center" id=bd-theme type=button aria-expanded=false data-bs-toggle=dropdown data-bs-display=static aria-label="Toggle theme (auto)"><svg class="bi my-1 theme-icon-active"><use href="#circle-half"/></svg></button><ul class="dropdown-menu dropdown-menu-end" aria-labelledby=bd-theme-text><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=light aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#sun-fill"/></svg>
Light<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center" data-bs-theme-value=dark aria-pressed=false>
<svg class="bi me-2 opacity-50"><use href="#moon-stars-fill"/></svg>
Dark<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li><li><button type=button class="dropdown-item d-flex align-items-center active" data-bs-theme-value=auto aria-pressed=true>
<svg class="bi me-2 opacity-50"><use href="#circle-half"/></svg>
Auto<svg class="bi ms-auto d-none"><use href="#check2"/></svg></button></li></ul></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.bf79adda07fcc5d320af94d941a21d7a.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/setup/install/>Return to the regular view of this page</a>.</p></div><h1 class=title>Installation</h1><div class=lead>Choose the guide that best suits your needs and platform.</div><ul><li>1: <a href=#pg-80cf3fa410b01619b38e4057af7ec5c8>Install with Helm</a></li><li>2: <a href=#pg-fad99d871e69bfc31c6b63374b6793ea>Set up a Playground on a GPU EC2 Instance</a></li><li>3: <a href=#pg-2822b0bc47bba2d10518e57315f06e35>Set up a Playground on a CPU-only Kind Cluster</a></li><li>4: <a href=#pg-8de5d0eb22b83ad8b3efcf34d11c8f4b>Install in a Single EKS Cluster</a></li><li>5: <a href=#pg-b97c20cd3e745f78416fc8eddf907efc>Install in a Single On-premise Cluster</a></li><li>6: <a href=#pg-830457b369082df60146f171d2fecbac>Install across Multiple Clusters</a></li><li>7: <a href=#pg-aa1273494c214f035a8e9386ce6707b0>Hosted Control Plane</a></li></ul><div class=content><p>LLMariner takes ControlPlane-Worker model. The control plane gets a request and gives instructions to the worker while the worker processes a task such as inference.</p><p>Both components can operate within a single cluster, but if you want to utilize GPU resources across multiple clusters, they can also be installed into separate clusters.</p><p class="mt-4 mb-4 text-center"><img src=/images/install_modes.png width=2504 height=773></p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-80cf3fa410b01619b38e4057af7ec5c8>1 - Install with Helm</h1><div class=lead>Install LLMariner with Helm.</div><h2 id=prerequisites>Prerequisites<a class=td-heading-self-link href=#prerequisites aria-label="Heading self-link"></a></h2><p>LLMariner requires the following resources:</p><ul><li><a href=https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html>Nvidia GPU Operator</a></li><li>Ingress controller (to route API requests)</li><li>SQL database (to store jobs/models/files metadata)</li><li>S3-compatible object store (to store training files and models)</li><li><a href=https://milvus.io/>Milvus</a> (for RAG, optional)</li></ul><p>LLMariner can process inference requests on CPU nodes, but it can be best used with GPU nodes. Nvidia GPU Operator is required to install the device plugin and make GPUs visible in the K8s cluster.</p><p>Preferably the ingress controller should have a DNS name or an IP that is reachable from the outside of the EKS cluster. If not, you can rely on port-forwarding to reach the API endpoints.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>When port-forwarding is used, the same port needs to be used consistently as the port number will be included the OIDC issuer URL. We will explain details later.</div><p>You can provision RDS and S3 in AWS, or you can deploy Postgres and <a href=https://min.io/>MinIO</a> inside your EKS cluster.</p><h2 id=install-with-helm>Install with Helm<a class=td-heading-self-link href=#install-with-helm aria-label="Heading self-link"></a></h2><p>We provide a Helm chart for installing LLMariner. You can obtain the Helm chart from our repository and install.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Logout of helm registry to perform an unauthenticated pull against the public ECR</span>
</span></span><span class=line><span class=cl>helm registry <span class=nb>logout</span> public.ecr.aws
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace &lt;namespace&gt; <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  llmariner oci://public.ecr.aws/cloudnatix/llmariner-charts/llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --values &lt;values.yaml&gt;
</span></span></code></pre></div><p>Once installation completes, you can interact with the API endpoint using the <a href=https://github.com/openai/openai-python>OpenAI Python library</a>, running our CLI, or directly hitting the endpoint. To download the CLI, run:</p><ul class="nav nav-tabs" id=tabs-0 role=tablist><li class=nav-item><button class="nav-link disabled" id=tabs-00-00-tab data-bs-toggle=tab data-bs-target=#tabs-00-00 role=tab aria-controls=tabs-00-00 aria-selected=false>
<strong>Install From</strong>:</button></li><li class=nav-item><button class="nav-link active" id=tabs-00-01-tab data-bs-toggle=tab data-bs-target=#tabs-00-01 role=tab data-td-tp-persist=script aria-controls=tabs-00-01 aria-selected=true>
Script</button></li><li class=nav-item><button class=nav-link id=tabs-00-02-tab data-bs-toggle=tab data-bs-target=#tabs-00-02 role=tab data-td-tp-persist=homebrew aria-controls=tabs-00-02 aria-selected=false>
Homebrew</button></li><li class=nav-item><button class=nav-link id=tabs-00-03-tab data-bs-toggle=tab data-bs-target=#tabs-00-03 role=tab data-td-tp-persist=go aria-controls=tabs-00-03 aria-selected=false>
Go</button></li><li class=nav-item><button class=nav-link id=tabs-00-04-tab data-bs-toggle=tab data-bs-target=#tabs-00-04 role=tab data-td-tp-persist=binary aria-controls=tabs-00-04 aria-selected=false>
Binary</button></li></ul><div class=tab-content id=tabs-0-content><div class="tab-body tab-pane fade" id=tabs-00-00 role=tabpanel aria-labelled-by=tabs-00-00-tab tabindex=0></div><div class="tab-body tab-pane fade show active" id=tabs-00-01 role=tabpanel aria-labelled-by=tabs-00-01-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl --silent https://llmariner.ai/get-cli <span class=p>|</span> bash
</span></span><span class=line><span class=cl>mv llma &lt;your/PATH&gt;
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-02 role=tabpanel aria-labelled-by=tabs-00-02-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>brew install llmariner/tap/llma
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-03 role=tabpanel aria-labelled-by=tabs-00-03-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>go install github.com/llmariner/llmariner/cli/cmd@latest
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-04 role=tabpanel aria-labelled-by=tabs-00-04-tab tabindex=0><p>Download the binary from <a href=https://github.com/llmariner/llmariner/releases/latest>GitHub Release Page</a>.</p></div></div></div><div class=td-content style=page-break-before:always><h1 id=pg-fad99d871e69bfc31c6b63374b6793ea>2 - Set up a Playground on a GPU EC2 Instance</h1><div class=lead>Set up the playground environment on an Amazon EC2 instance with GPUs.</div><p>You can easily set up a playground for LLMariner and learn it. In this page, we provision an EC2 instance, build a <a href=https://kind.sigs.k8s.io/>Kind</a> cluster, and deploy LLMariner and other required components.</p><div class="alert alert-secondary" role=alert><h4 class=alert-heading>Warn</h4>Playground environments are for experimentation use only. For a production-ready installation, please refere to the other installation guide.</div><p>Once all the setup completes, you can interact with the LLM service by directly hitting the API endpoints or using <a href=https://github.com/openai/openai-python>the OpenAI Python library</a>.</p><h2 id=step-1-install-terraform-and-ansible>Step 1: Install Terraform and Ansible<a class=td-heading-self-link href=#step-1-install-terraform-and-ansible aria-label="Heading self-link"></a></h2><p>We use Terraform and Ansible. Follow the links to install if you haven't.</p><ul><li><a href=https://developer.hashicorp.com/terraform/install>Terraform</a></li><li><a href=https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html>Ansible</a></li><li><a href=https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html>kubernetes.core.k8s module for Ansible</a></li></ul><p>To install <code>kubernetes.core.k8s</code> module, run the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ansible-galaxy collection install kubernetes.core
</span></span></code></pre></div><h2 id=step-2-clone-the-llmariner-repository>Step 2: Clone the LLMariner Repository<a class=td-heading-self-link href=#step-2-clone-the-llmariner-repository aria-label="Heading self-link"></a></h2><p>We use the Terraform configuration and Ansible playbook in the <a href=https://github.com/llmariner/llmariner>LLMariner repository</a>. Run the following commands to clone the repo and move to the directory where the Terraform configuration file is stored.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/llmariner/llmariner.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> llmariner/provision/aws
</span></span></code></pre></div><h2 id=step-3-run-terraform>Step 3: Run Terraform<a class=td-heading-self-link href=#step-3-run-terraform aria-label="Heading self-link"></a></h2><p>First create a <code>local.tfvars</code> file for your deployment. Here is an example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-terraform data-lang=terraform><span class=line><span class=cl><span class=na>project_name</span> = <span class=s2>&#34;&lt;instance-name&gt; (default: &#34;</span><span class=nx>llmariner</span><span class=o>-</span><span class=nx>demo</span><span class=s2>&#34;)&#34;</span>
</span></span><span class=line><span class=cl><span class=na>profile</span>      = <span class=s2>&#34;&lt;aws-profile&gt;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=na>public_key_path</span>  = <span class=s2>&#34;&lt;/path/to/public_key_path&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=na>private_key_path</span> = <span class=s2>&#34;&lt;/path/to/private_key_path&gt;&#34;</span>
</span></span><span class=line><span class=cl><span class=na>ssh_ip_range</span>     = <span class=s2>&#34;&lt;ingress CIDR block for SSH (default: &#34;</span><span class=m>0</span><span class=p>.</span><span class=m>0</span><span class=p>.</span><span class=m>0</span><span class=p>.</span><span class=m>0</span><span class=o>/</span><span class=m>0</span><span class=s2>&#34;)&gt;&#34;</span>
</span></span></code></pre></div><p><code>profile</code> is an AWS profile that is used to create an EC2 instance. <code>public_key_path</code> and <code>private_key_path</code> specify an SSH key used to access the EC2 instance.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>See <code>variables.tf</code> for other customizable and default values.</div><p>Then, run the following Terraform commands to initialize and create an EC2 instance. This will approximately take 10 minutes.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>terraform init
</span></span><span class=line><span class=cl>terraform apply -var-file<span class=o>=</span>local.tfvars
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>If you want to run only the Ansible playbook, you can just run <code>ansible-playbook -i inventory.ini playbook.yml</code>.</div><p>Once the deployment completes, a Kind cluster is built in the EC2 instance and LLMariner is running in the cluster. It will take another about five minutes for LLMariner to load base models, but you can move to the next step meanwhile.</p><h2 id=step-4-set-up-ssh-connection>Step 4: Set up SSH Connection<a class=td-heading-self-link href=#step-4-set-up-ssh-connection aria-label="Heading self-link"></a></h2><p>You can access the API endpoint and Grafana by establishing SSH port-forwarding.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ansible all <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -i inventory.ini <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --ssh-extra-args<span class=o>=</span><span class=s2>&#34;-L8080:localhost:80 -L8081:localhost:8081&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -a <span class=s2>&#34;kubectl port-forward -n monitoring service/grafana 8081:80&#34;</span>
</span></span></code></pre></div><p>With the above command, you can hit the API via <code>http://localhost:8080</code>. You can directly hit the endpoint via <code>curl</code> or other commands, or you can use the <a href=https://github.com/openai/openai-python>OpenAI Python library</a>.</p><p>You can also reach Grafana at <code>http://localhost:8081</code>. The login username is <code>admin</code>, and the password can be obtained with the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ansible all <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -i inventory.ini <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -a <span class=s2>&#34;kubectl get secrets -n monitoring grafana -o jsonpath=&#39;{.data.admin-password}&#39;&#34;</span> <span class=p>|</span> tail -1 <span class=p>|</span> base64 --decode<span class=p>;</span> <span class=nb>echo</span>
</span></span></code></pre></div><h2 id=step-5-obtain-an-api-key>Step 5: Obtain an API Key<a class=td-heading-self-link href=#step-5-obtain-an-api-key aria-label="Heading self-link"></a></h2><p>To access LLM service, you need an API key. You can download the LLMariner CLI and use that to login the system, and obtain the API key.</p><ul class="nav nav-tabs" id=tabs-0 role=tablist><li class=nav-item><button class="nav-link disabled" id=tabs-00-00-tab data-bs-toggle=tab data-bs-target=#tabs-00-00 role=tab aria-controls=tabs-00-00 aria-selected=false>
<strong>Install From</strong>:</button></li><li class=nav-item><button class="nav-link active" id=tabs-00-01-tab data-bs-toggle=tab data-bs-target=#tabs-00-01 role=tab data-td-tp-persist=script aria-controls=tabs-00-01 aria-selected=true>
Script</button></li><li class=nav-item><button class=nav-link id=tabs-00-02-tab data-bs-toggle=tab data-bs-target=#tabs-00-02 role=tab data-td-tp-persist=homebrew aria-controls=tabs-00-02 aria-selected=false>
Homebrew</button></li><li class=nav-item><button class=nav-link id=tabs-00-03-tab data-bs-toggle=tab data-bs-target=#tabs-00-03 role=tab data-td-tp-persist=go aria-controls=tabs-00-03 aria-selected=false>
Go</button></li><li class=nav-item><button class=nav-link id=tabs-00-04-tab data-bs-toggle=tab data-bs-target=#tabs-00-04 role=tab data-td-tp-persist=binary aria-controls=tabs-00-04 aria-selected=false>
Binary</button></li></ul><div class=tab-content id=tabs-0-content><div class="tab-body tab-pane fade" id=tabs-00-00 role=tabpanel aria-labelled-by=tabs-00-00-tab tabindex=0></div><div class="tab-body tab-pane fade show active" id=tabs-00-01 role=tabpanel aria-labelled-by=tabs-00-01-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl --silent https://llmariner.ai/get-cli <span class=p>|</span> bash
</span></span><span class=line><span class=cl>mv llma &lt;your/PATH&gt;
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-02 role=tabpanel aria-labelled-by=tabs-00-02-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>brew install llmariner/tap/llma
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-03 role=tabpanel aria-labelled-by=tabs-00-03-tab tabindex=0><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>go install github.com/llmariner/llmariner/cli/cmd@latest
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-00-04 role=tabpanel aria-labelled-by=tabs-00-04-tab tabindex=0><p>Download the binary from <a href=https://github.com/llmariner/llmariner/releases/latest>GitHub Release Page</a>.</p></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Login. Please see below for the details.</span>
</span></span><span class=line><span class=cl>llma auth login
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create an API key.</span>
</span></span><span class=line><span class=cl>llma auth api-keys create my-key
</span></span></code></pre></div><p><code>llma auth login</code> will ask for the endpoint URL and the issuer URL. Please use the default values for them (<code>http://localhost:8080/v1</code> and <code>http://kong-proxy.kong/v1/dex</code>).</p><p>Then the command will open a web browser to login. Please use the following username and the password.</p><ul><li>Username: <code>admin@example.com</code></li><li>Password: <code>password</code></li></ul><p>The output of <code>llma auth api-keys create</code> contains the secret of the created API key. Please save the value in the environment variable to use that in the following step:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_TOKEN</span><span class=o>=</span>&lt;Secret obtained from llma auth api-keys create&gt;
</span></span></code></pre></div><h2 id=step-6-interact-with-the-llm-service>Step 6: Interact with the LLM Service<a class=td-heading-self-link href=#step-6-interact-with-the-llm-service aria-label="Heading self-link"></a></h2><p>There are mainly three ways to interact with the LLM service.</p><p>The first option is to use the CLI. Here are example commands:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma models list
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion <span class=s2>&#34;What is k8s?&#34;</span>
</span></span></code></pre></div><p>The second option is to run the <code>curl</code> command and hit the API endpoint. Here is an example command for listing all available models and hitting the chat endpoint.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:8080/v1/models <span class=p>|</span> jq
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Authorization: Bearer </span><span class=si>${</span><span class=nv>LLMARINER_TOKEN</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data <span class=s1>&#39;{&#34;model&#34;: &#34;google-gemma-2b-it-q4_0&#34;, &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is k8s?&#34;}]}&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:8080/v1/chat/completions
</span></span></code></pre></div><p>The third option is to use Python. Here is an example Python code for hitting the chat endpoint.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>os</span> <span class=kn>import</span> <span class=n>environ</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>base_url</span><span class=o>=</span><span class=s2>&#34;http://localhost:8080/v1&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>api_key</span><span class=o>=</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;LLMARINER_TOKEN&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>completion</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>chat</span><span class=o>.</span><span class=n>completions</span><span class=o>.</span><span class=n>create</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>model</span><span class=o>=</span><span class=s2>&#34;google-gemma-2b-it-q4_0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>messages</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;What is k8s?&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>stream</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>response</span> <span class=ow>in</span> <span class=n>completion</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>choices</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>delta</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=n>end</span><span class=o>=</span><span class=s2>&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Please visit <code>tutorials</code>{.interpreted-text role=&ldquo;doc&rdquo;} to further exercise LLMariner.</p><h2 id=step-7-clean-up>Step 7: Clean up<a class=td-heading-self-link href=#step-7-clean-up aria-label="Heading self-link"></a></h2><p>Run the following command to destroy the EC2 instance.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>terraform destroy -var-file<span class=o>=</span>local.tfvars
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2822b0bc47bba2d10518e57315f06e35>3 - Set up a Playground on a CPU-only Kind Cluster</h1><div class=lead>Set up the playground environment on a local kind cluster (CPU-only).</div><p>Following this guide provides you with a simplified, local LLMariner installation by using the Kind and Helm. You can use this simple LLMariner deployment to try out features without GPUs.</p><div class="alert alert-secondary" role=alert><h4 class=alert-heading>Warn</h4>Playground environments are for experimentation use only. For a production-ready installation, please refere to the other installation guide.</div><h2 id=before-you-begin>Before you begin<a class=td-heading-self-link href=#before-you-begin aria-label="Heading self-link"></a></h2><p>Before you can get started with the LLMariner deployment you must install:</p><ul><li><a href=https://kind.sigs.k8s.io/docs/user/quick-start>kind (Kubernetes in Docker)</a></li><li><a href=https://helmfile.readthedocs.io/en/latest/#installation>Helmfile</a></li></ul><h2 id=step-1-clone-the-repository>Step 1: Clone the repository<a class=td-heading-self-link href=#step-1-clone-the-repository aria-label="Heading self-link"></a></h2><p>To get started, clone the LLMariner repository.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/llmariner/llmariner.git
</span></span></code></pre></div><h2 id=step-2-create-a-kind-cluster>Step 2: Create a kind cluster<a class=td-heading-self-link href=#step-2-create-a-kind-cluster aria-label="Heading self-link"></a></h2><p>The installation files are in <code>provision/dev/</code>. Create a new Kubernetes cluster using kind by running:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>cd</span> provision/dev/
</span></span><span class=line><span class=cl>./create_cluster.sh single
</span></span></code></pre></div><h2 id=step-3-install-llmariner>Step 3: Install LLMariner<a class=td-heading-self-link href=#step-3-install-llmariner aria-label="Heading self-link"></a></h2><p>To install LLMariner using helmfile, run the following commands:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helmfile apply --skip-diff-on-install
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Tips</h4>You can filter the components to deploy using the <code>--selector(-l)</code> flag. For example, to filter out the monitoring components, set the <code>-l tier!=monitoring</code> flag. For deploying just the llmariner, use <code>-l app=llmariner</code>.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-8de5d0eb22b83ad8b3efcf34d11c8f4b>4 - Install in a Single EKS Cluster</h1><div class=lead>Install LLMariner in an EKS cluster with the standalone mode.</div><p>This page goes through the concrete steps to create an EKS cluster, create necessary resources, and install LLMariner. You can skip some of the steps if you have already made necessary installation/setup.</p><h2 id=step-1-provision-an-eks-cluster>Step 1. Provision an EKS cluster<a class=td-heading-self-link href=#step-1-provision-an-eks-cluster aria-label="Heading self-link"></a></h2><h3 id=step-11-create-a-new-cluster-with-karpenter>Step 1.1. Create a new cluster with Karpenter<a class=td-heading-self-link href=#step-11-create-a-new-cluster-with-karpenter aria-label="Heading self-link"></a></h3><p>Either follow the <a href=https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/>Karpenter getting started guide</a> and create an EKS cluster with Karpenter, or run the following simplified installation steps.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CLUSTER_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner-demo&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>AWS_DEFAULT_REGION</span><span class=o>=</span><span class=s2>&#34;us-east-1&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>AWS_ACCOUNT_ID</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span>aws sts get-caller-identity --query Account --output text<span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>KARPENTER_NAMESPACE</span><span class=o>=</span><span class=s2>&#34;kube-system&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>KARPENTER_VERSION</span><span class=o>=</span><span class=s2>&#34;1.0.1&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>K8S_VERSION</span><span class=o>=</span><span class=s2>&#34;1.30&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>TEMPOUT</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span>mktemp<span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>curl -fsSL https://raw.githubusercontent.com/aws/karpenter-provider-aws/v<span class=s2>&#34;</span><span class=si>${</span><span class=nv>KARPENTER_VERSION</span><span class=si>}</span><span class=s2>&#34;</span>/website/content/en/preview/getting-started/getting-started-with-karpenter/cloudformation.yaml  &gt; <span class=s2>&#34;</span><span class=si>${</span><span class=nv>TEMPOUT</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=o>&amp;&amp;</span> aws cloudformation deploy <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --stack-name <span class=s2>&#34;Karpenter-</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --template-file <span class=s2>&#34;</span><span class=si>${</span><span class=nv>TEMPOUT</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --capabilities CAPABILITY_NAMED_IAM <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --parameter-overrides <span class=s2>&#34;ClusterName=</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>eksctl create cluster -f - <span class=s>&lt;&lt;EOF
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: eksctl.io/v1alpha5
</span></span></span><span class=line><span class=cl><span class=s>kind: ClusterConfig
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: ${CLUSTER_NAME}
</span></span></span><span class=line><span class=cl><span class=s>  region: ${AWS_DEFAULT_REGION}
</span></span></span><span class=line><span class=cl><span class=s>  version: &#34;${K8S_VERSION}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  tags:
</span></span></span><span class=line><span class=cl><span class=s>    karpenter.sh/discovery: ${CLUSTER_NAME}
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>iam:
</span></span></span><span class=line><span class=cl><span class=s>  withOIDC: true
</span></span></span><span class=line><span class=cl><span class=s>  podIdentityAssociations:
</span></span></span><span class=line><span class=cl><span class=s>  - namespace: &#34;${KARPENTER_NAMESPACE}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    serviceAccountName: karpenter
</span></span></span><span class=line><span class=cl><span class=s>    roleName: ${CLUSTER_NAME}-karpenter
</span></span></span><span class=line><span class=cl><span class=s>    permissionPolicyARNs:
</span></span></span><span class=line><span class=cl><span class=s>    - arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME}
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>iamIdentityMappings:
</span></span></span><span class=line><span class=cl><span class=s>- arn: &#34;arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  username: system:node:{{EC2PrivateDNSName}}
</span></span></span><span class=line><span class=cl><span class=s>  groups:
</span></span></span><span class=line><span class=cl><span class=s>  - system:bootstrappers
</span></span></span><span class=line><span class=cl><span class=s>  - system:nodes
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>managedNodeGroups:
</span></span></span><span class=line><span class=cl><span class=s>- instanceType: m5.large
</span></span></span><span class=line><span class=cl><span class=s>  amiFamily: AmazonLinux2
</span></span></span><span class=line><span class=cl><span class=s>  name: ${CLUSTER_NAME}-ng
</span></span></span><span class=line><span class=cl><span class=s>  desiredCapacity: 2
</span></span></span><span class=line><span class=cl><span class=s>  minSize: 1
</span></span></span><span class=line><span class=cl><span class=s>  maxSize: 10
</span></span></span><span class=line><span class=cl><span class=s>addons:
</span></span></span><span class=line><span class=cl><span class=s>- name: eks-pod-identity-agent
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create the service linked role if it does not exist. Ignore an already-exists error.</span>
</span></span><span class=line><span class=cl>aws iam create-service-linked-role --aws-service-name spot.amazonaws.com <span class=o>||</span> <span class=nb>true</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Logout of helm registry to perform an unauthenticated pull against the public ECR.</span>
</span></span><span class=line><span class=cl>helm registry <span class=nb>logout</span> public.ecr.aws
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Deploy Karpenter.</span>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace <span class=s2>&#34;</span><span class=si>${</span><span class=nv>KARPENTER_NAMESPACE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  karpenter oci://public.ecr.aws/karpenter/karpenter <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version <span class=s2>&#34;</span><span class=si>${</span><span class=nv>KARPENTER_VERSION</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=s2>&#34;settings.clusterName=</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=s2>&#34;settings.interruptionQueue=</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set controller.resources.requests.cpu<span class=o>=</span><span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set controller.resources.requests.memory<span class=o>=</span>1Gi <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set controller.resources.limits.cpu<span class=o>=</span><span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set controller.resources.limits.memory<span class=o>=</span>1Gi
</span></span></code></pre></div><h3 id=step-12-provision-gpu-nodes>Step 1.2. Provision GPU nodes<a class=td-heading-self-link href=#step-12-provision-gpu-nodes aria-label="Heading self-link"></a></h3><p>Once Karpenter is installed, we need to create an <code>EC2NodeClass</code> and a <code>NodePool</code> so that GPU nodes are provisioned. We configure <code>blockDeviceMappings</code> in the <code>EC2NodeClass</code> definition so that nodes have sufficient local storage to store model files.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>GPU_AMI_ID</span><span class=o>=</span><span class=s2>&#34;</span><span class=k>$(</span>aws ssm get-parameter --name /aws/service/eks/optimized-ami/<span class=si>${</span><span class=nv>K8S_VERSION</span><span class=si>}</span>/amazon-linux-2-gpu/recommended/image_id --query Parameter.Value --output text<span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst | kubectl apply -f -
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: karpenter.sh/v1
</span></span></span><span class=line><span class=cl><span class=s>kind: NodePool
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: default
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  template:
</span></span></span><span class=line><span class=cl><span class=s>    spec:
</span></span></span><span class=line><span class=cl><span class=s>      requirements:
</span></span></span><span class=line><span class=cl><span class=s>      - key: kubernetes.io/arch
</span></span></span><span class=line><span class=cl><span class=s>        operator: In
</span></span></span><span class=line><span class=cl><span class=s>        values: [&#34;amd64&#34;]
</span></span></span><span class=line><span class=cl><span class=s>      - key: kubernetes.io/os
</span></span></span><span class=line><span class=cl><span class=s>        operator: In
</span></span></span><span class=line><span class=cl><span class=s>        values: [&#34;linux&#34;]
</span></span></span><span class=line><span class=cl><span class=s>      - key: karpenter.sh/capacity-type
</span></span></span><span class=line><span class=cl><span class=s>        operator: In
</span></span></span><span class=line><span class=cl><span class=s>        values: [&#34;on-demand&#34;]
</span></span></span><span class=line><span class=cl><span class=s>      - key: karpenter.k8s.aws/instance-family
</span></span></span><span class=line><span class=cl><span class=s>        operator: In
</span></span></span><span class=line><span class=cl><span class=s>        values: [&#34;g5&#34;]
</span></span></span><span class=line><span class=cl><span class=s>      nodeClassRef:
</span></span></span><span class=line><span class=cl><span class=s>        group: karpenter.k8s.aws
</span></span></span><span class=line><span class=cl><span class=s>        kind: EC2NodeClass
</span></span></span><span class=line><span class=cl><span class=s>        name: default
</span></span></span><span class=line><span class=cl><span class=s>      expireAfter: 720h
</span></span></span><span class=line><span class=cl><span class=s>  disruption:
</span></span></span><span class=line><span class=cl><span class=s>    consolidationPolicy: WhenEmptyOrUnderutilized
</span></span></span><span class=line><span class=cl><span class=s>    consolidateAfter: 1m
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: karpenter.k8s.aws/v1
</span></span></span><span class=line><span class=cl><span class=s>kind: EC2NodeClass
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: default
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  amiFamily: AL2
</span></span></span><span class=line><span class=cl><span class=s>  role: &#34;KarpenterNodeRole-${CLUSTER_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  subnetSelectorTerms:
</span></span></span><span class=line><span class=cl><span class=s>  - tags:
</span></span></span><span class=line><span class=cl><span class=s>      karpenter.sh/discovery: &#34;${CLUSTER_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  securityGroupSelectorTerms:
</span></span></span><span class=line><span class=cl><span class=s>  - tags:
</span></span></span><span class=line><span class=cl><span class=s>      karpenter.sh/discovery: &#34;${CLUSTER_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  amiSelectorTerms:
</span></span></span><span class=line><span class=cl><span class=s>  - id: &#34;${GPU_AMI_ID}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  blockDeviceMappings:
</span></span></span><span class=line><span class=cl><span class=s>  - deviceName: /dev/xvda
</span></span></span><span class=line><span class=cl><span class=s>    ebs:
</span></span></span><span class=line><span class=cl><span class=s>      deleteOnTermination: true
</span></span></span><span class=line><span class=cl><span class=s>      encrypted: true
</span></span></span><span class=line><span class=cl><span class=s>      volumeSize: 256Gi
</span></span></span><span class=line><span class=cl><span class=s>      volumeType: gp3
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span></code></pre></div><h3 id=step-13-install-nvidia-gpu-operator>Step 1.3. Install Nvidia GPU Operator<a class=td-heading-self-link href=#step-13-install-nvidia-gpu-operator aria-label="Heading self-link"></a></h3><p>Nvidia GPU Operator is required to install the device plugin and make GPU resources visible in the K8s cluster. Run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace nvidia <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  gpu-operator nvidia/gpu-operator <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set cdi.enabled<span class=o>=</span><span class=nb>true</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set driver.enabled<span class=o>=</span><span class=nb>false</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set toolkit.enabled<span class=o>=</span><span class=nb>false</span>
</span></span></code></pre></div><h3 id=step-14-install-an-ingress-controller>Step 1.4. Install an ingress controller<a class=td-heading-self-link href=#step-14-install-an-ingress-controller aria-label="Heading self-link"></a></h3><p>An ingress controller is required to route HTTP/HTTPS requests to the LLMariner components. Any ingress controller works, and you can skip this step if your EKS cluster already has an ingress controller.</p><p>Here is an example that installs <a href=https://konghq.com/>Kong</a> and make the ingress controller reachable via AWS loadbalancer:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add kong https://charts.konghq.com
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace kong <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  kong-proxy kong/kong <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set proxy.annotations.service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout<span class=o>=</span><span class=m>300</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set ingressController.installCRDs<span class=o>=</span><span class=nb>false</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=nv>fullnameOverride</span><span class=o>=</span><span class=nb>false</span>
</span></span></code></pre></div><h2 id=step-2-create-an-rds-instance>Step 2. Create an RDS instance<a class=td-heading-self-link href=#step-2-create-an-rds-instance aria-label="Heading self-link"></a></h2><p>We will create an RDS in the same VPC as the EKS cluster so that it can be reachable from the LLMariner components. Here are example commands for creating a DB subnet group:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>DB_SUBNET_GROUP_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner-demo-db-subnet&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EKS_SUBNET_IDS</span><span class=o>=</span><span class=k>$(</span>aws eks describe-cluster --name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.cluster.resourcesVpcConfig.subnetIds | join(&#34; &#34;)&#39;</span> --raw-output<span class=k>)</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EKS_SUBNET_ID0</span><span class=o>=</span><span class=k>$(</span><span class=nb>echo</span> <span class=si>${</span><span class=nv>EKS_SUBNET_IDS</span><span class=si>}</span> <span class=p>|</span> cut -d<span class=s1>&#39; &#39;</span> -f1<span class=k>)</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EKS_SUBNET_ID1</span><span class=o>=</span><span class=k>$(</span><span class=nb>echo</span> <span class=si>${</span><span class=nv>EKS_SUBNET_IDS</span><span class=si>}</span> <span class=p>|</span> cut -d<span class=s1>&#39; &#39;</span> -f2<span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws rds create-db-subnet-group <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-subnet-group-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_SUBNET_GROUP_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-subnet-group-description <span class=s2>&#34;LLMariner Demo&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --subnet-ids <span class=s2>&#34;</span><span class=si>${</span><span class=nv>EKS_SUBNET_ID0</span><span class=si>}</span><span class=s2>&#34;</span> <span class=s2>&#34;</span><span class=si>${</span><span class=nv>EKS_SUBNET_ID1</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>and an RDS instance:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>DB_INSTANCE_ID</span><span class=o>=</span><span class=s2>&#34;llmariner-demo&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_USER</span><span class=o>=</span><span class=s2>&#34;admin_user&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_PASSWORD</span><span class=o>=</span><span class=s2>&#34;secret_password&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EKS_SECURITY_GROUP_ID</span><span class=o>=</span><span class=k>$(</span>aws eks describe-cluster --name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.cluster.resourcesVpcConfig.clusterSecurityGroupId&#39;</span> --raw-output<span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws rds create-db-instance <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-instance-identifier <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_INSTANCE_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-instance-class db.t3.small <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --engine postgres <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --allocated-storage <span class=m>10</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --storage-encrypted <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --master-username <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_USER</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --master-user-password <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_PASSWORD</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --vpc-security-group-ids <span class=s2>&#34;</span><span class=si>${</span><span class=nv>EKS_SECURITY_GROUP_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --db-subnet-group-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_SUBNET_GROUP_NAME</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>You can run the following command to check the provisioning status.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>aws rds describe-db-instances --db-instance-identifier <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_INSTANCE_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.DBInstances[].DBInstanceStatus&#39;</span>
</span></span></code></pre></div><p>Once the RDS instance is fully provisioned and its status becomes <code>available</code>, obtain the endpoint information for later use.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_ADDR</span><span class=o>=</span><span class=k>$(</span>aws rds describe-db-instances --db-instance-identifier <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_INSTANCE_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.DBInstances[].Endpoint.Address&#39;</span> --raw-output<span class=k>)</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_PORT</span><span class=o>=</span><span class=k>$(</span>aws rds describe-db-instances --db-instance-identifier <span class=s2>&#34;</span><span class=si>${</span><span class=nv>DB_INSTANCE_ID</span><span class=si>}</span><span class=s2>&#34;</span> <span class=p>|</span> jq <span class=s1>&#39;.DBInstances[].Endpoint.Port&#39;</span> --raw-output<span class=k>)</span>
</span></span></code></pre></div><p>You can verify if the DB instance is reachable from the EKS cluster by running the <code>psql</code> command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl run psql --image jbergknoff/postgresql-client --env<span class=o>=</span><span class=s2>&#34;PGPASSWORD=</span><span class=si>${</span><span class=nv>POSTGRES_PASSWORD</span><span class=si>}</span><span class=s2>&#34;</span> -- -h <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_ADDR</span><span class=si>}</span><span class=s2>&#34;</span> -U <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_USER</span><span class=si>}</span><span class=s2>&#34;</span> -p <span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_PORT</span><span class=si>}</span><span class=s2>&#34;</span> -d template1 -c <span class=s2>&#34;select now();&#34;</span>
</span></span><span class=line><span class=cl>kubectl logs psql
</span></span><span class=line><span class=cl>kubectl delete pods psql
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>LLMariner will create additional databases on the fly for each API service (e.g., <code>job_manager</code>, <code>model_manager</code>). You can see all created databases by running <code>SELECT count(datname) FROM pg_database;</code>.</div><h2 id=step-3-create-an-s3-bucket>Step 3. Create an S3 bucket<a class=td-heading-self-link href=#step-3-create-an-s3-bucket aria-label="Heading self-link"></a></h2><p>We will create an S3 bucket where model files are stored. Here is an example</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Please change the bucket name to something else.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_BUCKET_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner-demo&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_REGION</span><span class=o>=</span><span class=s2>&#34;us-east-1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws s3api create-bucket --bucket <span class=s2>&#34;</span><span class=si>${</span><span class=nv>S3_BUCKET_NAME</span><span class=si>}</span><span class=s2>&#34;</span> --region <span class=s2>&#34;</span><span class=si>${</span><span class=nv>S3_REGION</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>If you want to set up Milvus for RAG, please create another S3 bucket for Milvus:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Please change the bucket name to something else.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>MILVUS_S3_BUCKET_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner-demo-milvus&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws s3api create-bucket --bucket <span class=s2>&#34;</span><span class=si>${</span><span class=nv>MILVUS_S3_BUCKET_NAME</span><span class=si>}</span><span class=s2>&#34;</span> --region <span class=s2>&#34;</span><span class=si>${</span><span class=nv>S3_REGION</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>Pods running in the EKS cluster need to be able to access the S3 bucket. We will create an <a href=https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html>IAM role for service account</a> for that.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_NAMESPACE</span><span class=o>=</span>llmariner
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_POLICY</span><span class=o>=</span><span class=s2>&#34;LLMarinerPolicy&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_SERVICE_ACCOUNT_NAME</span><span class=o>=</span><span class=s2>&#34;llmariner&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LLMARINER_ROLE</span><span class=o>=</span><span class=s2>&#34;LLMarinerRole&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst &gt; policy.json
</span></span></span><span class=line><span class=cl><span class=s>{
</span></span></span><span class=line><span class=cl><span class=s>  &#34;Version&#34;: &#34;2012-10-17&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;Statement&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>    {
</span></span></span><span class=line><span class=cl><span class=s>      &#34;Effect&#34;: &#34;Allow&#34;,
</span></span></span><span class=line><span class=cl><span class=s>      &#34;Action&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>        &#34;s3:PutObject&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;s3:GetObject&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;s3:DeleteObject&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;s3:ListBucket&#34;
</span></span></span><span class=line><span class=cl><span class=s>      ],
</span></span></span><span class=line><span class=cl><span class=s>      &#34;Resource&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>        &#34;arn:aws:s3:::${S3_BUCKET_NAME}/*&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;arn:aws:s3:::${S3_BUCKET_NAME}&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;arn:aws:s3:::${MILVUS_S3_BUCKET_NAME}/*&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;arn:aws:s3:::${MILVUS_S3_BUCKET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      ]
</span></span></span><span class=line><span class=cl><span class=s>    }
</span></span></span><span class=line><span class=cl><span class=s>  ]
</span></span></span><span class=line><span class=cl><span class=s>}
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>aws iam create-policy --policy-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>LLMARINER_POLICY</span><span class=si>}</span><span class=s2>&#34;</span> --policy-document file://policy.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>eksctl create iamserviceaccount <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>LLMARINER_SERVICE_ACCOUNT_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace <span class=s2>&#34;</span><span class=si>${</span><span class=nv>LLMARINER_NAMESPACE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cluster <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --role-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>LLMARINER_ROLE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --attach-policy-arn <span class=s2>&#34;arn:aws:iam::</span><span class=si>${</span><span class=nv>AWS_ACCOUNT_ID</span><span class=si>}</span><span class=s2>:policy/</span><span class=si>${</span><span class=nv>LLMARINER_POLICY</span><span class=si>}</span><span class=s2>&#34;</span> --approve
</span></span></code></pre></div><h2 id=step-4-install-milvus>Step 4. Install Milvus<a class=td-heading-self-link href=#step-4-install-milvus aria-label="Heading self-link"></a></h2><p>Install <a href=https://milvus.io/>Milvus</a> as it is used a backend vector database for RAG.</p><p>Milvus creates Persistent Volumes. Follow <a href=https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html>https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html</a> and install EBS CSI driver.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>EBS_CSI_DRIVER_ROLE</span><span class=o>=</span><span class=s2>&#34;AmazonEKS_EBS_CSI_DriverRole&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>eksctl create iamserviceaccount <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name ebs-csi-controller-sa <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace kube-system <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cluster <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --role-name <span class=s2>&#34;</span><span class=si>${</span><span class=nv>EBS_CSI_DRIVER_ROLE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --role-only <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --approve
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>eksctl create addon <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cluster <span class=s2>&#34;</span><span class=si>${</span><span class=nv>CLUSTER_NAME</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --name aws-ebs-csi-driver <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --version latest <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --service-account-role-arn <span class=s2>&#34;arn:aws:iam::</span><span class=si>${</span><span class=nv>AWS_ACCOUNT_ID</span><span class=si>}</span><span class=s2>:role/</span><span class=si>${</span><span class=nv>EBS_CSI_DRIVER_ROLE</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --force
</span></span></code></pre></div><p>Then install the Helm chart. Milvus requires access to the S3 bucket. To use the same service account created above, we deploy Milvus in the same namespace as LLMariner.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst &gt; milvus-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>cluster:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>etcd:
</span></span></span><span class=line><span class=cl><span class=s>  replicaCount: 1
</span></span></span><span class=line><span class=cl><span class=s>  persistence:
</span></span></span><span class=line><span class=cl><span class=s>    storageClass: gp2 # Use gp3 if available
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>pulsar:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>minio:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>standalone:
</span></span></span><span class=line><span class=cl><span class=s>  persistence:
</span></span></span><span class=line><span class=cl><span class=s>    persistentVolumeClaim:
</span></span></span><span class=line><span class=cl><span class=s>      storageClass: gp2 # Use gp3 if available
</span></span></span><span class=line><span class=cl><span class=s>      size: 10Gi
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>  create: false
</span></span></span><span class=line><span class=cl><span class=s>  name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>externalS3:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: true
</span></span></span><span class=line><span class=cl><span class=s>  host: s3.us-east-1.amazonaws.com
</span></span></span><span class=line><span class=cl><span class=s>  port: 443
</span></span></span><span class=line><span class=cl><span class=s>  useSSL: true
</span></span></span><span class=line><span class=cl><span class=s>  bucketName: &#34;${MILVUS_S3_BUCKET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  useIAM: true
</span></span></span><span class=line><span class=cl><span class=s>  cloudProvider: aws
</span></span></span><span class=line><span class=cl><span class=s>  iamEndpoint: &#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s>  logLevel: info
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm repo add zilliztech https://zilliztech.github.io/milvus-helm/
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace milvus <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  milvus zilliztech/milvus <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f milvus-values.yaml
</span></span></code></pre></div><p>Please see the <a href=https://milvus.io/docs/install-overview.md>Milvus installation document</a> and the <a href=https://artifacthub.io/packages/helm/milvus/milvus>Helm chart</a> for other installation options.</p><p>Set the environmental variables so that LLMariner can later access the Postgres database.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>MILVUS_ADDR</span><span class=o>=</span>milvus.milvus
</span></span></code></pre></div><h2 id=step-5-install-llmariner>Step 5. Install LLMariner<a class=td-heading-self-link href=#step-5-install-llmariner aria-label="Heading self-link"></a></h2><p>Run the following command to set up a <code>values.yaml</code> and install LLMariner with Helm.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Set the endpoint URL of LLMariner. Please change if you are using a different ingress controller.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>INGRESS_CONTROLLER_URL</span><span class=o>=</span>http://<span class=k>$(</span>kubectl get services -n kong kong-proxy-kong-proxy  -o <span class=nv>jsonpath</span><span class=o>=</span><span class=s1>&#39;{.status.loadBalancer.ingress[0].hostname}&#39;</span><span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst &gt; llmariner-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>global:
</span></span></span><span class=line><span class=cl><span class=s>  # This is an ingress configuration with Kong. Please change if you are using a different ingress controller.
</span></span></span><span class=line><span class=cl><span class=s>  ingress:
</span></span></span><span class=line><span class=cl><span class=s>    ingressClassName: kong
</span></span></span><span class=line><span class=cl><span class=s>    # The URL of the ingress controller. this can be a port-forwarding URL (e.g., http://localhost:8080) if there is
</span></span></span><span class=line><span class=cl><span class=s>    # no URL that is reachable from the outside of the EKS cluster.
</span></span></span><span class=line><span class=cl><span class=s>    controllerUrl: &#34;${INGRESS_CONTROLLER_URL}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    annotations:
</span></span></span><span class=line><span class=cl><span class=s>      # To remove the buffering from the streaming output of chat completion.
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/response-buffering: &#34;false&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  database:
</span></span></span><span class=line><span class=cl><span class=s>    host: &#34;${POSTGRES_ADDR}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    port: ${POSTGRES_PORT}
</span></span></span><span class=line><span class=cl><span class=s>    username: &#34;${POSTGRES_USER}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    ssl:
</span></span></span><span class=line><span class=cl><span class=s>      mode: require
</span></span></span><span class=line><span class=cl><span class=s>    createDatabase: true
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  databaseSecret:
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${POSTGRES_SECRET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    key: password
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  objectStore:
</span></span></span><span class=line><span class=cl><span class=s>    s3:
</span></span></span><span class=line><span class=cl><span class=s>      bucket: &#34;${S3_BUCKET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      region: &#34;${S3_REGION}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      endpointUrl: &#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>prepare:
</span></span></span><span class=line><span class=cl><span class=s>  database:
</span></span></span><span class=line><span class=cl><span class=s>    createSecret: true
</span></span></span><span class=line><span class=cl><span class=s>    secret:
</span></span></span><span class=line><span class=cl><span class=s>      password: &#34;${POSTGRES_PASSWORD}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>dex-server:
</span></span></span><span class=line><span class=cl><span class=s>  staticPasswords:
</span></span></span><span class=line><span class=cl><span class=s>  - email: admin@example.com
</span></span></span><span class=line><span class=cl><span class=s>    # bcrypt hash of the string: $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)
</span></span></span><span class=line><span class=cl><span class=s>    hash: &#34;\$2a\$10\$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W&#34;
</span></span></span><span class=line><span class=cl><span class=s>    username: admin-user
</span></span></span><span class=line><span class=cl><span class=s>    userID: admin-id
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>file-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>inference-manager-engine:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  model:
</span></span></span><span class=line><span class=cl><span class=s>    default:
</span></span></span><span class=line><span class=cl><span class=s>      runtimeName: vllm
</span></span></span><span class=line><span class=cl><span class=s>      preloaded: true
</span></span></span><span class=line><span class=cl><span class=s>      resources:
</span></span></span><span class=line><span class=cl><span class=s>        limits:
</span></span></span><span class=line><span class=cl><span class=s>          nvidia.com/gpu: 1
</span></span></span><span class=line><span class=cl><span class=s>    overrides:
</span></span></span><span class=line><span class=cl><span class=s>      meta-llama/Meta-Llama-3.1-8B-Instruct-q4_0:
</span></span></span><span class=line><span class=cl><span class=s>        contextLength: 16384
</span></span></span><span class=line><span class=cl><span class=s>      google/gemma-2b-it-q4_0:
</span></span></span><span class=line><span class=cl><span class=s>        runtimeName: ollama
</span></span></span><span class=line><span class=cl><span class=s>        resources:
</span></span></span><span class=line><span class=cl><span class=s>         limits:
</span></span></span><span class=line><span class=cl><span class=s>           nvidia.com/gpu: 0
</span></span></span><span class=line><span class=cl><span class=s>      sentence-transformers/all-MiniLM-L6-v2-f16:
</span></span></span><span class=line><span class=cl><span class=s>        runtimeName: ollama
</span></span></span><span class=line><span class=cl><span class=s>        resources:
</span></span></span><span class=line><span class=cl><span class=s>         limits:
</span></span></span><span class=line><span class=cl><span class=s>           nvidia.com/gpu: 0
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>inference-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  service:
</span></span></span><span class=line><span class=cl><span class=s>    annotations:
</span></span></span><span class=line><span class=cl><span class=s>      # These annotations are only meaningful for Kong ingress controller to extend the timeout.
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/connect-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/read-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/write-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>job-manager-dispatcher:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  notebook:
</span></span></span><span class=line><span class=cl><span class=s>    # Used to set the base URL of the API endpoint. This can be same as global.ingress.controllerUrl
</span></span></span><span class=line><span class=cl><span class=s>    # if the URL is reachable from the inside cluster. Otherwise you can change this to the
</span></span></span><span class=line><span class=cl><span class=s>    # to the URL of the ingress controller that is reachable inside the K8s cluster.
</span></span></span><span class=line><span class=cl><span class=s>    llmarinerBaseUrl: &#34;${INGRESS_CONTROLLER_URL}/v1&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>model-manager-loader:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  baseModels:
</span></span></span><span class=line><span class=cl><span class=s>  - meta-llama/Meta-Llama-3.1-8B-Instruct-q4_0
</span></span></span><span class=line><span class=cl><span class=s>  - google/gemma-2b-it-q4_0
</span></span></span><span class=line><span class=cl><span class=s>  - sentence-transformers/all-MiniLM-L6-v2-f16
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s># Required when RAG is used.
</span></span></span><span class=line><span class=cl><span class=s>vector-store-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  vectorDatabase:
</span></span></span><span class=line><span class=cl><span class=s>    host: &#34;${MILVUS_ADDR}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  llmEngineAddr: ollama-sentence-transformers-all-minilm-l6-v2-f16:11434
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  llmariner oci://public.ecr.aws/cloudnatix/llmariner-charts/llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f llmariner-values.yaml
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Starting from Helm v3.8.0, the OCI registry is supported by default. If you are using an older version, please upgrade to v3.8.0 or later. For more details, please refer to <a href=https://helm.sh/docs/topics/registries/>Helm OCI-based registries</a>.</div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>If you are getting a 403 forbidden error, please try <code>docker logout public.ecr.aws</code>. Please see <a href=https://docs.aws.amazon.com/AmazonECR/latest/public/public-troubleshooting.html>AWS document</a> for more details.</div><p>If you would like to install only the control-plane components or the worker-plane components, please see <code>multi_cluster_deployment</code>{.interpreted-text role=&ldquo;doc&rdquo;}.</p><h2 id=step-6-verify-the-installation>Step 6. Verify the installation<a class=td-heading-self-link href=#step-6-verify-the-installation aria-label="Heading self-link"></a></h2><p>You can verify the installation by sending sample chat completion requests.</p><p>Note, if you have used LLMariner in other cases before you may need to delete the previous config by running <code>rm -rf ~/.config/llmariner</code></p><p>The default login user name is <code>admin@example.com</code> and the password is
<code>password</code>. You can change this by updating the Dex configuration
(<a href=/docs/features/user_management/>link</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;This is your endpoint URL: </span><span class=si>${</span><span class=nv>INGRESS_CONTROLLER_URL</span><span class=si>}</span><span class=s2>/v1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma auth login
</span></span><span class=line><span class=cl><span class=c1># Type the above endpoint URL.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma models list
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion <span class=s2>&#34;what is k8s?&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model meta-llama-Meta-Llama-3.1-8B-Instruct-q4_0 --role user --completion <span class=s2>&#34;hello&#34;</span>
</span></span></code></pre></div><h2 id=optional-monitor-gpu-utilization>Optional: Monitor GPU utilization<a class=td-heading-self-link href=#optional-monitor-gpu-utilization aria-label="Heading self-link"></a></h2><p>If you would like to install Prometheus and Grafana to see GPU utilization, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Add Prometheus</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; prom-scrape-configs.yaml
</span></span></span><span class=line><span class=cl><span class=s>- job_name: nvidia-dcgm
</span></span></span><span class=line><span class=cl><span class=s>  scrape_interval: 5s
</span></span></span><span class=line><span class=cl><span class=s>  static_configs:
</span></span></span><span class=line><span class=cl><span class=s>  - targets: [&#39;nvidia-dcgm-exporter.nvidia.svc:9400&#39;]
</span></span></span><span class=line><span class=cl><span class=s>- job_name: inference-manager-engine-metrics
</span></span></span><span class=line><span class=cl><span class=s>  scrape_interval: 5s
</span></span></span><span class=line><span class=cl><span class=s>  static_configs:
</span></span></span><span class=line><span class=cl><span class=s>  - targets: [&#39;inference-manager-server-http.llmariner.svc:8083&#39;]
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace monitoring <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set-file <span class=nv>extraScrapeConfigs</span><span class=o>=</span>prom-scrape-configs.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  prometheus prometheus-community/prometheus
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add Grafana with DCGM dashboard</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; grafana-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>datasources:
</span></span></span><span class=line><span class=cl><span class=s> datasources.yaml:
</span></span></span><span class=line><span class=cl><span class=s>   apiVersion: 1
</span></span></span><span class=line><span class=cl><span class=s>   datasources:
</span></span></span><span class=line><span class=cl><span class=s>   - name: Prometheus
</span></span></span><span class=line><span class=cl><span class=s>     type: prometheus
</span></span></span><span class=line><span class=cl><span class=s>     url: http://prometheus-server
</span></span></span><span class=line><span class=cl><span class=s>     isDefault: true
</span></span></span><span class=line><span class=cl><span class=s>dashboardProviders:
</span></span></span><span class=line><span class=cl><span class=s>  dashboardproviders.yaml:
</span></span></span><span class=line><span class=cl><span class=s>    apiVersion: 1
</span></span></span><span class=line><span class=cl><span class=s>    providers:
</span></span></span><span class=line><span class=cl><span class=s>    - name: &#39;default&#39;
</span></span></span><span class=line><span class=cl><span class=s>      orgId: 1
</span></span></span><span class=line><span class=cl><span class=s>      folder: &#39;default&#39;
</span></span></span><span class=line><span class=cl><span class=s>      type: file
</span></span></span><span class=line><span class=cl><span class=s>      disableDeletion: true
</span></span></span><span class=line><span class=cl><span class=s>      editable: true
</span></span></span><span class=line><span class=cl><span class=s>      options:
</span></span></span><span class=line><span class=cl><span class=s>        path: /var/lib/grafana/dashboards/standard
</span></span></span><span class=line><span class=cl><span class=s>dashboards:
</span></span></span><span class=line><span class=cl><span class=s>  default:
</span></span></span><span class=line><span class=cl><span class=s>    nvidia-dcgm-exporter:
</span></span></span><span class=line><span class=cl><span class=s>      gnetId: 12239
</span></span></span><span class=line><span class=cl><span class=s>      datasource: Prometheus
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>helm repo add grafana https://grafana.github.io/helm-charts
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace monitoring <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f grafana-values.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  grafana grafana/grafana
</span></span></code></pre></div><h2 id=optional-enable-tls>Optional: Enable TLS<a class=td-heading-self-link href=#optional-enable-tls aria-label="Heading self-link"></a></h2><p>First follow the <a href=https://cert-manager.io/docs/installation/>cert-manager installation document</a> and install cert-manager to your K8s cluster if you don&rsquo;t have one. Then create a <code>ClusterIssuer</code> for your domain. Here is an example manifest that uses Let's Encrypt.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert-manager.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIssuer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>acme</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>server</span><span class=p>:</span><span class=w> </span><span class=l>https://acme-v02.api.letsencrypt.org/directory</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>user@mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>privateKeySecretRef</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>solvers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>http01</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>       </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>ingressClassName</span><span class=p>:</span><span class=w> </span><span class=l>kong</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>dnsZones</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dns01</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div><p>Then you can add the following to <code>values.yaml</code> of LLMariner to enable TLS.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span></code></pre></div><p>The ingresses created from the Helm chart will have the following annotation and spec:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nn>...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-b97c20cd3e745f78416fc8eddf907efc>5 - Install in a Single On-premise Cluster</h1><div class=lead>Install LLMariner in an on-premise Kubernetes cluster with the standalone mode.</div><p>This page goes through the concrete steps to install LLMariener on a on-premise K8s cluster (or a local K8s cluster).
You can skip some of the steps if you have already made necessary installation/setup.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Installation of Postgres, MinIO, SeawoodFS, and Milvus are just example purposes, and
they are not intended for the production usage.
Please configure based on your requirements if you want to use LLMariner for your production environment.</div><h2 id=step-1-install-nvidia-gpu-operator>Step 1. Install Nvidia GPU Operator<a class=td-heading-self-link href=#step-1-install-nvidia-gpu-operator aria-label="Heading self-link"></a></h2><p>Nvidia GPU Operator is required to install the device plugin and make GPU resources visible in the K8s cluster. Run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace nvidia <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  gpu-operator nvidia/gpu-operator <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set cdi.enabled<span class=o>=</span><span class=nb>true</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set driver.enabled<span class=o>=</span><span class=nb>false</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set toolkit.enabled<span class=o>=</span><span class=nb>false</span>
</span></span></code></pre></div><h2 id=step-2-install-an-ingress-controller>Step 2. Install an ingress controller<a class=td-heading-self-link href=#step-2-install-an-ingress-controller aria-label="Heading self-link"></a></h2><p>An ingress controller is required to route HTTP/HTTPS requests to the LLMariner components. Any ingress controller works, and you can skip this step if your EKS cluster already has an ingress controller.</p><p>Here is an example that installs <a href=https://konghq.com/>Kong</a> and make the ingress controller:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm repo add kong https://charts.konghq.com
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; kong-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>proxy:
</span></span></span><span class=line><span class=cl><span class=s> type: NodePort
</span></span></span><span class=line><span class=cl><span class=s> http:
</span></span></span><span class=line><span class=cl><span class=s>   hostPort: 80
</span></span></span><span class=line><span class=cl><span class=s> tls:
</span></span></span><span class=line><span class=cl><span class=s>   hostPort: 443
</span></span></span><span class=line><span class=cl><span class=s> annotations:
</span></span></span><span class=line><span class=cl><span class=s>   service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: &#34;300&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>nodeSelector:
</span></span></span><span class=line><span class=cl><span class=s>  ingress-ready: &#34;true&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>tolerations:
</span></span></span><span class=line><span class=cl><span class=s>- key: node-role.kubernetes.io/control-plane
</span></span></span><span class=line><span class=cl><span class=s>  operator: Equal
</span></span></span><span class=line><span class=cl><span class=s>  effect: NoSchedule
</span></span></span><span class=line><span class=cl><span class=s>- key: node-role.kubernetes.io/master
</span></span></span><span class=line><span class=cl><span class=s>  operator: Equal
</span></span></span><span class=line><span class=cl><span class=s>  effect: NoSchedule
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>fullnameOverride: kong
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace kong <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  kong-proxy kong/kong <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f kong-values.yaml
</span></span></code></pre></div><h2 id=step-3-install-a-postgres-database>Step 3. Install a Postgres database<a class=td-heading-self-link href=#step-3-install-a-postgres-database aria-label="Heading self-link"></a></h2><p>Run the following to deploy an Postgres deployment:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_USER</span><span class=o>=</span><span class=s2>&#34;admin_user&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_PASSWORD</span><span class=o>=</span><span class=s2>&#34;secret_password&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace postgres <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  postgres oci://registry-1.docker.io/bitnamicharts/postgresql <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=nv>nameOverride</span><span class=o>=</span>postgres <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.database<span class=o>=</span>ps_db <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.username<span class=o>=</span><span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_USER</span><span class=si>}</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.password<span class=o>=</span><span class=s2>&#34;</span><span class=si>${</span><span class=nv>POSTGRES_PASSWORD</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>Set the environmental variables so that LLMariner can later access the Postgres database.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_ADDR</span><span class=o>=</span>postgres.postgres
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>POSTGRES_PORT</span><span class=o>=</span><span class=m>5432</span>
</span></span></code></pre></div><h2 id=step-4-install-an-s3-compatible-object-store>Step 4. Install an S3-compatible object store<a class=td-heading-self-link href=#step-4-install-an-s3-compatible-object-store aria-label="Heading self-link"></a></h2><p>LLMariner requires an S3-compatible object store such as <a href=https://min.io/>MinIO</a> or <a href=https://seaweedfs.com>SeaweedfS</a>.</p><p>First set environmental variables to specify installation configuration:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Bucket name and the dummy region.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_BUCKET_NAME</span><span class=o>=</span>llmariner
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_REGION</span><span class=o>=</span>dummy
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Credentials for accessing the S3 bucket.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>AWS_ACCESS_KEY_ID</span><span class=o>=</span>llmariner-key
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>AWS_SECRET_ACCESS_KEY</span><span class=o>=</span>llmariner-secret
</span></span></code></pre></div><p>Then install an object store. Here are the example installation commands for MinIO and SeaweedFS.</p><ul class="nav nav-tabs" id=tabs-2 role=tablist><li class=nav-item><button class="nav-link disabled" id=tabs-02-00-tab data-bs-toggle=tab data-bs-target=#tabs-02-00 role=tab aria-controls=tabs-02-00 aria-selected=false>
<strong>Install</strong>:</button></li><li class=nav-item><button class="nav-link active" id=tabs-02-01-tab data-bs-toggle=tab data-bs-target=#tabs-02-01 role=tab data-td-tp-persist=minio aria-controls=tabs-02-01 aria-selected=true>
MinIO</button></li><li class=nav-item><button class=nav-link id=tabs-02-02-tab data-bs-toggle=tab data-bs-target=#tabs-02-02 role=tab data-td-tp-persist=seaweedfs aria-controls=tabs-02-02 aria-selected=false>
SeaweedFS</button></li></ul><div class=tab-content id=tabs-2-content><div class="tab-body tab-pane fade" id=tabs-02-00 role=tabpanel aria-labelled-by=tabs-02-00-tab tabindex=2></div><div class="tab-body tab-pane fade show active" id=tabs-02-01 role=tabpanel aria-labelled-by=tabs-02-01-tab tabindex=2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace minio <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  minio oci://registry-1.docker.io/bitnamicharts/minio <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.rootUser<span class=o>=</span>minioadmin <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set auth.rootPassword<span class=o>=</span>minioadmin <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set <span class=nv>defaultBuckets</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>${</span><span class=nv>S3_BUCKET_NAME</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl port-forward -n minio service/minio <span class=m>9001</span> <span class=p>&amp;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Wait until the port-forwarding connection is established.</span>
</span></span><span class=line><span class=cl>sleep <span class=m>5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Obtain the cookie and store in cookies.txt.</span>
</span></span><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:9001/api/v1/login <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cookie-jar cookies.txt <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s1>&#39;Content-Type: application/json&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data @- <span class=s>&lt;&lt; EOF
</span></span></span><span class=line><span class=cl><span class=s>{
</span></span></span><span class=line><span class=cl><span class=s>  &#34;accessKey&#34;: &#34;minioadmin&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;secretKey&#34;: &#34;minioadmin&#34;
</span></span></span><span class=line><span class=cl><span class=s>}
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a new API key.</span>
</span></span><span class=line><span class=cl>curl <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  http://localhost:9001/api/v1/service-account-credentials <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --cookie cookies.txt <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --request POST <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --header <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data @- <span class=s>&lt;&lt; EOF &gt;/dev/null
</span></span></span><span class=line><span class=cl><span class=s>{
</span></span></span><span class=line><span class=cl><span class=s>  &#34;name&#34;: &#34;LLMariner&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;accessKey&#34;: &#34;$AWS_ACCESS_KEY_ID&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;secretKey&#34;: &#34;$AWS_SECRET_ACCESS_KEY&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;description&#34;: &#34;&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;comment&#34;: &#34;&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;policy&#34;: &#34;&#34;,
</span></span></span><span class=line><span class=cl><span class=s>  &#34;expiry&#34;: null
</span></span></span><span class=line><span class=cl><span class=s>}
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>rm cookies.txt
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>kill</span> %1
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-02-02 role=tabpanel aria-labelled-by=tabs-02-02-tab tabindex=2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubectl create namespace seaweedfs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a secret.</span>
</span></span><span class=line><span class=cl><span class=c1># See https://github.com/seaweedfs/seaweedfs/wiki/Amazon-S3-API#public-access-with-anonymous-download for details.</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; s3-config.json
</span></span></span><span class=line><span class=cl><span class=s>{
</span></span></span><span class=line><span class=cl><span class=s>  &#34;identities&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>    {
</span></span></span><span class=line><span class=cl><span class=s>      &#34;name&#34;: &#34;me&#34;,
</span></span></span><span class=line><span class=cl><span class=s>      &#34;credentials&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>        {
</span></span></span><span class=line><span class=cl><span class=s>          &#34;accessKey&#34;: &#34;${AWS_ACCESS_KEY_ID}&#34;,
</span></span></span><span class=line><span class=cl><span class=s>          &#34;secretKey&#34;: &#34;${AWS_SECRET_ACCESS_KEY}&#34;
</span></span></span><span class=line><span class=cl><span class=s>        }
</span></span></span><span class=line><span class=cl><span class=s>      ],
</span></span></span><span class=line><span class=cl><span class=s>      &#34;actions&#34;: [
</span></span></span><span class=line><span class=cl><span class=s>        &#34;Admin&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;Read&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;ReadAcp&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;List&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;Tagging&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;Write&#34;,
</span></span></span><span class=line><span class=cl><span class=s>        &#34;WriteAcp&#34;
</span></span></span><span class=line><span class=cl><span class=s>      ]
</span></span></span><span class=line><span class=cl><span class=s>    }
</span></span></span><span class=line><span class=cl><span class=s>  ]
</span></span></span><span class=line><span class=cl><span class=s>}
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl create secret generic -n seaweedfs seaweedfs --from-file<span class=o>=</span>s3-config.json
</span></span><span class=line><span class=cl>rm s3-config.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># deploy seaweedfs</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | kubectl apply -n seaweedfs -f -
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: v1
</span></span></span><span class=line><span class=cl><span class=s>kind: PersistentVolume
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: seaweedfs-volume
</span></span></span><span class=line><span class=cl><span class=s>  labels:
</span></span></span><span class=line><span class=cl><span class=s>    type: local
</span></span></span><span class=line><span class=cl><span class=s>    app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  storageClassName: manual
</span></span></span><span class=line><span class=cl><span class=s>  capacity:
</span></span></span><span class=line><span class=cl><span class=s>    storage: 500Mi
</span></span></span><span class=line><span class=cl><span class=s>  accessModes:
</span></span></span><span class=line><span class=cl><span class=s>  - ReadWriteMany
</span></span></span><span class=line><span class=cl><span class=s>  hostPath:
</span></span></span><span class=line><span class=cl><span class=s>    path: /data/seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: v1
</span></span></span><span class=line><span class=cl><span class=s>kind: PersistentVolumeClaim
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: seaweedfs-volume-claim
</span></span></span><span class=line><span class=cl><span class=s>  labels:
</span></span></span><span class=line><span class=cl><span class=s>    app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  storageClassName: manual
</span></span></span><span class=line><span class=cl><span class=s>  accessModes:
</span></span></span><span class=line><span class=cl><span class=s>  - ReadWriteMany
</span></span></span><span class=line><span class=cl><span class=s>  resources:
</span></span></span><span class=line><span class=cl><span class=s>    requests:
</span></span></span><span class=line><span class=cl><span class=s>      storage: 500Mi
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: apps/v1
</span></span></span><span class=line><span class=cl><span class=s>kind: Deployment
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  replicas: 1
</span></span></span><span class=line><span class=cl><span class=s>  selector:
</span></span></span><span class=line><span class=cl><span class=s>    matchLabels:
</span></span></span><span class=line><span class=cl><span class=s>      app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>  template:
</span></span></span><span class=line><span class=cl><span class=s>    metadata:
</span></span></span><span class=line><span class=cl><span class=s>      labels:
</span></span></span><span class=line><span class=cl><span class=s>        app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>    spec:
</span></span></span><span class=line><span class=cl><span class=s>      containers:
</span></span></span><span class=line><span class=cl><span class=s>      - name: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>        image: chrislusf/seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>        args:
</span></span></span><span class=line><span class=cl><span class=s>        - server -s3 -s3.config=/etc/config/s3-config.json -dir=/data
</span></span></span><span class=line><span class=cl><span class=s>        ports:
</span></span></span><span class=line><span class=cl><span class=s>        - name: master
</span></span></span><span class=line><span class=cl><span class=s>          containerPort: 9333
</span></span></span><span class=line><span class=cl><span class=s>          protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>        - name: s3
</span></span></span><span class=line><span class=cl><span class=s>          containerPort: 8333
</span></span></span><span class=line><span class=cl><span class=s>          protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>        volumeMounts:
</span></span></span><span class=line><span class=cl><span class=s>        - name: seaweedfsdata
</span></span></span><span class=line><span class=cl><span class=s>          mountPath: /data
</span></span></span><span class=line><span class=cl><span class=s>        - name: config
</span></span></span><span class=line><span class=cl><span class=s>          mountPath: /etc/config
</span></span></span><span class=line><span class=cl><span class=s>      volumes:
</span></span></span><span class=line><span class=cl><span class=s>      - name: seaweedfsdata
</span></span></span><span class=line><span class=cl><span class=s>        persistentVolumeClaim:
</span></span></span><span class=line><span class=cl><span class=s>          claimName: seaweedfs-volume-claim
</span></span></span><span class=line><span class=cl><span class=s>      - name: config
</span></span></span><span class=line><span class=cl><span class=s>        secret:
</span></span></span><span class=line><span class=cl><span class=s>          secretName: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>---
</span></span></span><span class=line><span class=cl><span class=s>apiVersion: v1
</span></span></span><span class=line><span class=cl><span class=s>kind: Service
</span></span></span><span class=line><span class=cl><span class=s>metadata:
</span></span></span><span class=line><span class=cl><span class=s>  name: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>  labels:
</span></span></span><span class=line><span class=cl><span class=s>    app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>spec:
</span></span></span><span class=line><span class=cl><span class=s>  type: NodePort
</span></span></span><span class=line><span class=cl><span class=s>  ports:
</span></span></span><span class=line><span class=cl><span class=s>  - port: 9333
</span></span></span><span class=line><span class=cl><span class=s>    targetPort: master
</span></span></span><span class=line><span class=cl><span class=s>    protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>    name: master
</span></span></span><span class=line><span class=cl><span class=s>    nodePort: 31238
</span></span></span><span class=line><span class=cl><span class=s>  - port: 8333
</span></span></span><span class=line><span class=cl><span class=s>    targetPort: s3
</span></span></span><span class=line><span class=cl><span class=s>    protocol: TCP
</span></span></span><span class=line><span class=cl><span class=s>    name: s3
</span></span></span><span class=line><span class=cl><span class=s>    nodePort: 31239
</span></span></span><span class=line><span class=cl><span class=s>  selector:
</span></span></span><span class=line><span class=cl><span class=s>    app: seaweedfs
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl <span class=nb>wait</span> --timeout<span class=o>=</span>60s --for<span class=o>=</span><span class=nv>condition</span><span class=o>=</span>ready pod -n seaweedfs -l <span class=nv>app</span><span class=o>=</span>seaweedfs
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl port-forward -n seaweedfs service/seaweedfs <span class=m>8333</span> <span class=p>&amp;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Wait until the port-forwarding connection is established.</span>
</span></span><span class=line><span class=cl>sleep <span class=m>5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create the bucket.</span>
</span></span><span class=line><span class=cl>aws --endpoint-url http://localhost:8333 s3 mb s3://<span class=si>${</span><span class=nv>S3_BUCKET_NAME</span><span class=si>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>kill</span> %1
</span></span></code></pre></div></div></div><p>Then set environmental variable <code>S3_ENDPOINT_URL</code> to the URL of the object store. The URL should be accessible from LLMariner pods that will run on the same cluster.</p><ul class="nav nav-tabs" id=tabs-3 role=tablist><li class=nav-item><button class="nav-link disabled" id=tabs-03-00-tab data-bs-toggle=tab data-bs-target=#tabs-03-00 role=tab aria-controls=tabs-03-00 aria-selected=false>
<strong>Set up the endpoint URL with</strong>:</button></li><li class=nav-item><button class="nav-link active" id=tabs-03-01-tab data-bs-toggle=tab data-bs-target=#tabs-03-01 role=tab data-td-tp-persist=minio aria-controls=tabs-03-01 aria-selected=true>
MinIO</button></li><li class=nav-item><button class=nav-link id=tabs-03-02-tab data-bs-toggle=tab data-bs-target=#tabs-03-02 role=tab data-td-tp-persist=seaweedfs aria-controls=tabs-03-02 aria-selected=false>
SeaweedFs</button></li></ul><div class=tab-content id=tabs-3-content><div class="tab-body tab-pane fade" id=tabs-03-00 role=tabpanel aria-labelled-by=tabs-03-00-tab tabindex=3></div><div class="tab-body tab-pane fade show active" id=tabs-03-01 role=tabpanel aria-labelled-by=tabs-03-01-tab tabindex=3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_ENDPOINT_URL</span><span class=o>=</span>http://minio.minio:9000
</span></span></code></pre></div></div><div class="tab-body tab-pane fade" id=tabs-03-02 role=tabpanel aria-labelled-by=tabs-03-02-tab tabindex=3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>S3_ENDPOINT_URL</span><span class=o>=</span>http://seaweedfs.seaweedfs:8333
</span></span></code></pre></div></div></div><h2 id=step-5-install-milvus>Step 5. Install Milvus<a class=td-heading-self-link href=#step-5-install-milvus aria-label="Heading self-link"></a></h2><p>Install <a href=https://milvus.io/>Milvus</a> as it is used a backend vector database for RAG.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF &gt; milvus-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>cluster:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>etcd:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>pulsar:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>minio:
</span></span></span><span class=line><span class=cl><span class=s>  enabled: false
</span></span></span><span class=line><span class=cl><span class=s>  tls:
</span></span></span><span class=line><span class=cl><span class=s>    enabled: false
</span></span></span><span class=line><span class=cl><span class=s>extraConfigFiles:
</span></span></span><span class=line><span class=cl><span class=s>  user.yaml: |+
</span></span></span><span class=line><span class=cl><span class=s>    etcd:
</span></span></span><span class=line><span class=cl><span class=s>      use:
</span></span></span><span class=line><span class=cl><span class=s>        embed: true
</span></span></span><span class=line><span class=cl><span class=s>      data:
</span></span></span><span class=line><span class=cl><span class=s>        dir: /var/lib/milvus/etcd
</span></span></span><span class=line><span class=cl><span class=s>    common:
</span></span></span><span class=line><span class=cl><span class=s>      storageType: local
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm repo add zilliztech https://zilliztech.github.io/milvus-helm/
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace milvus <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  milvus zilliztech/milvus <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f milvus-values.yaml
</span></span></code></pre></div><p>Set the environmental variables so that LLMariner can later access the Postgres database.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>MILVUS_ADDR</span><span class=o>=</span>milvus.milvus
</span></span></code></pre></div><h2 id=step-6-install-llmariner>Step 6. Install LLMariner<a class=td-heading-self-link href=#step-6-install-llmariner aria-label="Heading self-link"></a></h2><p>Run the following command to set up a <code>values.yaml</code> and install LLMariner with Helm.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Set the endpoint URL of LLMariner. Please change if you are using a different ingress controller.</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>INGRESS_CONTROLLER_URL</span><span class=o>=</span>http://localhost:8080
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt; EOF | envsubst &gt; llmariner-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>global:
</span></span></span><span class=line><span class=cl><span class=s>  # This is an ingress configuration with Kong. Please change if you are using a different ingress controller.
</span></span></span><span class=line><span class=cl><span class=s>  ingress:
</span></span></span><span class=line><span class=cl><span class=s>    ingressClassName: kong
</span></span></span><span class=line><span class=cl><span class=s>    # The URL of the ingress controller. this can be a port-forwarding URL (e.g., http://localhost:8080) if there is
</span></span></span><span class=line><span class=cl><span class=s>    # no URL that is reachable from the outside of the EKS cluster.
</span></span></span><span class=line><span class=cl><span class=s>    controllerUrl: &#34;${INGRESS_CONTROLLER_URL}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    annotations:
</span></span></span><span class=line><span class=cl><span class=s>      # To remove the buffering from the streaming output of chat completion.
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/response-buffering: &#34;false&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  database:
</span></span></span><span class=line><span class=cl><span class=s>    host: &#34;${POSTGRES_ADDR}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    port: ${POSTGRES_PORT}
</span></span></span><span class=line><span class=cl><span class=s>    username: &#34;${POSTGRES_USER}&#34;
</span></span></span><span class=line><span class=cl><span class=s>    ssl:
</span></span></span><span class=line><span class=cl><span class=s>      mode: disable
</span></span></span><span class=line><span class=cl><span class=s>    createDatabase: true
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  databaseSecret:
</span></span></span><span class=line><span class=cl><span class=s>    name: postgres
</span></span></span><span class=line><span class=cl><span class=s>    key: password
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  objectStore:
</span></span></span><span class=line><span class=cl><span class=s>    s3:
</span></span></span><span class=line><span class=cl><span class=s>      endpointUrl: &#34;${S3_ENDPOINT_URL}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      bucket: &#34;${S3_BUCKET_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      region: &#34;${S3_REGION}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>  awsSecret:
</span></span></span><span class=line><span class=cl><span class=s>    name: aws
</span></span></span><span class=line><span class=cl><span class=s>    accessKeyIdKey: accessKeyId
</span></span></span><span class=line><span class=cl><span class=s>    secretAccessKeyKey: secretAccessKey
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>prepare:
</span></span></span><span class=line><span class=cl><span class=s>  database:
</span></span></span><span class=line><span class=cl><span class=s>    createSecret: true
</span></span></span><span class=line><span class=cl><span class=s>    secret:
</span></span></span><span class=line><span class=cl><span class=s>      password: &#34;${POSTGRES_PASSWORD}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  objectStore:
</span></span></span><span class=line><span class=cl><span class=s>    createSecret: true
</span></span></span><span class=line><span class=cl><span class=s>    secret:
</span></span></span><span class=line><span class=cl><span class=s>      accessKeyId: &#34;${AWS_ACCESS_KEY_ID}&#34;
</span></span></span><span class=line><span class=cl><span class=s>      secretAccessKey: &#34;${AWS_SECRET_ACCESS_KEY}&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>dex-server:
</span></span></span><span class=line><span class=cl><span class=s>  staticPasswords:
</span></span></span><span class=line><span class=cl><span class=s>  - email: admin@example.com
</span></span></span><span class=line><span class=cl><span class=s>    # bcrypt hash of the string: $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)
</span></span></span><span class=line><span class=cl><span class=s>    hash: &#34;\$2a\$10\$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W&#34;
</span></span></span><span class=line><span class=cl><span class=s>    username: admin-user
</span></span></span><span class=line><span class=cl><span class=s>    userID: admin-id
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>inference-manager-engine:
</span></span></span><span class=line><span class=cl><span class=s>  model:
</span></span></span><span class=line><span class=cl><span class=s>    default:
</span></span></span><span class=line><span class=cl><span class=s>      runtimeName: vllm
</span></span></span><span class=line><span class=cl><span class=s>      preloaded: true
</span></span></span><span class=line><span class=cl><span class=s>      resources:
</span></span></span><span class=line><span class=cl><span class=s>        limits:
</span></span></span><span class=line><span class=cl><span class=s>          nvidia.com/gpu: 1
</span></span></span><span class=line><span class=cl><span class=s>    overrides:
</span></span></span><span class=line><span class=cl><span class=s>      meta-llama/Meta-Llama-3.1-8B-Instruct-q4_0:
</span></span></span><span class=line><span class=cl><span class=s>        contextLength: 16384
</span></span></span><span class=line><span class=cl><span class=s>      google/gemma-2b-it-q4_0:
</span></span></span><span class=line><span class=cl><span class=s>        runtimeName: ollama
</span></span></span><span class=line><span class=cl><span class=s>        resources:
</span></span></span><span class=line><span class=cl><span class=s>         limits:
</span></span></span><span class=line><span class=cl><span class=s>           nvidia.com/gpu: 0
</span></span></span><span class=line><span class=cl><span class=s>      sentence-transformers/all-MiniLM-L6-v2-f16:
</span></span></span><span class=line><span class=cl><span class=s>        runtimeName: ollama
</span></span></span><span class=line><span class=cl><span class=s>        resources:
</span></span></span><span class=line><span class=cl><span class=s>         limits:
</span></span></span><span class=line><span class=cl><span class=s>           nvidia.com/gpu: 0
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>inference-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  service:
</span></span></span><span class=line><span class=cl><span class=s>    annotations:
</span></span></span><span class=line><span class=cl><span class=s>      # These annotations are only meaningful for Kong ingress controller to extend the timeout.
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/connect-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/read-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>      konghq.com/write-timeout: &#34;360000&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>job-manager-dispatcher:
</span></span></span><span class=line><span class=cl><span class=s>  serviceAccount:
</span></span></span><span class=line><span class=cl><span class=s>    create: false
</span></span></span><span class=line><span class=cl><span class=s>    name: &#34;${LLMARINER_SERVICE_ACCOUNT_NAME}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  notebook:
</span></span></span><span class=line><span class=cl><span class=s>    # Used to set the base URL of the API endpoint. This can be same as global.ingress.controllerUrl
</span></span></span><span class=line><span class=cl><span class=s>    # if the URL is reachable from the inside cluster. Otherwise you can change this to the
</span></span></span><span class=line><span class=cl><span class=s>    # to the URL of the ingress controller that is reachable inside the K8s cluster.
</span></span></span><span class=line><span class=cl><span class=s>    llmarinerBaseUrl: &#34;${INGRESS_CONTROLLER_URL}/v1&#34;
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s>model-manager-loader:
</span></span></span><span class=line><span class=cl><span class=s>  baseModels:
</span></span></span><span class=line><span class=cl><span class=s>  - meta-llama/Meta-Llama-3.1-8B-Instruct-q4_0
</span></span></span><span class=line><span class=cl><span class=s>  - google/gemma-2b-it-q4_0
</span></span></span><span class=line><span class=cl><span class=s>  - sentence-transformers/all-MiniLM-L6-v2-f16
</span></span></span><span class=line><span class=cl><span class=s>
</span></span></span><span class=line><span class=cl><span class=s># Required when RAG is used.
</span></span></span><span class=line><span class=cl><span class=s>vector-store-manager-server:
</span></span></span><span class=line><span class=cl><span class=s>  vectorDatabase:
</span></span></span><span class=line><span class=cl><span class=s>    host: &#34;${MILVUS_ADDR}&#34;
</span></span></span><span class=line><span class=cl><span class=s>  llmEngineAddr: ollama-sentence-transformers-all-minilm-l6-v2-f16:11434
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  llmariner oci://public.ecr.aws/cloudnatix/llmariner-charts/llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f llmariner-values.yaml
</span></span></code></pre></div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Starting from Helm v3.8.0, the OCI registry is supported by default. If you are using an older version, please upgrade to v3.8.0 or later. For more details, please refer to <a href=https://helm.sh/docs/topics/registries/>Helm OCI-based registries</a>.</div><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>If you are getting a 403 forbidden error, please try <code>docker logout public.ecr.aws</code>. Please see <a href=https://docs.aws.amazon.com/AmazonECR/latest/public/public-troubleshooting.html>AWS document</a> for more details.</div><p>If you would like to install only the control-plane components or the worker-plane components, please see <code>multi_cluster_deployment</code>{.interpreted-text role=&ldquo;doc&rdquo;}.</p><h2 id=step-7-verify-the-installation>Step 7. Verify the installation<a class=td-heading-self-link href=#step-7-verify-the-installation aria-label="Heading self-link"></a></h2><p>You can verify the installation by sending sample chat completion requests.</p><p>Note, if you have used LLMariner in other cases before you may need to delete the previous config by running <code>rm -rf ~/.config/llmariner</code></p><p>The default login user name is <code>admin@example.com</code> and the password is
<code>password</code>. You can change this by updating the Dex configuration
(<a href=/docs/features/user_management/>link</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>echo</span> <span class=s2>&#34;This is your endpoint URL: </span><span class=si>${</span><span class=nv>INGRESS_CONTROLLER_URL</span><span class=si>}</span><span class=s2>/v1&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma auth login
</span></span><span class=line><span class=cl><span class=c1># Type the above endpoint URL.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma models list
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model google-gemma-2b-it-q4_0 --role user --completion <span class=s2>&#34;what is k8s?&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>llma chat completions create --model meta-llama-Meta-Llama-3.1-8B-Instruct-q4_0 --role user --completion <span class=s2>&#34;hello&#34;</span>
</span></span></code></pre></div><h2 id=optional-monitor-gpu-utilization>Optional: Monitor GPU utilization<a class=td-heading-self-link href=#optional-monitor-gpu-utilization aria-label="Heading self-link"></a></h2><p>If you would like to install Prometheus and Grafana to see GPU utilization, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Add Prometheus</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; prom-scrape-configs.yaml
</span></span></span><span class=line><span class=cl><span class=s>- job_name: nvidia-dcgm
</span></span></span><span class=line><span class=cl><span class=s>  scrape_interval: 5s
</span></span></span><span class=line><span class=cl><span class=s>  static_configs:
</span></span></span><span class=line><span class=cl><span class=s>  - targets: [&#39;nvidia-dcgm-exporter.nvidia.svc:9400&#39;]
</span></span></span><span class=line><span class=cl><span class=s>- job_name: inference-manager-engine-metrics
</span></span></span><span class=line><span class=cl><span class=s>  scrape_interval: 5s
</span></span></span><span class=line><span class=cl><span class=s>  static_configs:
</span></span></span><span class=line><span class=cl><span class=s>  - targets: [&#39;inference-manager-server-http.llmariner.svc:8083&#39;]
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace monitoring <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --set-file <span class=nv>extraScrapeConfigs</span><span class=o>=</span>prom-scrape-configs.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  prometheus prometheus-community/prometheus
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add Grafana with DCGM dashboard</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; grafana-values.yaml
</span></span></span><span class=line><span class=cl><span class=s>datasources:
</span></span></span><span class=line><span class=cl><span class=s> datasources.yaml:
</span></span></span><span class=line><span class=cl><span class=s>   apiVersion: 1
</span></span></span><span class=line><span class=cl><span class=s>   datasources:
</span></span></span><span class=line><span class=cl><span class=s>   - name: Prometheus
</span></span></span><span class=line><span class=cl><span class=s>     type: prometheus
</span></span></span><span class=line><span class=cl><span class=s>     url: http://prometheus-server
</span></span></span><span class=line><span class=cl><span class=s>     isDefault: true
</span></span></span><span class=line><span class=cl><span class=s>dashboardProviders:
</span></span></span><span class=line><span class=cl><span class=s>  dashboardproviders.yaml:
</span></span></span><span class=line><span class=cl><span class=s>    apiVersion: 1
</span></span></span><span class=line><span class=cl><span class=s>    providers:
</span></span></span><span class=line><span class=cl><span class=s>    - name: &#39;default&#39;
</span></span></span><span class=line><span class=cl><span class=s>      orgId: 1
</span></span></span><span class=line><span class=cl><span class=s>      folder: &#39;default&#39;
</span></span></span><span class=line><span class=cl><span class=s>      type: file
</span></span></span><span class=line><span class=cl><span class=s>      disableDeletion: true
</span></span></span><span class=line><span class=cl><span class=s>      editable: true
</span></span></span><span class=line><span class=cl><span class=s>      options:
</span></span></span><span class=line><span class=cl><span class=s>        path: /var/lib/grafana/dashboards/standard
</span></span></span><span class=line><span class=cl><span class=s>dashboards:
</span></span></span><span class=line><span class=cl><span class=s>  default:
</span></span></span><span class=line><span class=cl><span class=s>    nvidia-dcgm-exporter:
</span></span></span><span class=line><span class=cl><span class=s>      gnetId: 12239
</span></span></span><span class=line><span class=cl><span class=s>      datasource: Prometheus
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>helm repo add grafana https://grafana.github.io/helm-charts
</span></span><span class=line><span class=cl>helm repo update
</span></span><span class=line><span class=cl>helm upgrade --install --wait <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --namespace monitoring <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --create-namespace <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -f grafana-values.yaml <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  grafana grafana/grafana
</span></span></code></pre></div><h2 id=optional-enable-tls>Optional: Enable TLS<a class=td-heading-self-link href=#optional-enable-tls aria-label="Heading self-link"></a></h2><p>First follow the <a href=https://cert-manager.io/docs/installation/>cert-manager installation document</a> and install cert-manager to your K8s cluster if you don&rsquo;t have one. Then create a <code>ClusterIssuer</code> for your domain. Here is an example manifest that uses Let's Encrypt.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert-manager.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIssuer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>acme</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>server</span><span class=p>:</span><span class=w> </span><span class=l>https://acme-v02.api.letsencrypt.org/directory</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>user@mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>privateKeySecretRef</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>solvers</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>http01</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>       </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>ingressClassName</span><span class=p>:</span><span class=w> </span><span class=l>kong</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=nt>selector</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>dnsZones</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span>- <span class=l>llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dns01</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div><p>Then you can add the following to <code>values.yaml</code> of LLMariner to enable TLS.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span></code></pre></div><p>The ingresses created from the Helm chart will have the following annotation and spec:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nn>...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-830457b369082df60146f171d2fecbac>6 - Install across Multiple Clusters</h1><div class=lead>Install LLMarinr across multiple Kubernetes clusters.</div><p>LLMariner deploys Kubernetes deployments to provision the LLM stack. In a typical configuration, all the services are deployed into a single Kubernetes cluster, but you can also deploy these services on multiple Kubernetes clusters. For example, you can deploy a control plane component in a CPU K8s cluster and deploy the rest of the components in GPU compute clusters.</p><p>LLMariner can be deployed into multiple GPU clusters, and the clusters can span across multiple cloud providers (including GPU specific clouds like CoreWeave) and on-prem.</p><p class="mt-4 mb-4 text-center"><img src=/images/multi_cluster.png width=1027 height=817></p><h2 id=deploying-control-plane-components>Deploying Control Plane Components<a class=td-heading-self-link href=#deploying-control-plane-components aria-label="Heading self-link"></a></h2><p>You can deploy only Control Plane components by specifying additional parameters the LLMariner helm chart.</p><p>In the <code>values.yaml</code>, you need to set <code>tag.worker</code> to <code>false</code>, <code>global.workerServiceIngress.create</code> to <code>true</code>, and set other values so that an ingress and a service are created to receive requests from worker nodes.</p><p>Here is an example <code>values.yaml</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>tags</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>worker</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>ingressClassName</span><span class=p>:</span><span class=w> </span><span class=l>kong</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>controllerUrl</span><span class=p>:</span><span class=w> </span><span class=l>https://api.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>konghq.com/response-buffering</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;false&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c># Enable TLS for the ingresses.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>hosts</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>api.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>api-tls</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=c># Create ingress for gRPC requests coming from worker clusters.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceIngress</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>create</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>cert-manager.io/cluster-issuer</span><span class=p>:</span><span class=w> </span><span class=l>letsencrypt</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>konghq.com/protocols</span><span class=p>:</span><span class=w> </span><span class=l>grpc,grpcs</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceGrpcService</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>konghq.com/protocol</span><span class=p>:</span><span class=w> </span><span class=l>grpc</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># Create a separate load balancer for gRPC streaming requests from inference-manager-engine.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>inference-manager-server</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceTls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>inference-cert</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceGrpcService</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>LoadBalancer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>external-dns.alpha.kubernetes.io/hostname</span><span class=p>:</span><span class=w> </span><span class=l>inference.llm.mydomain.com</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=c># Create a separate load balancer for HTTPS requests from session-manager-agent.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>session-manager-server</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceTls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>session-cert</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>workerServiceHttpService</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>LoadBalancer</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>externalTrafficPolicy</span><span class=p>:</span><span class=w> </span><span class=l>Local</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>service.beta.kubernetes.io/aws-load-balancer-type</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;nlb&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>external-dns.alpha.kubernetes.io/hostname</span><span class=p>:</span><span class=w> </span><span class=l>session.llm.mydomain.com</span><span class=w>
</span></span></span></code></pre></div><h2 id=deploying-worker-components>Deploying Worker Components<a class=td-heading-self-link href=#deploying-worker-components aria-label="Heading self-link"></a></h2><p>To deploy LLMariner to a worker GPU cluster, you first need to obtain a registration key for the cluster.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llma admin clusters register &lt;cluster-name&gt;
</span></span></code></pre></div><p>The following is an example command that sets the registration key to the environment variable.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>REGISTRATION_KEY</span><span class=o>=</span><span class=k>$(</span>llma admin clusters register &lt;cluster-name&gt; <span class=p>|</span> sed -n <span class=s1>&#39;s/.*Registration Key: &#34;\([^&#34;]*\)&#34;.*/\1/p&#39;</span><span class=k>)</span>
</span></span></code></pre></div><p>The command generates a new registration key.</p><p>Then you need to make LLMariner worker components to use the registration key when making gRPC calls to the control plane.</p><p>To make that happen, you first need to create a K8s secret.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>REGISTRATION_KEY</span><span class=o>=</span>clusterkey-...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl create secret generic <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -n llmariner <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  cluster-registration-key <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --from-literal<span class=o>=</span><span class=nv>regKey</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>${</span><span class=nv>REGISTRATION_KEY</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span></code></pre></div><p>The secret needs to be created in a namespace where LLMariner will be deployed.</p><p>When installing the Helm chart for the worker components, you need to specify addition configurations in <code>values.yaml</code>. Here is an example.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>tags</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>control-plane</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>global</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>objectStore</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>s3</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>endpointUrl</span><span class=p>:</span><span class=w> </span><span class=l>&lt;S3 endpoint&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>region</span><span class=p>:</span><span class=w> </span><span class=l>&lt;S3 regiona&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>bucket</span><span class=p>:</span><span class=w> </span><span class=l>&lt;S3 bucket name&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>awsSecret</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>aws</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>accessKeyIdKey</span><span class=p>:</span><span class=w> </span><span class=l>accessKeyId</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>secretAccessKeyKey</span><span class=p>:</span><span class=w> </span><span class=l>secretAccessKey</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>worker</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>controlPlaneAddr</span><span class=p>:</span><span class=w> </span><span class=l>api.llm.mydomain.com:443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>enable</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>registrationKeySecret</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cluster-registration-key</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>key</span><span class=p>:</span><span class=w> </span><span class=l>regKey</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>inference-manager-engine</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>inferenceManagerServerWorkerServiceAddr</span><span class=p>:</span><span class=w> </span><span class=l>inference.llm.mydomain.com:443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>job-manager-dispatcher</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>notebook</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>llmarinerBaseUrl</span><span class=p>:</span><span class=w> </span><span class=l>https://api.llm.mydomain.com/v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>session-manager-agent</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>sessionManagerServerWorkerServiceAddr</span><span class=p>:</span><span class=w> </span><span class=l>session.llm.mydomain.com:443</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>model-manager-loader</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>baseModels</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span>- <span class=l>&lt;model name, e.g. google/gemma-2b-it-q4_0&gt;</span><span class=w>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-aa1273494c214f035a8e9386ce6707b0>7 - Hosted Control Plane</h1><div class=lead>Install just the worker plane and use it with the hosted control plane.</div><p>CloudNatix provides a hosted control plane of LLMariner.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Note</h4>Work-in-progress. This is not fully ready yet, and the terms and conditions are subject to change as we might limit the usage based on the number of API calls or the number of GPU nodes.</div><p>CloudNatix provides a hosted control plane of LLMariner. End users can use the full functionality of LLMariner just by registering their worker GPU clusters to this hosted control plane.</p><h2 id=step-1-create-a-cloudnatix-account>Step 1. Create a CloudNatix account<a class=td-heading-self-link href=#step-1-create-a-cloudnatix-account aria-label="Heading self-link"></a></h2><p>Create a CloudNatix account if you haven't. Please visit <a href=https://app.cloudnatix.com>https://app.cloudnatix.com</a>. You can click one of the "Sign in or sing up" buttons for SSO login or you can click "Sign up" at the bottom for the email & password login.</p><h2 id=step-2-deploy-the-worker-plane-components>Step 2. Deploy the worker plane components<a class=td-heading-self-link href=#step-2-deploy-the-worker-plane-components aria-label="Heading self-link"></a></h2><p>Deploy the worker plane components LLMariner into your GPU cluster.</p><p>The API endpoint of the hosted control plane is <a href=https://api.llm.cloudnatix.com/v1>https://api.llm.cloudnatix.com/v1</a>.</p><p>Run <code>llma auth login</code> and use the above for the endpoint URL. Then follow <code>multi_cluster_deployment</code>{.interpreted-text role=&ldquo;doc&rdquo;} to obtain a cluster registration key and deploy LLMariner.</p><p>TODO: Add an example <code>values.yaml</code>.</p></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/llmariner/llmariner aria-label=GitHub><i class="fab fa-github"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=Slack aria-label=Slack><a target=_blank rel=noopener href=https://join.slack.com/t/llmariner/shared_invite/zt-2rbwooslc-LIrUCmK9kklfKsMEirUZbg aria-label=Slack><i class="fab fa-slack"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2024
<span class=td-footer__authors>CloudNatix, Inc.</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/js/main.min.f72d00502781aaf278a14088eb2356b1ba2a05ad698a0c43fe86314d74ceab56.js integrity="sha256-9y0AUCeBqvJ4oUCI6yNWsboqBa1pigxD/oYxTXTOq1Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>