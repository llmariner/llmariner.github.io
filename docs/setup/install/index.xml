<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Installation on LLMariner</title><link>https://llmariner.ai/docs/setup/install/</link><description>Recent content in Installation on LLMariner</description><generator>Hugo</generator><language>en</language><atom:link href="https://llmariner.ai/docs/setup/install/index.xml" rel="self" type="application/rss+xml"/><item><title>Install with Helm</title><link>https://llmariner.ai/docs/setup/install/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/overview/</guid><description>&lt;h2 id="prerequisites">Prerequisites&lt;a class="td-heading-self-link" href="#prerequisites" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>LLMariner requires the following resources:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html">Nvidia GPU Operator&lt;/a>&lt;/li>
&lt;li>Ingress controller (to route API requests)&lt;/li>
&lt;li>SQL database (to store jobs/models/files metadata)&lt;/li>
&lt;li>S3-compatible object store (to store training files and models)&lt;/li>
&lt;li>&lt;a href="https://milvus.io/">Milvus&lt;/a> (for RAG, optional)&lt;/li>
&lt;/ul>
&lt;p>LLMariner can process inference requests on CPU nodes, but it can be best used with GPU nodes. Nvidia GPU Operator is required to install the device plugin and make GPUs visible in the K8s cluster.&lt;/p></description></item><item><title>Set up a Playground on a GPU EC2 Instance</title><link>https://llmariner.ai/docs/setup/install/playground/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/playground/</guid><description>&lt;p>You can easily set up a playground for LLMariner and learn it. In this page, we provision an EC2 instance, build a &lt;a href="https://kind.sigs.k8s.io/">Kind&lt;/a> cluster, and deploy LLMariner and other required components.&lt;/p>


&lt;div class="alert alert-secondary" role="alert">
&lt;h4 class="alert-heading">Warn&lt;/h4>

 Playground environments are for experimentation use only. For a production-ready installation, please refere to the other installation guide.

&lt;/div>

&lt;p>Once all the setup completes, you can interact with the LLM service by directly hitting the API endpoints or using &lt;a href="https://github.com/openai/openai-python">the OpenAI Python library&lt;/a>.&lt;/p></description></item><item><title>Set up a Playground on a CPU-only Kind Cluster</title><link>https://llmariner.ai/docs/setup/install/cpu-only/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/cpu-only/</guid><description>&lt;p>Following this guide provides you with a simplified, local LLMariner installation by using the Kind and Helm. You can use this simple LLMariner deployment to try out features without GPUs.&lt;/p>


&lt;div class="alert alert-secondary" role="alert">
&lt;h4 class="alert-heading">Warn&lt;/h4>

 Playground environments are for experimentation use only. For a production-ready installation, please refere to the other installation guide.

&lt;/div>

&lt;h2 id="before-you-begin">Before you begin&lt;a class="td-heading-self-link" href="#before-you-begin" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Before you can get started with the LLMariner deployment you must install:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start">kind (Kubernetes in Docker)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://helmfile.readthedocs.io/en/latest/#installation">Helmfile&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="step-1-clone-the-repository">Step 1: Clone the repository&lt;a class="td-heading-self-link" href="#step-1-clone-the-repository" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>To get started, clone the LLMariner repository.&lt;/p></description></item><item><title>Install in a Single EKS Cluster</title><link>https://llmariner.ai/docs/setup/install/single_cluster_eks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/single_cluster_eks/</guid><description>&lt;p>This page goes through the concrete steps to create an EKS cluster, create necessary resources, and install LLMariner. You can skip some of the steps if you have already made necessary installation/setup.&lt;/p>
&lt;h2 id="step-1-provision-an-eks-cluster">Step 1. Provision an EKS cluster&lt;a class="td-heading-self-link" href="#step-1-provision-an-eks-cluster" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="step-11-create-a-new-cluster-with-karpenter">Step 1.1. Create a new cluster with Karpenter&lt;a class="td-heading-self-link" href="#step-11-create-a-new-cluster-with-karpenter" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>Either follow the &lt;a href="https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/">Karpenter getting started guide&lt;/a> and create an EKS cluster with Karpenter, or run the following simplified installation steps.&lt;/p></description></item><item><title>Install in a Single On-premise Cluster</title><link>https://llmariner.ai/docs/setup/install/single_cluster_onpremise/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/single_cluster_onpremise/</guid><description>&lt;p>This page goes through the concrete steps to install LLMariner on a on-premise K8s cluster (or a local K8s cluster).
You can skip some of the steps if you have already made necessary installation/setup.&lt;/p>


&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Installation of Postgres, MinIO, SeaweedFS, and Milvus are just example purposes, and
they are not intended for the production usage.
Please configure based on your requirements if you want to use LLMariner for your production environment.

&lt;/div>

&lt;h2 id="step-1-install-nvidia-gpu-operator">Step 1. Install Nvidia GPU Operator&lt;a class="td-heading-self-link" href="#step-1-install-nvidia-gpu-operator" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Nvidia GPU Operator is required to install the device plugin and make GPU resources visible in the K8s cluster. Run:&lt;/p></description></item><item><title>Install across Multiple Clusters</title><link>https://llmariner.ai/docs/setup/install/multi_cluster_production/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/multi_cluster_production/</guid><description>&lt;p>LLMariner deploys Kubernetes deployments to provision the LLM stack. In a typical configuration, all the services are deployed into a single Kubernetes cluster, but you can also deploy these services on multiple Kubernetes clusters. For example, you can deploy a control plane component in a CPU K8s cluster and deploy the rest of the components in GPU compute clusters.&lt;/p>
&lt;p>LLMariner can be deployed into multiple GPU clusters, and the clusters can span across multiple cloud providers (including GPU specific clouds like CoreWeave) and on-prem.&lt;/p></description></item><item><title>Hosted Control Plane</title><link>https://llmariner.ai/docs/setup/install/hosted_control_plane/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://llmariner.ai/docs/setup/install/hosted_control_plane/</guid><description>&lt;p>CloudNatix provides a hosted control plane of LLMariner.&lt;/p>


&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 Work-in-progress. This is not fully ready yet, and the terms and conditions are subject to change as we might limit the usage based on the number of API calls or the number of GPU nodes.

&lt;/div>

&lt;p>CloudNatix provides a hosted control plane of LLMariner. End users can use the full functionality of LLMariner just by registering their worker GPU clusters to this hosted control plane.&lt;/p></description></item></channel></rss>